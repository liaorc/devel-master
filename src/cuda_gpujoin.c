const char *pgstrom_cuda_gpujoin_code =
  "/*\n"
  " * cuda_gpujoin.h\n"
  " *\n"
  " * GPU accelerated parallel relations join based on hash-join or\n"
  " * nested-loop logic.\n"
  " * --\n"
  " * Copyright 2011-2016 (C) KaiGai Kohei <kaigai@kaigai.gr.jp>\n"
  " * Copyright 2014-2016 (C) The PG-Strom Development Team\n"
  " *\n"
  " * This program is free software; you can redistribute it and/or modify\n"
  " * it under the terms of the GNU General Public License version 2 as\n"
  " * published by the Free Software Foundation.\n"
  " *\n"
  " * This program is distributed in the hope that it will be useful,\n"
  " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
  " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
  " * GNU General Public License for more details.\n"
  " */\n"
  "#ifndef CUDA_GPUJOIN_H\n"
  "#define CUDA_GPUJOIN_H\n"
  "\n"
  "/*\n"
  " * definition of the inner relations structure. it can load multiple\n"
  " * kern_data_store or kern_hash_table.\n"
  " */\n"
  "typedef struct\n"
  "{\n"
  "\tcl_uint\t\t\tpg_crc32_table[256];\t/* used to hashjoin */\n"
  "\tcl_uint\t\t\tnrels;\t\t\t/* number of relations */\n"
  "\tcl_uint\t\t\tojmap_length;\t/* length of outer-join map, if any */\n"
  "\tstruct\n"
  "\t{\n"
  "\t\tcl_uint\t\tchunk_offset;\t/* offset to KDS or Hash */\n"
  "\t\tcl_uint\t\tojmap_offset;\t/* offset to outer-join map, if any */\n"
  "\t\tcl_bool\t\tis_nestloop;\t/* true, if NestLoop. */\n"
  "\t\tcl_bool\t\tleft_outer;\t\t/* true, if JOIN_LEFT or JOIN_FULL */\n"
  "\t\tcl_bool\t\tright_outer;\t/* true, if JOIN_RIGHT or JOIN_FULL */\n"
  "\t\tcl_char\t\t__padding__[1];\n"
  "\t} chunks[FLEXIBLE_ARRAY_MEMBER];\n"
  "} kern_multirels;\n"
  "\n"
  "#define KERN_MULTIRELS_INNER_KDS(kmrels, depth)\t\\\n"
  "\t((kern_data_store *)\t\t\t\t\t\t\\\n"
  "\t ((char *)(kmrels) + (kmrels)->chunks[(depth)-1].chunk_offset))\n"
  "\n"
  "#define KERN_MULTIRELS_OUTER_JOIN_MAP(kmrels, depth, outer_join_map)\t\\\n"
  "\t((cl_bool *)((kmrels)->chunks[(depth)-1].right_outer\t\t\t\t\\\n"
  "\t\t\t\t ? ((char *)(outer_join_map) +\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t(kmrels)->chunks[(depth)-1].ojmap_offset)\t\t\t\\\n"
  "\t\t\t\t : NULL))\n"
  "\n"
  "#define KERN_MULTIRELS_LEFT_OUTER_JOIN(kmrels, depth)\t\\\n"
  "\t((kmrels)->chunks[(depth)-1].left_outer)\n"
  "\n"
  "#define KERN_MULTIRELS_RIGHT_OUTER_JOIN(kmrels, depth)\t\\\n"
  "\t((kmrels)->chunks[(depth)-1].right_outer)\n"
  "\n"
  "/*\n"
  " * kern_gpujoin - control object of GpuJoin\n"
  " */\n"
  "typedef struct\n"
  "{\n"
  "\tcl_uint\t\twindow_orig;\t/* 'window_base' value on kernel invocation */\n"
  "\tcl_uint\t\twindow_base;\t/* base of the virtual partition window */\n"
  "\tcl_uint\t\twindow_size;\t/* size of the virtual partition window */\n"
  "\tcl_uint\t\tinner_nitems;\t/* out: number of inner join results */\n"
  "\tcl_uint\t\tright_nitems;\t/* out: number of right join results */\n"
  "\tcl_float\trow_dist_score;\t/* out: count of non-zero histgram items on\n"
  "\t\t\t\t\t\t\t\t * window resize. larger score means more\n"
  "\t\t\t\t\t\t\t\t * distributed depth, thus to be target of\n"
  "\t\t\t\t\t\t\t\t * the window split */\n"
  "\tcl_uint\t\twindow_base_saved;\t/* internal: last value of 'window_base' */\n"
  "\tcl_uint\t\twindow_size_saved;\t/* internal: last value of 'window_size' */\n"
  "\tcl_uint\t\tinner_nitems_stage;\t/* internal: # of inner join results */\n"
  "\tcl_uint\t\tright_nitems_stage;\t/* internal: # of right join results */\n"
  "} kern_join_scale;\n"
  "\n"
  "typedef struct\n"
  "{\n"
  "\tcl_uint\t\t\tkparams_offset;\t\t/* offset to the kparams */\n"
  "\tcl_uint\t\t\tkresults_1_offset;\t/* offset to the 1st kresults buffer */\n"
  "\tcl_uint\t\t\tkresults_2_offset;\t/* offset to the 2nd kresults buffer */\n"
  "\tcl_uint\t\t\tkresults_max_items;\t/* max items kresult_buf can hold */\n"
  "\t/* number of inner relations */\n"
  "\tcl_uint\t\t\tnum_rels;\n"
  "\t/* error status to be backed (OUT) */\n"
  "\tkern_errorbuf\tkerror;\n"
  "\t/* performance profiler */\n"
  "\tstruct {\n"
  "\t\tcl_uint\t\tnum_kern_outer_scan;\n"
  "\t\tcl_uint\t\tnum_kern_exec_nestloop;\n"
  "\t\tcl_uint\t\tnum_kern_exec_hashjoin;\n"
  "\t\tcl_uint\t\tnum_kern_outer_nestloop;\n"
  "\t\tcl_uint\t\tnum_kern_outer_hashjoin;\n"
  "\t\tcl_uint\t\tnum_kern_projection;\n"
  "\t\tcl_uint\t\tnum_kern_rows_dist;\n"
  "\t\tcl_float\ttv_kern_outer_scan;\n"
  "\t\tcl_float\ttv_kern_exec_nestloop;\n"
  "\t\tcl_float\ttv_kern_exec_hashjoin;\n"
  "\t\tcl_float\ttv_kern_outer_nestloop;\n"
  "\t\tcl_float\ttv_kern_outer_hashjoin;\n"
  "\t\tcl_float\ttv_kern_projection;\n"
  "\t\tcl_float\ttv_kern_rows_dist;\n"
  "\t\tcl_uint\t\tnum_minor_retry;\n"
  "\t\tcl_uint\t\tnum_major_retry;\n"
  "\t} pfm;\n"
  "\t/*\n"
  "\t * Scale of inner virtual window for each depth\n"
  "\t * (note that jscale has (num_rels + 1) elements\n"
  "\t */\n"
  "\tkern_join_scale\tjscale[FLEXIBLE_ARRAY_MEMBER];\n"
  "} kern_gpujoin;\n"
  "\n"
  "#define KERN_GPUJOIN_PARAMBUF(kgjoin)\t\t\t\\\n"
  "\t((kern_parambuf *)((char *)(kgjoin) + (kgjoin)->kparams_offset))\n"
  "#define KERN_GPUJOIN_PARAMBUF_LENGTH(kgjoin)\t\\\n"
  "\tSTROMALIGN(KERN_GPUJOIN_PARAMBUF(kgjoin)->length)\n"
  "#define KERN_GPUJOIN_HEAD_LENGTH(kgjoin)\t\t\t\t\\\n"
  "\tSTROMALIGN((char *)KERN_GPUJOIN_PARAMBUF(kgjoin) +\t\\\n"
  "\t\t\t   KERN_GPUJOIN_PARAMBUF_LENGTH(kgjoin) -\t\\\n"
  "\t\t\t   (char *)(kgjoin))\n"
  "\n"
  "#define KERN_GPUJOIN_1ST_RESULTBUF(kgjoin)\t\t\\\n"
  "\t((kern_resultbuf *)((char *)(kgjoin) + (kgjoin)->kresults_1_offset))\n"
  "#define KERN_GPUJOIN_2ND_RESULTBUF(kgjoin)\t\t\\\n"
  "\t((kern_resultbuf *)((char *)(kgjoin) + (kgjoin)->kresults_2_offset))\n"
  "\n"
  "#ifdef __CUDACC__\n"
  "\n"
  "/* utility macros for automatically generated code */\n"
  "#define GPUJOIN_REF_HTUP(chunk,offset)\t\t\t\\\n"
  "\t((offset) == 0\t\t\t\t\t\t\t\t\\\n"
  "\t ? NULL\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t : (HeapTupleHeaderData *)((char *)(chunk) + (size_t)(offset)))\n"
  "/* utility macros for automatically generated code */\n"
  "#define GPUJOIN_REF_DATUM(colmeta,htup,colidx)\t\\\n"
  "\t(!(htup) ? NULL : kern_get_datum_tuple((colmeta),(htup),(colidx)))\n"
  "\n"
  "/*\n"
  " * gpujoin_join_quals\n"
  " *\n"
  " * Evaluation of join qualifier in the given depth. It shall return true\n"
  " * if supplied pair of the rows matches the join condition.\n"
  " *\n"
  " * NOTE: if x-axil (outer input) or y-axil (inner input) are out of range,\n"
  " * we expect outer_index or inner_htup are NULL. Don't skip to call this\n"
  " * function, because nested-loop internally uses __syncthread operation\n"
  " * to reduce DRAM accesses.\n"
  " */\n"
  "STATIC_FUNCTION(cl_bool)\n"
  "gpujoin_join_quals(kern_context *kcxt,\n"
  "\t\t\t\t   kern_data_store *kds,\n"
  "\t\t\t\t   kern_multirels *kmrels,\n"
  "\t\t\t\t   int depth,\n"
  "\t\t\t\t   cl_uint *x_buffer,\n"
  "\t\t\t\t   HeapTupleHeaderData *inner_htup,\n"
  "\t\t\t\t   cl_bool *joinquals_matched);\n"
  "\n"
  "/*\n"
  " * gpujoin_hash_value\n"
  " *\n"
  " * Calculation of hash value if this depth uses hash-join logic.\n"
  " */\n"
  "STATIC_FUNCTION(cl_uint)\n"
  "gpujoin_hash_value(kern_context *kcxt,\n"
  "\t\t\t\t   cl_uint *pg_crc32_table,\n"
  "\t\t\t\t   kern_data_store *kds,\n"
  "\t\t\t\t   kern_multirels *kmrels,\n"
  "\t\t\t\t   cl_int depth,\n"
  "\t\t\t\t   cl_uint *x_buffer,\n"
  "\t\t\t\t   cl_bool *p_is_null_keys);\n"
  "\n"
  "/*\n"
  " * gpujoin_projection\n"
  " *\n"
  " * Implementation of device projection. Extract a pair of outer/inner tuples\n"
  " * on the tup_values/tup_isnull array.\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpujoin_projection(kern_context *kcxt,\n"
  "\t\t\t\t   kern_data_store *kds_src,\n"
  "\t\t\t\t   kern_multirels *kmrels,\n"
  "\t\t\t\t   cl_uint *r_buffer,\n"
  "\t\t\t\t   kern_data_store *kds_dst,\n"
  "\t\t\t\t   Datum *tup_values,\n"
  "\t\t\t\t   cl_bool *tup_isnull,\n"
  "\t\t\t\t   cl_short *tup_depth,\n"
  "\t\t\t\t   cl_char *extra_buf,\n"
  "\t\t\t\t   cl_uint *extra_len);\n"
  "\n"
  "/*\n"
  " * argument layout to launch inner/outer join functions\n"
  " */\n"
  "typedef struct\n"
  "{\n"
  "\tkern_gpujoin\t   *kgjoin;\n"
  "\tkern_data_store\t   *kds;\n"
  "\tkern_multirels\t   *kmrels;\n"
  "\tkern_resultbuf\t   *kresults_src;\n"
  "\tkern_resultbuf\t   *kresults_dst;\n"
  "\tcl_bool\t\t\t   *outer_join_map;\n"
  "\tcl_int\t\t\t\tdepth;\n"
  "\tcl_int\t\t\t\tcuda_index;\n"
  "\tcl_uint\t\t\t\twindow_base;\n"
  "\tcl_uint\t\t\t\twindow_size;\n"
  "} kern_join_args_t;\n"
  "\n"
  "/*\n"
  " * gpujoin_exec_outerscan\n"
  " *\n"
  " * Evaluation of outer-relation's qualifier, if any. Elsewhere, it always\n"
  " * returns true.\n"
  " */\n"
  "STATIC_FUNCTION(cl_bool)\n"
  "gpujoin_outer_quals(kern_context *kcxt,\n"
  "\t\t\t\t\tkern_data_store *kds,\n"
  "\t\t\t\t\tsize_t kds_index);\n"
  "\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_exec_outerscan(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t   kern_data_store *kds,\n"
  "\t\t\t\t\t   kern_resultbuf *kresults)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_context\tkcxt;\n"
  "\tcl_uint\t\t\twindow_base = kgjoin->jscale[0].window_base;\n"
  "\tcl_uint\t\t\twindow_size = kgjoin->jscale[0].window_size;\n"
  "\tcl_uint\t\t\tkds_index = window_base + get_global_id();\n"
  "\tcl_uint\t\t\tcount;\n"
  "\tcl_uint\t\t\toffset;\n"
  "\tcl_bool\t\t\tmatched;\n"
  "\t__shared__ cl_int base;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpujoin_exec_outerscan,kparams);\n"
  "\tassert(kresults->nrels == 1);\t/* only happen if depth == 1 */\n"
  "\n"
  "\tif (kds_index < min(kds->nitems, window_base + window_size))\n"
  "\t\tmatched = gpujoin_outer_quals(&kcxt, kds, kds_index);\n"
  "\telse\n"
  "\t\tmatched = false;\n"
  "\n"
  "\t/* expand kresults->nitems */\n"
  "\toffset = pgstromStairlikeSum(matched ? 1 : 0, &count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kresults->nitems, count);\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\tif (base + offset >= kresults->nrooms)\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\telse if (matched)\n"
  "\t\t{\n"
  "\t\t\tHeapTupleHeaderData *htup = kern_get_tuple_row(kds, kds_index);\n"
  "\t\t\tkresults->results[base + offset] = (size_t)htup - (size_t)kds;\n"
  "\t\t}\n"
  "\t}\n"
  "\tkern_writeback_error_status(&kresults->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_exec_nestloop(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t  kern_data_store *kds,\n"
  "\t\t\t\t\t  kern_multirels *kmrels,\n"
  "\t\t\t\t\t  kern_resultbuf *kresults_src,\n"
  "\t\t\t\t\t  kern_resultbuf *kresults_dst,\n"
  "\t\t\t\t\t  cl_bool *outer_join_map,\n"
  "\t\t\t\t\t  cl_int depth,\n"
  "\t\t\t\t\t  cl_int cuda_index,\n"
  "\t\t\t\t\t  cl_uint window_base,\n"
  "\t\t\t\t\t  cl_uint window_size)\n"
  "{\n"
  "\tkern_parambuf\t   *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_context\t\tkcxt;\n"
  "\tkern_data_store\t   *kds_in;\n"
  "\tcl_bool\t\t\t   *oj_map;\n"
  "\tHeapTupleHeaderData *y_htup;\n"
  "\tcl_uint\t\t\t   *x_buffer;\n"
  "\tcl_uint\t\t\t   *r_buffer;\n"
  "\tcl_uint\t\t\t\tx_index;\n"
  "\tcl_uint\t\t\t\tx_limit;\n"
  "\tcl_uint\t\t\t\ty_index;\n"
  "\tcl_uint\t\t\t\ty_limit;\n"
  "\tcl_uint\t\t\t\ty_offset = UINT_MAX;\t/* poison initial value */\n"
  "\tcl_bool\t\t\t\tis_matched;\n"
  "\tcl_uint\t\t\t\toffset;\n"
  "\tcl_uint\t\t\t\tcount;\n"
  "\t__shared__ cl_uint\tbase;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpujoin_exec_nestloop,kparams);\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(depth > 0 && depth <= kgjoin->num_rels);\n"
  "\tassert(kresults_dst->nrels == depth + 1);\n"
  "\tassert(kresults_src->nrels == depth);\n"
  "\tassert(kresults_src->nitems <= kresults_src->nrooms);\n"
  "\n"
  "\t/* inner tuple is pointed by y_index */\n"
  "\tkds_in = KERN_MULTIRELS_INNER_KDS(kmrels, depth);\n"
  "\tassert(kds_in != NULL);\n"
  "\n"
  "\t/*\n"
  "\t * Because of a historic reason, we call index of outer tuple 'x_index',\n"
  "\t * and index of inner tuple 'y_index'.\n"
  "\t */\n"
  "\tx_limit = kresults_src->nitems;\n"
  "\tx_index = lget_global_id() % x_limit;\n"
  "\ty_limit = min(window_base + window_size, kds_in->nitems);\n"
  "\ty_index = window_base + (lget_global_id() / x_limit);\n"
  "\n"
  "\t/* will be valid, if LEFT OUTER JOIN */\n"
  "\toj_map = KERN_MULTIRELS_OUTER_JOIN_MAP(kmrels, depth, outer_join_map);\n"
  "\t/* inside of the range? */\n"
  "\tif (x_index < x_limit && y_index < y_limit)\n"
  "\t{\n"
  "\t\t/* outer input */\n"
  "\t\tx_buffer = KERN_GET_RESULT(kresults_src, x_index);\n"
  "\t\t/* inner input */\n"
  "\t\ty_htup = kern_get_tuple_row(kds_in, y_index);\n"
  "\n"
  "\t\t/* does it satisfies join condition? */\n"
  "\t\tis_matched = gpujoin_join_quals(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\tkds,\n"
  "\t\t\t\t\t\t\t\t\t\tkmrels,\n"
  "\t\t\t\t\t\t\t\t\t\tdepth,\n"
  "\t\t\t\t\t\t\t\t\t\tx_buffer,\n"
  "\t\t\t\t\t\t\t\t\t\ty_htup,\n"
  "\t\t\t\t\t\t\t\t\t\tNULL);\n"
  "\t\tif (is_matched)\n"
  "\t\t{\n"
  "\t\t\ty_offset = (size_t)y_htup - (size_t)kds_in;\n"
  "\t\t\tif (oj_map && !oj_map[y_index])\n"
  "\t\t\t\toj_map[y_index] = true;\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t\tis_matched = false;\n"
  "\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/*\n"
  "\t * Expand kresults_dst->nitems, and put values\n"
  "\t */\n"
  "\toffset = pgstromStairlikeSum(is_matched ? 1 : 0, &count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/* still have space to store? */\n"
  "\t\tif (base + count >= kresults_dst->nrooms)\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\telse if (is_matched)\n"
  "\t\t{\n"
  "\t\t\tassert(x_buffer != NULL && y_htup != NULL);\n"
  "\t\t\tr_buffer = KERN_GET_RESULT(kresults_dst, base + offset);\n"
  "\t\t\tmemcpy(r_buffer, x_buffer, sizeof(cl_int) * depth);\n"
  "\t\t\tr_buffer[depth] = y_offset;\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\t}\n"
  "\tkern_writeback_error_status(&kresults_dst->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_exec_hashjoin\n"
  " *\n"
  " *\n"
  " *\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_exec_hashjoin(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t  kern_data_store *kds,\n"
  "\t\t\t\t\t  kern_multirels *kmrels,\n"
  "\t\t\t\t\t  kern_resultbuf *kresults_src,\n"
  "\t\t\t\t\t  kern_resultbuf *kresults_dst,\n"
  "\t\t\t\t\t  cl_bool *outer_join_map,\n"
  "\t\t\t\t\t  cl_int depth,\n"
  "\t\t\t\t\t  cl_int cuda_index,\n"
  "\t\t\t\t\t  cl_uint window_base,\n"
  "\t\t\t\t\t  cl_uint window_size)\n"
  "{\n"
  "\tkern_parambuf\t   *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_data_store\t   *kds_hash = KERN_MULTIRELS_INNER_KDS(kmrels, depth);\n"
  "\tkern_context\t\tkcxt;\n"
  "\tkern_hashitem\t   *khitem = NULL;\n"
  "\tcl_uint\t\t\t   *x_buffer = NULL;\n"
  "\tcl_uint\t\t\t   *r_buffer;\n"
  "\tcl_bool\t\t\t   *oj_map;\n"
  "\tcl_uint\t\t\t\tcrc_index;\n"
  "\tcl_uint\t\t\t\thash_value;\n"
  "\tcl_uint\t\t\t\toffset;\n"
  "\tcl_uint\t\t\t\tcount;\n"
  "\tcl_bool\t\t\t\tis_matched;\n"
  "\tcl_bool\t\t\t\tneeds_outer_row = false;\n"
  "\tcl_bool\t\t\t\tis_null_keys;\n"
  "\t__shared__ cl_uint\tbase;\n"
  "\t__shared__ cl_uint\tpg_crc32_table[256];\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpujoin_exec_hashjoin,kparams);\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(depth > 0 && depth <= kgjoin->num_rels);\n"
  "\tassert(kresults_dst->nrels == depth + 1);\n"
  "\tassert(kresults_src->nrels == depth);\n"
  "\tassert(kresults_src->nitems <= kresults_src->nrooms);\n"
  "\n"
  "\t/* move crc32 table to __local memory from __global memory.\n"
  "\t *\n"
  "\t * NOTE: calculation of hash value (based on crc32 in GpuHashJoin) is\n"
  "\t * the core of calculation workload in the GpuHashJoin implementation.\n"
  "\t * If we keep the master table is global memory, it will cause massive\n"
  "\t * amount of computing core stall because of RAM access latency.\n"
  "\t * So, we try to move them into local shared memory at the beginning.\n"
  "\t */\n"
  "\tfor (crc_index = get_local_id();\n"
  "\t\t crc_index < 256;\n"
  "\t\t crc_index += get_local_size())\n"
  "\t{\n"
  "\t\tpg_crc32_table[crc_index] = kmrels->pg_crc32_table[crc_index];\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/* will be valid, if RIGHT OUTER JOIN */\n"
  "\toj_map = KERN_MULTIRELS_OUTER_JOIN_MAP(kmrels, depth, outer_join_map);\n"
  "\n"
  "\t/* Calculation of hash-value by the outer join keys */\n"
  "\tif (get_global_id() < kresults_src->nitems)\n"
  "\t{\n"
  "\t\tx_buffer = KERN_GET_RESULT(kresults_src, get_global_id());\n"
  "\t\tassert(((size_t)x_buffer[0] & (sizeof(cl_ulong) - 1)) == 0);\n"
  "\t\thash_value = gpujoin_hash_value(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\tpg_crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\tkds,\n"
  "\t\t\t\t\t\t\t\t\t\tkmrels,\n"
  "\t\t\t\t\t\t\t\t\t\tdepth,\n"
  "\t\t\t\t\t\t\t\t\t\tx_buffer,\n"
  "\t\t\t\t\t\t\t\t\t\t&is_null_keys);\n"
  "\t\tif (hash_value >= kds_hash->hash_min &&\n"
  "\t\t\thash_value <= kds_hash->hash_max)\n"
  "\t\t{\n"
  "\t\t\t/* NOTE: NULL-keys never match on inner join */\n"
  "\t\t\tif (!is_null_keys)\n"
  "\t\t\t\tkhitem = KERN_HASH_FIRST_ITEM(kds_hash, hash_value);\n"
  "\t\t\tneeds_outer_row = true;\n"
  "\t\t}\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * Walks on the hash entries chain from the khitem\n"
  "\t */\n"
  "\tdo {\n"
  "\t\tif (khitem && (khitem->hash  == hash_value &&\n"
  "\t\t\t\t\t   khitem->rowid >= window_base &&\n"
  "\t\t\t\t\t   khitem->rowid <  window_base + window_size))\n"
  "\t\t{\n"
  "\t\t\tHeapTupleHeaderData *h_htup = &khitem->t.htup;\n"
  "\t\t\tcl_bool\t\t\tjoinquals_matched;\n"
  "\n"
  "\t\t\tis_matched = gpujoin_join_quals(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\tkds,\n"
  "\t\t\t\t\t\t\t\t\t\t\tkmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\tdepth,\n"
  "\t\t\t\t\t\t\t\t\t\t\tx_buffer,\n"
  "\t\t\t\t\t\t\t\t\t\t\th_htup,\n"
  "\t\t\t\t\t\t\t\t\t\t\t&joinquals_matched);\n"
  "\t\t\tif (joinquals_matched)\n"
  "\t\t\t{\n"
  "\t\t\t\t/* no need LEFT/FULL OUTER JOIN */\n"
  "\t\t\t\tneeds_outer_row = false;\n"
  "\t\t\t\t/* no need RIGHT/FULL OUTER JOIN */\n"
  "\t\t\t\tassert(khitem->rowid < kds_hash->nitems);\n"
  "\t\t\t\tif (oj_map && !oj_map[khitem->rowid])\n"
  "\t\t\t\t\toj_map[khitem->rowid] = true;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t\tis_matched = false;\n"
  "\n"
  "\t\t/* Expand kresults_dst->nitems */\n"
  "\t\toffset = pgstromStairlikeSum(is_matched ? 1 : 0, &count);\n"
  "\t\tif (count > 0)\n"
  "\t\t{\n"
  "\t\t\tif (get_local_id() == 0)\n"
  "\t\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t\t__syncthreads();\n"
  "\n"
  "\t\t\t/* kresults_dst still have enough space? */\n"
  "\t\t\tif (base + count >= kresults_dst->nrooms)\n"
  "\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\t\telse if (is_matched)\n"
  "\t\t\t{\n"
  "\t\t\t\tr_buffer = KERN_GET_RESULT(kresults_dst, base + offset);\n"
  "\t\t\t\tmemcpy(r_buffer, x_buffer, sizeof(cl_int) * depth);\n"
  "\t\t\t\tr_buffer[depth] = (size_t)&khitem->t.htup - (size_t)kds_hash;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Fetch next hash entry, then checks whether all the local\n"
  "\t\t * threads still have valid hash-entry or not.\n"
  "\t\t * (NOTE: this routine contains reduction operation)\n"
  "\t\t */\n"
  "\t\tkhitem = KERN_HASH_NEXT_ITEM(kds_hash, khitem);\n"
  "\t\tpgstromStairlikeSum(khitem != NULL ? 1 : 0, &count);\n"
  "\t} while (count > 0);\n"
  "\n"
  "\t/*\n"
  "\t * If no inner rows were matched on LEFT OUTER JOIN case, we fill up\n"
  "\t * the inner-side of result tuple with NULL.\n"
  "\t */\n"
  "\tif (KERN_MULTIRELS_LEFT_OUTER_JOIN(kmrels, depth))\n"
  "\t{\n"
  "\t\tif (needs_outer_row)\n"
  "\t\t{\n"
  "\t\t\tassert(x_buffer != NULL);\n"
  "\t\t\tis_matched = gpujoin_join_quals(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\tkds,\n"
  "\t\t\t\t\t\t\t\t\t\t\tkmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\tdepth,\n"
  "\t\t\t\t\t\t\t\t\t\t\tx_buffer,\n"
  "\t\t\t\t\t\t\t\t\t\t\tNULL,\n"
  "\t\t\t\t\t\t\t\t\t\t\tNULL);\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t\tis_matched = false;\n"
  "\n"
  "\t\toffset = pgstromStairlikeSum(is_matched ? 1 : 0, &count);\n"
  "\t\tif (count > 0)\n"
  "\t\t{\n"
  "\t\t\tif (get_local_id() == 0)\n"
  "\t\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t\t__syncthreads();\n"
  "\n"
  "\t\t\t/* kresults_dst still have enough space? */\n"
  "\t\t\tif (base + count >= kresults_dst->nrooms)\n"
  "\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\t\telse if (is_matched)\n"
  "\t\t\t{\n"
  "\t\t\t\tassert(x_buffer != NULL);\n"
  "\t\t\t\tr_buffer = KERN_GET_RESULT(kresults_dst, base + offset);\n"
  "\t\t\t\tmemcpy(r_buffer, x_buffer, sizeof(cl_int) * depth);\n"
  "\t\t\t\tr_buffer[depth] = 0;\t/* inner NULL */\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t}\n"
  "\tkern_writeback_error_status(&kresults_dst->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_collocate_outer_join_map\n"
  " *\n"
  " * it merges the result of other GPU devices and CPU fallback\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_colocate_outer_join_map(kern_multirels *kmrels,\n"
  "\t\t\t\t\t\t\t\tcl_bool *outer_join_map)\n"
  "{\n"
  "\tcl_uint\t\tnitems = kmrels->ojmap_length / sizeof(cl_uint);\n"
  "\tcl_uint\t   *self_map = (cl_uint *)outer_join_map;\n"
  "\tcl_uint\t   *recv_map = (cl_uint *)((char *)outer_join_map +\n"
  "\t\t\t\t\t\t\t\t\t   kmrels->ojmap_length);\n"
  "\tif (get_global_id() < nitems)\n"
  "\t\tself_map[get_global_id()] |= recv_map[get_global_id()];\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_outer_nestloop\n"
  " *\n"
  " * It injects unmatched tuples to kresuts_out buffer if RIGHT OUTER JOIN\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_outer_nestloop(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t   kern_data_store *kds,\t/* never referenced */\n"
  "\t\t\t\t\t   kern_multirels *kmrels,\n"
  "\t\t\t\t\t   kern_resultbuf *kresults_src,\t/* never referenced */\n"
  "\t\t\t\t\t   kern_resultbuf *kresults_dst,\n"
  "\t\t\t\t\t   cl_bool *outer_join_map,\n"
  "\t\t\t\t\t   cl_int depth,\n"
  "\t\t\t\t\t   cl_int cuda_index,\n"
  "\t\t\t\t\t   cl_uint window_base,\n"
  "\t\t\t\t\t   cl_uint window_size)\n"
  "{\n"
  "\tkern_parambuf\t   *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_data_store\t   *kds_in = KERN_MULTIRELS_INNER_KDS(kmrels, depth);\n"
  "\tkern_context\t\tkcxt;\n"
  "\tcl_bool\t\t\t\tneeds_outer_row;\n"
  "\tcl_uint\t\t\t\ty_index = window_base + get_global_id();\n"
  "\tcl_uint\t\t\t\ty_limit = min(window_base + window_size,\n"
  "\t\t\t\t\t\t\t\t\t  kds_in->nitems);\n"
  "\tcl_uint\t\t\t\tcount;\n"
  "\tcl_uint\t\t\t\toffset;\n"
  "\tcl_uint\t\t\t   *r_buffer;\n"
  "\t__shared__ cl_uint\tbase;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpujoin_outer_nestloop,kparams);\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(depth > 0 && depth <= kgjoin->num_rels);\n"
  "\tassert(kresults_dst->nrels == depth + 1);\n"
  "\n"
  "\t/*\n"
  "\t * check whether the relevant inner tuple has any matched outer tuples,\n"
  "\t * including the jobs by other devices.\n"
  "\t */\n"
  "\tif (y_index < y_limit)\n"
  "\t{\n"
  "\t\tcl_bool\t   *oj_map = KERN_MULTIRELS_OUTER_JOIN_MAP(kmrels, depth,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   outer_join_map);\n"
  "\n"
  "\t\tassert(oj_map != NULL);\n"
  "\t\tneeds_outer_row = (oj_map[y_index] ? false : true);\n"
  "\t\t/* check non-join-quals again */\n"
  "        if (needs_outer_row)\n"
  "        {\n"
  "\t\t\tHeapTupleHeaderData *htup = kern_get_tuple_row(kds_in, y_index);\n"
  "            needs_outer_row = gpujoin_join_quals(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t kds,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t depth,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t NULL,\t/* NULL for Left */\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t htup,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t NULL);\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t\tneeds_outer_row = false;\n"
  "\n"
  "\t/*\n"
  "\t * Count up number of inner tuples that were not matched with outer-\n"
  "\t * relations. Then, we allocates slot in kresults_dst for outer-join\n"
  "\t * tuples.\n"
  "\t */\n"
  "\toffset = pgstromStairlikeSum(needs_outer_row ? 1 : 0, &count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/* Does kresults_dst still have rooms to store? */\n"
  "\t\tif (base + count >= kresults_dst->nrooms)\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\telse if (needs_outer_row)\n"
  "\t\t{\n"
  "\t\t\t/*\n"
  "\t\t\t * OK, we know which row should be materialized using left outer\n"
  "\t\t\t * join manner, and result buffer was acquired. Let's put result\n"
  "\t\t\t * for the next stage.\n"
  "\t\t\t */\n"
  "\t\t\tHeapTupleHeaderData\t*htup = kern_get_tuple_row(kds_in, y_index);\n"
  "\t\t\tr_buffer = KERN_GET_RESULT(kresults_dst, base + offset);\n"
  "\t\t\tmemset(r_buffer, 0, sizeof(cl_int) * depth);\t/* NULL */\n"
  "\t\t\tr_buffer[depth] = (size_t)htup - (size_t)kds_in;\n"
  "\t\t}\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\tkern_writeback_error_status(&kresults_dst->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_outer_hashjoin\n"
  " *\n"
  " * It injects unmatched tuples to kresuts_out buffer if RIGHT OUTER JOIN.\n"
  " * We expect kernel is launched with larger than nslots threads.\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_outer_hashjoin(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t   kern_data_store *kds,\t/* never referenced */\n"
  "\t\t\t\t\t   kern_multirels *kmrels,\n"
  "\t\t\t\t\t   kern_resultbuf *kresults_src,\n"
  "\t\t\t\t\t   kern_resultbuf *kresults_dst,\n"
  "\t\t\t\t\t   cl_bool *outer_join_map,\n"
  "\t\t\t\t\t   cl_int depth,\n"
  "\t\t\t\t\t   cl_int cuda_index,\n"
  "\t\t\t\t\t   cl_uint window_base,\n"
  "\t\t\t\t\t   cl_uint window_size)\n"
  "{\n"
  "\tkern_parambuf\t   *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_data_store\t   *kds_hash = KERN_MULTIRELS_INNER_KDS(kmrels, depth);\n"
  "\tkern_hashitem\t   *khitem = NULL;\n"
  "\tkern_context\t\tkcxt;\n"
  "\tcl_uint\t\t\t\tkds_index;\n"
  "\tcl_bool\t\t\t\tneeds_outer_row;\n"
  "\tcl_uint\t\t\t\toffset;\n"
  "\tcl_uint\t\t\t\tcount;\n"
  "\tcl_uint\t\t\t   *r_buffer;\n"
  "\t__shared__ cl_uint\tbase;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpujoin_outer_hashjoin,kparams);\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(depth > 0 && depth <= kgjoin->num_rels);\n"
  "\tassert(kresults_dst->nrels == depth + 1);\n"
  "\tassert(window_base + window_size <= kds_hash->nitems);\n"
  "\tassert(window_size > 0);\n"
  "\n"
  "\tkds_index = window_base + get_global_id();\n"
  "\tif (kds_index < min(window_base + window_size,\n"
  "\t\t\t\t\t\tkds_hash->nitems))\n"
  "\t{\n"
  "\t\tcl_bool\t   *oj_map = KERN_MULTIRELS_OUTER_JOIN_MAP(kmrels, depth,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   outer_join_map);\n"
  "\t\tkhitem = KERN_DATA_STORE_HASHITEM(kds_hash, kds_index);\n"
  "\t\tassert(khitem->rowid == kds_index);\n"
  "\t\tneeds_outer_row = (oj_map[kds_index] ? false : true);\n"
  "\n"
  "\t\t/* check non-join-quals again */\n"
  "\t\tif (needs_outer_row)\n"
  "\t\t{\n"
  "\t\t\tneeds_outer_row = gpujoin_join_quals(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t kds,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t depth,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t NULL,\t/* NULL for Left */\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t &khitem->t.htup,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t NULL);\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t\tneeds_outer_row = false;\n"
  "\n"
  "\t/* expand kresults->nitems */\n"
  "\toffset = pgstromStairlikeSum(needs_outer_row ? 1 : 0, &count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\tif (base + offset >= kresults_dst->nrooms)\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\telse if (needs_outer_row)\n"
  "\t\t{\n"
  "\t\t\tr_buffer = KERN_GET_RESULT(kresults_dst, base + offset);\n"
  "\t\t\tmemset(r_buffer, 0, sizeof(cl_uint) * depth);\t/* NULL */\n"
  "\t\t\tassert((size_t)&khitem->t.htup > (size_t)kds_hash);\n"
  "\t\t\tr_buffer[depth] = (size_t)&khitem->t.htup - (size_t)kds_hash;\n"
  "\t\t}\n"
  "\t}\n"
  "\tkern_writeback_error_status(&kresults_dst->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_projection_row\n"
  " *\n"
  " * It makes joined relation on kds_dst\n"
  " */\n"
  "\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_projection_row(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t   kern_multirels *kmrels,\n"
  "\t\t\t\t\t   kern_data_store *kds_src,\n"
  "\t\t\t\t\t   kern_data_store *kds_dst,\n"
  "\t\t\t\t\t   kern_resultbuf *kresults)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_context\tkcxt;\n"
  "\tcl_uint\t\t\tdest_nitems = kds_dst->nitems + kresults->nitems;\n"
  "\tcl_uint\t\t\tdest_index = kds_dst->nitems + get_global_id();\n"
  "\tcl_uint\t\t\trequired;\n"
  "\tcl_uint\t\t\toffset;\n"
  "\tcl_uint\t\t\tcount;\n"
  "\t__shared__ cl_uint base;\n"
  "#if GPUJOIN_DEVICE_PROJECTION_NFIELDS > 0\n"
  "\tDatum\t\t\ttup_values[GPUJOIN_DEVICE_PROJECTION_NFIELDS];\n"
  "\tcl_bool\t\t\ttup_isnull[GPUJOIN_DEVICE_PROJECTION_NFIELDS];\n"
  "\tcl_short\t\ttup_depth[GPUJOIN_DEVICE_PROJECTION_NFIELDS];\n"
  "#else\n"
  "\tDatum\t\t   *tup_values = NULL;\n"
  "\tcl_bool\t\t   *tup_isnull = NULL;\n"
  "\tcl_short\t   *tup_depth = NULL;\n"
  "#endif\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(kresults->nrels == kgjoin->num_rels + 1);\n"
  "\tassert(kds_src == NULL || kds_src->format == KDS_FORMAT_ROW);\n"
  "\tassert(kds_dst->format == KDS_FORMAT_ROW && kds_dst->nslots == 0);\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt, gpujoin_projection_row, kparams);\n"
  "\n"
  "\tassert(kds_dst->nitems + kresults->nitems <= kds_dst->nrooms);\n"
  "\tif (get_global_id() < kresults->nitems)\n"
  "\t{\n"
  "#if GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE > 0\n"
  "\t\tcl_char\t\textra_buf[GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE]\n"
  "\t\t\t\t\t__attribute__ ((aligned(MAXIMUM_ALIGNOF)));\n"
  "#else\n"
  "\t\tcl_char\t   *extra_buf = NULL;\n"
  "#endif\n"
  "\t\tcl_uint\t\textra_len;\n"
  "\n"
  "\t\t/*\n"
  "\t\t * result buffer\n"
  "\t\t * -------------\n"
  "\t\t * r_buffer[0] -> offset from the 'kds_src'\n"
  "\t\t * r_buffer[i; i > 0] -> offset from the kern_data_store of individual\n"
  "\t\t *   depth in the kern_multirels buffer.\n"
  "\t\t *   (can be picked up using KERN_MULTIRELS_INNER_KDS)\n"
  "\t\t * r_buffer[*] may be 0, if NULL-tuple was set\n"
  "\t\t */\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Step.1 - compute length of the result tuple to be written\n"
  "\t\t */\n"
  "\t\tcl_uint\t   *r_buffer = KERN_GET_RESULT(kresults, get_global_id());\n"
  "\n"
  "\t\tgpujoin_projection(&kcxt,\n"
  "\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t   r_buffer,\n"
  "\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t   tup_values,\n"
  "\t\t\t\t\t\t   tup_isnull,\n"
  "\t\t\t\t\t\t   tup_depth,\n"
  "\t\t\t\t\t\t   extra_buf,\n"
  "\t\t\t\t\t\t   &extra_len);\n"
  "\t\tassert(extra_len <= GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE);\n"
  "\t\trequired = MAXALIGN(offsetof(kern_tupitem, htup) +\n"
  "\t\t\t\t\t\t\tcompute_heaptuple_size(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   tup_values,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   tup_isnull,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   NULL));\n"
  "\t}\n"
  "\telse\n"
  "\t\trequired = 0;\t/* out of the range */\n"
  "\n"
  "\t/*\n"
  "\t * Step.2 - increment the buffer usage of kds_dst\n"
  "\t */\n"
  "\toffset = pgstromStairlikeSum(required, &count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kds_dst->usage, count);\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\tif (KERN_DATA_STORE_HEAD_LENGTH(kds_dst) +\n"
  "\t\t\tSTROMALIGN(sizeof(cl_uint) * dest_nitems) +\n"
  "\t\t\tbase + count > kds_dst->length)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/*\n"
  "\t * Step.3 - write out the HeapTuple on the destination buffer\n"
  "\t */\n"
  "\tif (required > 0)\n"
  "\t{\n"
  "\t\tcl_uint\t\t\tpos = kds_dst->length - (base + offset + required);\n"
  "\t\tcl_uint\t\t   *tup_pos = KERN_DATA_STORE_ROWINDEX(kds_dst);\n"
  "\t\tkern_tupitem   *tupitem = (kern_tupitem *)((char *)kds_dst + pos);\n"
  "\n"
  "\t\tassert(pos == (pos & ~7));\n"
  "\t\ttup_pos[dest_index] = pos;\n"
  "\t\tform_kern_heaptuple(&kcxt, kds_dst, tupitem,\n"
  "\t\t\t\t\t\t\ttup_values, tup_isnull, NULL);\n"
  "\t}\n"
  "out:\n"
  "\t/* write-back execution status to host-side */\n"
  "\tkern_writeback_error_status(&kresults->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_projection_slot(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t\tkern_multirels *kmrels,\n"
  "\t\t\t\t\t\tkern_data_store *kds_src,\n"
  "\t\t\t\t\t\tkern_data_store *kds_dst,\n"
  "\t\t\t\t\t\tkern_resultbuf *kresults)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_context\tkcxt;\n"
  "\tcl_uint\t\t\tdest_index = kds_dst->nitems + get_global_id();\n"
  "\tcl_uint\t\t   *r_buffer;\n"
  "\tDatum\t\t   *tup_values;\n"
  "\tcl_bool\t\t   *tup_isnull;\n"
  "#if GPUJOIN_DEVICE_PROJECTION_NFIELDS > 0\n"
  "\tcl_short\t\ttup_depth[GPUJOIN_DEVICE_PROJECTION_NFIELDS];\n"
  "#else\n"
  "\tcl_short\t   *tup_depth = NULL;\n"
  "#endif\n"
  "#if GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE > 0\n"
  "\tcl_char\t\t\textra_buf[GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE]\n"
  "\t\t\t\t\t__attribute__ ((aligned(MAXIMUM_ALIGNOF)));\n"
  "#else\n"
  "\tcl_char\t\t   *extra_buf = NULL;\n"
  "#endif\n"
  "\tcl_uint\t\t\textra_len;\n"
  "\tchar\t\t   *vl_buf\t__attribute__ ((unused)) = NULL;\n"
  "\tcl_uint\t\t\toffset\t__attribute__ ((unused));\n"
  "\tcl_uint\t\t\tcount\t__attribute__ ((unused));\n"
  "\t__shared__ cl_uint base\t__attribute__ ((unused));\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(kresults->nrels == kgjoin->num_rels + 1);\n"
  "\tassert(kds_src == NULL || kds_src->format == KDS_FORMAT_ROW);\n"
  "\tassert(kds_dst->format == KDS_FORMAT_SLOT);\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt, gpujoin_projection_slot, kparams);\n"
  "\n"
  "\t/* Do projection if thread is responsible */\n"
  "\tif (get_global_id() < kresults->nitems)\n"
  "\t{\n"
  "\t\tr_buffer = KERN_GET_RESULT(kresults, get_global_id());\n"
  "\t\tassert(dest_index < kds_dst->nrooms);\n"
  "\t\ttup_values = KERN_DATA_STORE_VALUES(kds_dst, dest_index);\n"
  "\t\ttup_isnull = KERN_DATA_STORE_ISNULL(kds_dst, dest_index);\n"
  "\n"
  "\t\tgpujoin_projection(&kcxt,\n"
  "\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t   r_buffer,\n"
  "\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t   tup_values,\n"
  "\t\t\t\t\t\t   tup_isnull,\n"
  "\t\t\t\t\t\t   tup_depth,\n"
  "\t\t\t\t\t\t   extra_buf,\n"
  "\t\t\t\t\t\t   &extra_len);\n"
  "\t}\n"
  "\telse\n"
  "\t\textra_len = 0;\n"
  "\n"
  "#if GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE > 0\n"
  "\t/*\n"
  "\t * In case when GpuJoin result contains any indirect or numeric\n"
  "\t * data types, we have to allocate extra area on the kds_dst\n"
  "\t * buffer to store the contents.\n"
  "\t */\n"
  "\tassert(extra_len <= GPUJOIN_DEVICE_PROJECTION_EXTRA_SIZE);\n"
  "\tassert(extra_len == MAXALIGN(extra_len));\n"
  "\toffset = pgstromStairlikeSum(extra_len, &count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kds_dst->usage, count);\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\tif (KERN_DATA_STORE_SLOT_LENGTH(kds_dst, (kds_dst->nitems +\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t  kresults->nitems)) +\n"
  "\t\t\tbase + count > kds_dst->length)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tvl_buf = ((char *)kds_dst + kds_dst->length -\n"
  "\t\t\t\t  (base + offset + extra_len));\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "#else\n"
  "\tif (extra_len > 0)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt.e, StromError_WrongCodeGeneration);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "#endif\n"
  "\t/*\n"
  "\t * At this point, tup_values have device pointer or internal\n"
  "\t * data representation. We have to fix up these values to fit\n"
  "\t * host-side representation.\n"
  "\t */\n"
  "\tif (get_global_id() < kresults->nitems)\n"
  "\t{\n"
  "\t\tcl_uint\t\ti, ncols = kds_dst->ncols;\n"
  "\n"
  "\t\tfor (i=0; i < ncols; i++)\n"
  "\t\t{\n"
  "\t\t\tkern_colmeta\tcmeta = kds_dst->colmeta[i];\n"
  "\n"
  "\t\t\tif (tup_isnull[i])\n"
  "\t\t\t\ttup_values[i] = (Datum) 0;\t/* clean up */\n"
  "\t\t\telse if (!cmeta.attbyval)\n"
  "\t\t\t{\n"
  "\t\t\t\tchar   *addr = DatumGetPointer(tup_values[i]);\n"
  "\n"
  "\t\t\t\tif (tup_depth[i] == 0)\n"
  "\t\t\t\t\ttup_values[i] = devptr_to_host(kds_src, addr);\n"
  "\t\t\t\telse if (tup_depth[i] > 0)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tkern_data_store *kds_in =\n"
  "\t\t\t\t\t\tKERN_MULTIRELS_INNER_KDS(kmrels, tup_depth[i]);\n"
  "\t\t\t\t\ttup_values[i] = devptr_to_host(kds_in, addr);\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse if (tup_depth[i] == -1)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\t/* value is stored in the local array */\n"
  "\t\t\t\t\tcl_uint\t\tvl_len = (cmeta.attlen > 0 ?\n"
  "\t\t\t\t\t\t\t\t\t\t  cmeta.attlen :\n"
  "\t\t\t\t\t\t\t\t\t\t  VARSIZE_ANY(addr));\n"
  "\t\t\t\t\tmemcpy(vl_buf, addr, vl_len);\n"
  "\t\t\t\t\ttup_values[i] = devptr_to_host(kds_dst, vl_buf);\n"
  "\t\t\t\t\tvl_buf += MAXALIGN(vl_len);\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse if (tup_depth[i] == -2)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\t/* value is simple reference to kparams */\n"
  "\t\t\t\t\tassert(addr >= (char *)kparams &&\n"
  "\t\t\t\t\t\t   addr <  (char *)kparams + kparams->length);\n"
  "\t\t\t\t\ttup_values[i] = devptr_to_host(kparams, addr);\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse\n"
  "\t\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_WrongCodeGeneration);\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t}\n"
  "out:\n"
  "\t/* write-back execution status to host-side */\n"
  "\tkern_writeback_error_status(&kresults->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_count_rows_dist\n"
  " *\n"
  " * It counts a rough rows distribution degreee on buffer overflow.\n"
  " * No needs to be a strict count, so we use a histgram constructed on\n"
  " * a shared memory region as an indicator of the distribution.\n"
  " */\n"
  "typedef struct\n"
  "{\n"
  "\tkern_gpujoin   *kgjoin;\n"
  "\tkern_multirels *kmrels;\n"
  "\tkern_resultbuf *kresults;\n"
  "\tcl_int\t\t\tthread_width;\n"
  "} kern_rows_dist_args_t;\n"
  "\n"
  "#define ROW_DIST_COUNT_MAX_THREAD_WIDTH\t\t18\n"
  "#define ROW_DIST_HIST_SIZE\t\t(1024 * ROW_DIST_COUNT_MAX_THREAD_WIDTH)\n"
  "\n"
  "KERNEL_FUNCTION_MAXTHREADS(void)\n"
  "gpujoin_count_rows_dist(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t\tkern_multirels *kmrels,\n"
  "\t\t\t\t\t\tkern_resultbuf *kresults,\n"
  "\t\t\t\t\t\tcl_int thread_width)\n"
  "{\n"
  "\tcl_uint\t\tnvalids = min(kresults->nitems, kresults->nrooms);\n"
  "\tcl_uint\t\tlimit;\n"
  "\tcl_uint\t\ti, j;\n"
  "\t__shared__ cl_uint\tpg_crc32_table[256];\n"
  "\t__shared__ cl_bool\trow_dist_hist[ROW_DIST_HIST_SIZE];\n"
  "\n"
  "\t/* move crc32 table to __local memory from __global memory */\n"
  "\tfor (i = get_local_id();\n"
  "\t\t i < 256;\n"
  "\t\t i += get_local_size())\n"
  "\t{\n"
  "\t\tpg_crc32_table[i] = kmrels->pg_crc32_table[i];\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\n"
  "\tlimit = ((ROW_DIST_HIST_SIZE +\n"
  "\t\t\t  get_local_size() - 1) / get_local_size()) * get_local_size();\n"
  "\n"
  "\tfor (i=0; i < kresults->nrels; i++)\n"
  "\t{\n"
  "\t\tcl_uint\t\tcount = 0;\n"
  "\t\tcl_uint\t\thash;\n"
  "\n"
  "\t\t/* clear the row distribution histgram of this depth */\n"
  "\t\tfor (j = get_local_id();\n"
  "\t\t\t j < ROW_DIST_HIST_SIZE;\n"
  "\t\t\t j += get_local_size())\n"
  "\t\t{\n"
  "\t\t\trow_dist_hist[j] = 0;\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/* Makes row distribution histgram */\n"
  "\t\tfor (j=0; j < thread_width; j++)\n"
  "\t\t{\n"
  "\t\t\tcl_uint\t\tr_index = get_global_id() * thread_width + j;\n"
  "\n"
  "\t\t\tif (r_index < nvalids)\n"
  "\t\t\t{\n"
  "\t\t\t\tcl_uint\t   *r_buffer = KERN_GET_RESULT(kresults, r_index);\n"
  "\n"
  "\t\t\t\thash = pg_common_comp_crc32(pg_crc32_table, 0U,\n"
  "\t\t\t\t\t\t\t\t\t\t\t(const char *)(r_buffer + i),\n"
  "\t\t\t\t\t\t\t\t\t\t\tsizeof(cl_uint));\n"
  "\t\t\t\trow_dist_hist[hash % ROW_DIST_HIST_SIZE] = 1;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/* Count the row distribution histgram */\n"
  "\t\tfor (j = get_local_id();\n"
  "\t\t\t j < limit;\n"
  "\t\t\t j += get_local_size())\n"
  "\t\t{\n"
  "\t\t\tcount += __syncthreads_count(j < ROW_DIST_HIST_SIZE\n"
  "\t\t\t\t\t\t\t\t\t\t ? (int)row_dist_hist[j]\n"
  "\t\t\t\t\t\t\t\t\t\t : 0);\n"
  "\t\t}\n"
  "\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tatomicAdd(&kgjoin->jscale[i].row_dist_score, (cl_float)count);\n"
  "\t\t__syncthreads();\n"
  "\t}\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_resize_window\n"
  " *\n"
  " * It resizes the current virtual partition window size according to the\n"
  " * histogram of the latest join results, then, returns the victim depth\n"
  " * number. If negative number, it means we cannot split window any more.\n"
  " */\n"
  "STATIC_FUNCTION(cl_int)\n"
  "gpujoin_resize_window(kern_context *kcxt,\n"
  "\t\t\t\t\t  kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t  kern_multirels *kmrels,\n"
  "\t\t\t\t\t  kern_data_store *kds_src,\n"
  "\t\t\t\t\t  kern_data_store *kds_dst,\n"
  "\t\t\t\t\t  kern_resultbuf *kresults,\n"
  "\t\t\t\t\t  cl_int nsplits,\n"
  "\t\t\t\t\t  cl_uint dest_consumed,\n"
  "\t\t\t\t\t  kern_errorbuf kerror)\n"
  "{\n"
  "\tcudaFuncAttributes fattrs;\n"
  "\tkern_rows_dist_args_t *kern_args;\n"
  "\tdim3\t\t\tgrid_sz;\n"
  "\tdim3\t\t\tblock_sz;\n"
  "\tcl_uint\t\t\tnvalids;\n"
  "\tcl_uint\t\t\tunitsz;\n"
  "\tcl_uint\t\t\tthread_width;\n"
  "\tcl_uint\t\t\tdepth;\n"
  "\tcl_uint\t\t\ttarget_depth = 0;\n"
  "\tcl_float\t\trow_dist_score_largest = FLT_MIN;\n"
  "\tcl_ulong\t\ttv_start;\n"
  "\tcudaError_t\t\tstatus = cudaSuccess;\n"
  "\n"
  "\t/* To be called with an error status */\n"
  "\tassert(kerror.errcode != StromError_Success);\n"
  "\n"
  "\t/*\n"
  "\t * Even if we got StromError_DataStoreNoSpace once, it may not make much\n"
  "\t * sense to investigate an optimal window size when more than half of the\n"
  "\t * destination buffer was already consumed by the previous trial.\n"
  "\t * In this case, we will terminate the kernel execution instead of finding\n"
  "\t * out new optimal window size and retry.\n"
  "\t */\n"
  "\tif (2 * dest_consumed > kds_dst->length)\n"
  "\t{\n"
  "\t\tkern_join_scale\t   *jscale = kgjoin->jscale;\n"
  "\t\t/*\n"
  "\t\t * NOTE: jscale[*]->window_base and window_size must be reverted\n"
  "\t\t * if we give up next try with smaller window size.\n"
  "\t\t * The host code considers (window_base ... window_base + window_size)\n"
  "\t\t * are already completed and kds_dst contains the results from this\n"
  "\t\t * range, however, it is not processed actually. The next GpuJoin task\n"
  "\t\t * has to begin from the last window_base with proper size.\n"
  "\t\t *\n"
  "\t\t * Also note that, window_(base|size)_saved is initialized at end of\n"
  "\t\t * the last trial. However, dest_consumed is zero until first trial,\n"
  "\t\t * so we never reference these variables without second or later\n"
  "\t\t * trials.\n"
  "\t\t */\n"
  "\t\tfor (depth=0; depth <= kgjoin->num_rels; depth++)\n"
  "\t\t{\n"
  "\t\t\tjscale[depth].window_base = jscale[depth].window_base_saved;\n"
  "\t\t\tjscale[depth].window_size = jscale[depth].window_size_saved;\n"
  "\t\t}\n"
  "\t\treturn -1;\n"
  "\t}\n"
  "\n"
  "\t/* get max available block size */\n"
  "\tstatus = cudaFuncGetAttributes(&fattrs, (const void *)\n"
  "\t\t\t\t\t\t\t\t   gpujoin_count_rows_dist);\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt->e, status);\n"
  "\t\treturn -1;\n"
  "\t}\n"
  "\n"
  "\t/* how many items to be processed per thread? */\n"
  "\tnvalids = min(kresults->nitems, kresults->nrooms);\n"
  "\tthread_width = (nvalids - 1) / (NumSmx() * fattrs.maxThreadsPerBlock) + 1;\n"
  "\tif (thread_width > ROW_DIST_COUNT_MAX_THREAD_WIDTH)\n"
  "\t\tthread_width = ROW_DIST_COUNT_MAX_THREAD_WIDTH;\n"
  "\n"
  "\t/* allocation of argument buffer */\n"
  "\ttv_start = GlobalTimer();\n"
  "\tkern_args = (kern_rows_dist_args_t *)\n"
  "\t\tcudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t   sizeof(kern_rows_dist_args_t));\n"
  "\tif (!kern_args)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt->e, StromError_OutOfKernelArgs);\n"
  "\t\treturn -1;\n"
  "\t}\n"
  "\tkern_args->kgjoin = kgjoin;\n"
  "\tkern_args->kmrels = kmrels;\n"
  "\tkern_args->kresults = kresults;\n"
  "\tkern_args->thread_width = thread_width;\n"
  "\n"
  "\t/* special calculation of the kernel block size */\n"
  "\tblock_sz.x = fattrs.maxThreadsPerBlock;\n"
  "\tblock_sz.y = 1;\n"
  "\tblock_sz.z = 1;\n"
  "\tunitsz = thread_width * fattrs.maxThreadsPerBlock;\n"
  "\tgrid_sz.x = (nvalids - 1) / unitsz + 1;\n"
  "\tgrid_sz.y = 1;\n"
  "\tgrid_sz.z = 1;\n"
  "\n"
  "\t/* OK, launch the histgram calculation kernel */\n"
  "\tstatus = cudaLaunchDevice((void *)gpujoin_count_rows_dist,\n"
  "\t\t\t\t\t\t\t  kern_args, grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t  0,\n"
  "\t\t\t\t\t\t\t  NULL);\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt->e, status);\n"
  "\t\treturn -1;\n"
  "\t}\n"
  "\n"
  "\tstatus = cudaDeviceSynchronize();\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt->e, status);\n"
  "\t\treturn -1;\n"
  "\t}\n"
  "\tTIMEVAL_RECORD(kgjoin,kern_rows_dist,tv_start);\n"
  "\t/* NOTE: gpujoin_count_rows_dist never returns PG-Strom's error code */\n"
  "\n"
  "\t/*\n"
  "\t * Find out the most distributed depth\n"
  "\t */\n"
  "\tfor (depth=0; depth < kresults->nrels; depth++)\n"
  "\t{\n"
  "\t\tcl_uint     window_size = kgjoin->jscale[depth].window_size;\n"
  "\t\tcl_float    row_dist_score;\n"
  "\n"
  "\t\tif (window_size == 0)\n"
  "\t\t\tcontinue;\t/* should not happen */\n"
  "\t\t/*\n"
  "\t\t * NOTE: Adjustment of the row-distribution score\n"
  "\t\t *\n"
  "\t\t * Because of performance reason, we don't make a strict histgram\n"
  "\t\t * (it is almost equivalent to GpuPreAgg!). It is a sum of per-block\n"
  "\t\t * summary, thus, may contain duplications to be adjusted.\n"
  "\t\t * A unit size of the histgram is (thread-width * block-size).\n"
  "\t\t * If window size is less than the unit-size, it is obvious that rows\n"
  "\t\t * may be distributed in any histgrams. So, simply we divide the score\n"
  "\t\t * by grid-size.\n"
  "\t\t * If window size is larger than unit-size, we have to pay attention\n"
  "\t\t * for duplications across histgram. We adopted a simple approximate\n"
  "\t\t * that assumes rows will duplicate according to the square root of\n"
  "\t\t * number of histgrams.\n"
  "\t\t */\n"
  "\t\trow_dist_score = kgjoin->jscale[depth].row_dist_score;\n"
  "\t\tif (window_size <= unitsz)\n"
  "\t\t\trow_dist_score /= (cl_float)grid_sz.x;\n"
  "\t\telse\n"
  "\t\t\trow_dist_score /= (1.0 + sqrt((cl_float)(grid_sz.x - 1)));\n"
  "\n"
  "\t\tif (row_dist_score > row_dist_score_largest)\n"
  "\t\t{\n"
  "\t\t\ttarget_depth = depth;\n"
  "\t\t\trow_dist_score_largest = row_dist_score;\n"
  "\t\t}\n"
  "\t\tkgjoin->jscale[depth].row_dist_score = row_dist_score;\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * Reduction of the virtual partition window. Of course, we cannot\n"
  "\t * reduce the window size less than 1.\n"
  "\t */\n"
  "\tkgjoin->jscale[target_depth].window_size\n"
  "\t\t= kgjoin->jscale[target_depth].window_size / nsplits + 1;\n"
  "\tif (kgjoin->jscale[target_depth].window_size <= 1)\n"
  "\t{\n"
  "\t\t/* The original error code shall be set, and exit */\n"
  "\t\tif (kcxt->e.errcode == StromError_Success)\n"
  "\t\t\tkcxt->e = kerror;\n"
  "\t\treturn -1;\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * Inform caller the victim depth. If it is the last depth, we may\n"
  "\t * not need to retry from the head, but from the last depth\n"
  "\t */\n"
  "\treturn target_depth;\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpujoin_try_next_window\n"
  " *\n"
  " * It tries to move the current partition window to the next position, and\n"
  " * adjust size if we still have enough space on the kds_dst buffer.\n"
  " */\n"
  "STATIC_FUNCTION(cl_bool)\n"
  "gpujoin_try_next_window(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t\t\tkern_multirels *kmrels,\n"
  "\t\t\t\t\t\tkern_data_store *kds_src,\n"
  "\t\t\t\t\t\tkern_data_store *kds_dst,\n"
  "\t\t\t\t\t\tcl_uint dest_consumed,\n"
  "\t\t\t\t\t\tcl_uint last_nitems,\n"
  "\t\t\t\t\t\tcl_uint last_usage)\n"
  "{\n"
  "\tcl_int\t\ti, j;\n"
  "\tdouble\t\tratio = -1.0;\n"
  "\n"
  "\tif (kds_dst->format == KDS_FORMAT_ROW)\n"
  "\t{\n"
  "\t\t/* can we run the same scale join again? */\n"
  "\t\tif (dest_consumed +\n"
  "\t\t\tSTROMALIGN(sizeof(cl_uint) * last_nitems) +\n"
  "\t\t\tSTROMALIGN(last_usage) > kds_dst->length)\n"
  "\t\t\treturn false;\n"
  "\n"
  "\t\t/* how much larger window size if last nitems is not zero */\n"
  "\t\tif (last_nitems > 0)\n"
  "\t\t{\n"
  "\t\t\tassert(last_usage > 0);\n"
  "\t\t\tratio = ((double)(kds_dst->length - dest_consumed) /\n"
  "\t\t\t\t\t (double)(STROMALIGN(sizeof(cl_uint) * last_nitems) +\n"
  "\t\t\t\t\t\t\t  STROMALIGN(last_usage)));\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\t/* can we run the same scale join again? */\n"
  "\t\tif (dest_consumed +\n"
  "\t\t\tLONGALIGN((sizeof(Datum) +\n"
  "\t\t\t\t\t   sizeof(char)) * kds_dst->ncols) * last_nitems +\n"
  "\t\t\tSTROMALIGN(last_usage) > kds_dst->length)\n"
  "\t\t\treturn false;\n"
  "\n"
  "\t\t/* how much larger window size if last nitems is not zero */\n"
  "\t\tif (last_nitems > 0)\n"
  "\t\t{\n"
  "\t\t\tratio = ((double)(kds_dst->length - dest_consumed) /\n"
  "\t\t\t\t\t (double)(LONGALIGN(sizeof(Datum) +\n"
  "\t\t\t\t\t\t\t\t\t\tsizeof(char)) * kds_dst->ncols *\n"
  "\t\t\t\t\t\t\t  last_nitems + STROMALIGN(last_usage)));\n"
  "\t\t}\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * At this point, less than half of the kds_dst buffer is in use.\n"
  "\t * It means we can run the same scale join twice using this buffer,\n"
  "\t * if any of the inner/outer relations are virtually partitioned,\n"
  "\t * thus, a part of them were invisible on the previous try.\n"
  "\t *\n"
  "\t * XXX - We may need to revise the logic for more adeque adjustment\n"
  "\t * For example, 'window_orig' may be valuable to inform CPU exact\n"
  "\t * size of the next try. Based on heuristics, we expand the deepest\n"
  "\t * partitioned depth. It's uncertain whether it is right decision.\n"
  "\t */\n"
  "\tfor (i = kgjoin->num_rels; i >= 0; i--)\n"
  "\t{\n"
  "\t\tkern_join_scale\t   *jscale = kgjoin->jscale;\n"
  "\t\tcl_uint\t\t\t\tcurr_nitems;\n"
  "\n"
  "\t\tif (i == 0)\n"
  "\t\t\tcurr_nitems = (kds_src ? kds_src->nitems : 0);\n"
  "\t\telse\n"
  "\t\t\tcurr_nitems = KERN_MULTIRELS_INNER_KDS(kmrels, i)->nitems;\n"
  "\n"
  "\t\tif (jscale[i].window_base + jscale[i].window_size < curr_nitems)\n"
  "\t\t{\n"
  "\t\t\tcl_bool\t\tmeet_partition = false;\n"
  "\n"
  "\t\t\tjscale[i].window_base += jscale[i].window_size;\n"
  "\t\t\tjscale[i].window_size = Min(jscale[i].window_size,\n"
  "\t\t\t\t\t\t\t\t\t\tcurr_nitems - jscale[i].window_base);\n"
  "\t\t\t/* rewind deeper */\n"
  "\t\t\tfor (j = kgjoin->num_rels; j > i; j--)\n"
  "\t\t\t{\n"
  "\t\t\t\tcl_uint\t\tnitems;\n"
  "\n"
  "\t\t\t\tnitems = KERN_MULTIRELS_INNER_KDS(kmrels, j)->nitems;\n"
  "\n"
  "\t\t\t\tjscale[j].window_base = 0;\n"
  "\t\t\t\tif (!meet_partition &&\n"
  "\t\t\t\t\tjscale[j].window_size != nitems)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tif (last_nitems == 0)\n"
  "\t\t\t\t\t\tjscale[j].window_size = nitems;\n"
  "\t\t\t\t\telse\n"
  "\t\t\t\t\t{\n"
  "\t\t\t\t\t\tjscale[j].window_size = (cl_uint)\n"
  "\t\t\t\t\t\t\t((double)jscale[j].window_size * ratio);\n"
  "\t\t\t\t\t\tif (jscale[j].window_size > nitems)\n"
  "\t\t\t\t\t\t\tjscale[j].window_size = nitems;\n"
  "\t\t\t\t\t}\n"
  "\t\t\t\t\tmeet_partition = true;\n"
  "\t\t\t\t}\n"
  "\t\t\t}\n"
  "\n"
  "\t\t\tif (!meet_partition)\n"
  "\t\t\t{\n"
  "\t\t\t\tif (last_nitems == 0)\n"
  "\t\t\t\t\tjscale[i].window_size = (curr_nitems -\n"
  "\t\t\t\t\t\t\t\t\t\t\t jscale[i].window_base);\n"
  "\t\t\t\telse\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tjscale[i].window_size = (cl_uint)\n"
  "\t\t\t\t\t\t((double)jscale[i].window_size * ratio);\n"
  "\t\t\t\t\tif (jscale[i].window_base +\n"
  "\t\t\t\t\t\tjscale[i].window_size > curr_nitems)\n"
  "\t\t\t\t\t\tjscale[i].window_size = (curr_nitems -\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t jscale[i].window_base);\n"
  "\t\t\t\t}\n"
  "\t\t\t}\n"
  "\t\t\treturn true;\n"
  "\t\t}\n"
  "\t}\n"
  "\treturn false;\n"
  "}\n"
  "\n"
  "\n"
  "/* NOTE: This macro assumes name of local variables in gpujoin_main() */\n"
  "#define SETUP_KERN_JOIN_ARGS(argbuf)\t\t\t\\\n"
  "\t(argbuf)->kgjoin = kgjoin;\t\t\t\t\t\\\n"
  "\t(argbuf)->kds = kds_src;\t\t\t\t\t\\\n"
  "\t(argbuf)->kmrels = kmrels;\t\t\t\t\t\\\n"
  "\t(argbuf)->kresults_src = kresults_src;\t\t\\\n"
  "\t(argbuf)->kresults_dst = kresults_dst;\t\t\\\n"
  "\t(argbuf)->outer_join_map = outer_join_map;\t\\\n"
  "\t(argbuf)->depth = depth;\t\t\t\t\t\\\n"
  "\t(argbuf)->cuda_index = cuda_index;\t\t\t\\\n"
  "\t(argbuf)->window_base = window_base;\t\t\\\n"
  "\t(argbuf)->window_size = window_size\n"
  "\n"
  "/*\n"
  " * gpujoin_main - controller function of GpuJoin logic\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpujoin_main(kern_gpujoin *kgjoin,\t\t/* in/out: misc stuffs */\n"
  "\t\t\t kern_multirels *kmrels,\t/* in: inner sources */\n"
  "\t\t\t cl_bool *outer_join_map,\t/* internal buffer */\n"
  "\t\t\t kern_data_store *kds_src,\t/* in: outer source (may be NULL) */\n"
  "\t\t\t kern_data_store *kds_dst,\t/* out: join results */\n"
  "\t\t\t cl_int cuda_index)\t\t\t/* device index on the host side */\n"
  "{\n"
  "\tkern_parambuf\t   *kparams = KERN_GPUJOIN_PARAMBUF(kgjoin);\n"
  "\tkern_resultbuf\t   *kresults_src = KERN_GPUJOIN_1ST_RESULTBUF(kgjoin);\n"
  "\tkern_resultbuf\t   *kresults_dst = KERN_GPUJOIN_2ND_RESULTBUF(kgjoin);\n"
  "\tkern_resultbuf\t   *kresults_tmp;\n"
  "\tkern_join_scale\t   *jscale = kgjoin->jscale;\n"
  "\tkern_context\t\tkcxt;\n"
  "\tconst void\t\t   *kernel_projection;\n"
  "\tvoid\t\t\t  **kern_args;\n"
  "\tkern_join_args_t   *kern_join_args;\n"
  "\tdim3\t\t\t\tgrid_sz;\n"
  "\tdim3\t\t\t\tblock_sz;\n"
  "\tcl_uint\t\t\t\tkresults_max_items = kgjoin->kresults_max_items;\n"
  "\tcl_uint\t\t\t\twindow_base;\n"
  "\tcl_uint\t\t\t\twindow_size;\n"
  "\tcl_uint\t\t\t\tdest_usage_saved;\n"
  "\tcl_uint\t\t\t\tdest_consumed;\n"
  "\tcl_int\t\t\t\tdevice;\n"
  "\tcl_int\t\t\t\tdepth;\n"
  "\tcl_int\t\t\t\tnsplits;\n"
  "\tcl_int\t\t\t\tvictim;\n"
  "\tcl_ulong\t\t\ttv_start;\n"
  "\tcudaError_t\t\t\tstatus = cudaSuccess;\n"
  "\n"
  "\t/* Init kernel context */\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt, gpujoin_main, kparams);\n"
  "\tassert(get_global_size() == 1);\t\t/* only single thread */\n"
  "\tassert(!kds_src || kds_src->format == KDS_FORMAT_ROW);\n"
  "\tassert(kds_dst->format == KDS_FORMAT_ROW ||\n"
  "\t\t   kds_dst->format == KDS_FORMAT_SLOT);\n"
  "\n"
  "\t/* Get device clock for performance monitor */\n"
  "\tstatus = cudaGetDevice(&device);\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\n"
  "\tkds_dst->nitems = 0;\n"
  "\tdest_usage_saved = 0;\n"
  "\tdest_consumed = 0;\n"
  "retry_major:\n"
  "\t/* rewind the destination buffer; because kds_dst->usage is increased\n"
  "\t * by atomic operation even if this trial made too much joined results.\n"
  "\t * So, we save kds_dst->usage at the beginning. */\n"
  "\tkds_dst->usage = dest_usage_saved;\n"
  "\n"
  "\tfor (depth = 1; depth <= kgjoin->num_rels; depth++)\n"
  "\t{\n"
  "\t\tcl_bool\t\texec_right_outer;\n"
  "\n"
  "\tretry_minor:\n"
  "\t\t/* Does this depth need to launch RIGHT OUTER JOIN? */\n"
  "\t\texec_right_outer = (kds_src == NULL &&\n"
  "\t\t\t\t\t\t\tKERN_MULTIRELS_RIGHT_OUTER_JOIN(kmrels, depth));\n"
  "\t\t/*\n"
  "\t\t * Initialization of the kresults_src buffer if start depth.\n"
  "\t\t * Elsewhere, kresults_dst buffer of the last depth is also\n"
  "\t\t * kresults_src buffer in this depth.\n"
  "\t\t */\n"
  "\t\tif (depth == 1)\n"
  "\t\t{\n"
  "\t\t\tmemset(kresults_src, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\t\tkresults_src->nrels = depth;\n"
  "\t\t\tkresults_src->nrooms = kresults_max_items / depth;\n"
  "\t\t\tif (kds_src != NULL)\n"
  "\t\t\t{\n"
  "\t\t\t\t/* only happen if depth == 1 */\n"
  "\t\t\t\tassert(depth == 1);\n"
  "\n"
  "\t\t\t\t/* Launch:\n"
  "\t\t\t\t * gpujoin_exec_outerscan(kern_gpujoin *kgjoin,\n"
  "\t\t\t\t *                        kern_data_store *kds,\n"
  "\t\t\t\t *                        kern_resultbuf *kresults)\n"
  "\t\t\t\t */\n"
  "\t\t\t\ttv_start = GlobalTimer();\n"
  "\n"
  "\t\t\t\tkern_args = (void **)\n"
  "\t\t\t\t\tcudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t   sizeof(void *) * 3);\n"
  "\t\t\t\tif (!kern_args)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tkern_args[0] = kgjoin;\n"
  "\t\t\t\tkern_args[1] = kds_src;\n"
  "\t\t\t\tkern_args[2] = kresults_src;\n"
  "\n"
  "\t\t\t\twindow_size = kgjoin->jscale[0].window_size;\n"
  "\t\t\t\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tgpujoin_exec_outerscan,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\twindow_size,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaLaunchDevice((void *)gpujoin_exec_outerscan,\n"
  "\t\t\t\t\t\t\t\t\t\t  kern_args, grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tTIMEVAL_RECORD(kgjoin,kern_outer_scan,tv_start);\n"
  "\n"
  "\t\t\t\tif (kresults_src->kerror.errcode != StromError_Success)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tkcxt.e = kresults_src->kerror;\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\t/* update run-time statistics */\n"
  "\t\t\t\tjscale[0].inner_nitems_stage = kresults_src->nitems;\n"
  "\t\t\t\tjscale[0].right_nitems_stage = 0;\n"
  "\n"
  "\t\t\t\t/*\n"
  "\t\t\t\t * Once source nitems became zero, we cannot produce any\n"
  "\t\t\t\t * result tuples without RIGHT OUTER JOIN (and, it shall\n"
  "\t\t\t\t * not be kicked when kds_src != NULL).\n"
  "\t\t\t\t */\n"
  "\t\t\t\tif (kresults_src->nitems == 0)\n"
  "\t\t\t\t\tbreak;\n"
  "\t\t\t}\n"
  "\t\t\telse\n"
  "\t\t\t{\n"
  "\t\t\t\t/* In RIGHT OUTER JOIN, no input rows in depth==0 */\n"
  "\t\t\t\tjscale[0].inner_nitems_stage = 0;\n"
  "\t\t\t\tjscale[0].right_nitems_stage = 0;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t\t/* make the kresults_dst buffer empty */\n"
  "\t\tmemset(kresults_dst, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_dst->nrels = depth + 1;\n"
  "\t\tkresults_dst->nrooms = kresults_max_items / (depth + 1);\n"
  "\n"
  "\t\t/* inner partition window in this depth */\n"
  "\t\twindow_base = kgjoin->jscale[depth].window_base;\n"
  "\t\twindow_size = kgjoin->jscale[depth].window_size;\n"
  "\n"
  "\t\t/* Launch:\n"
  "\t\t * KERNEL_FUNCTION_MAXTHREADS(void)\n"
  "\t\t * gpujoin_exec_nestloop(kern_gpujoin *kgjoin,\n"
  "\t\t *                       kern_data_store *kds,\n"
  "\t\t *                       kern_multirels *kmrels,\n"
  "\t\t *                       kern_resultbuf *kresults_src,\n"
  "\t\t *                       kern_resultbuf *kresults_dst,\n"
  "\t\t *                       cl_bool *outer_join_map,\n"
  "\t\t *                       cl_int depth,\n"
  "\t\t *                       cl_int cuda_index,\n"
  "\t\t *                       cl_uint window_base,\n"
  "\t\t *                       cl_uint window_size)\n"
  "\t\t */\n"
  "\t\tif (kmrels->chunks[depth-1].is_nestloop)\n"
  "\t\t{\n"
  "\t\t\tif (kresults_src->nitems > 0)\n"
  "\t\t\t{\n"
  "\t\t\t\ttv_start = GlobalTimer();\n"
  "\t\t\t\tkern_join_args = (kern_join_args_t *)\n"
  "\t\t\t\t\tcudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t   sizeof(kern_join_args_t));\n"
  "\t\t\t\tif (!kern_join_args)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tSETUP_KERN_JOIN_ARGS(kern_join_args);\n"
  "\n"
  "\t\t\t\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tgpujoin_exec_nestloop,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(size_t)kresults_src->nitems *\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(size_t)window_size,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tstatus = cudaLaunchDevice((void *)gpujoin_exec_nestloop,\n"
  "\t\t\t\t\t\t\t\t\t\t  kern_join_args,\n"
  "\t\t\t\t\t\t\t\t\t\t  grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t  sizeof(cl_uint) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tTIMEVAL_RECORD(kgjoin,kern_exec_nestloop,tv_start);\n"
  "\n"
  "\t\t\t\tif (kresults_dst->kerror.errcode==StromError_DataStoreNoSpace)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tassert(kresults_dst->nitems > kresults_dst->nrooms);\n"
  "\t\t\t\t\tnsplits = kresults_dst->nitems / kresults_dst->nrooms + 1;\n"
  "\t\t\t\t\tvictim = gpujoin_resize_window(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kgjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   nsplits,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   dest_consumed,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst->kerror);\n"
  "\t\t\t\t\tif (victim < 0)\n"
  "\t\t\t\t\t\tgoto out;\n"
  "\t\t\t\t\telse if (victim < depth)\n"
  "\t\t\t\t\t{\n"
  "\t\t\t\t\t\tkgjoin->pfm.num_major_retry++;\n"
  "\t\t\t\t\t\tgoto retry_major;\n"
  "\t\t\t\t\t}\n"
  "\t\t\t\t\tkgjoin->pfm.num_minor_retry++;\n"
  "\t\t\t\t\tgoto retry_minor;\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse if (kresults_dst->kerror.errcode != StromError_Success)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tkcxt.e = kresults_dst->kerror;\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\t/* update run-time statistics */\n"
  "\t\t\t\tjscale[depth].inner_nitems_stage = kresults_dst->nitems;\n"
  "\t\t\t\tjscale[depth].right_nitems_stage = 0;\n"
  "\t\t\t}\n"
  "\t\t\telse\n"
  "\t\t\t{\n"
  "\t\t\t\t/* in case when no input rows. INNER JOIN produce no rows */\n"
  "\t\t\t\tjscale[depth].inner_nitems_stage = 0;\n"
  "\t\t\t\tjscale[depth].right_nitems_stage = 0;\n"
  "\t\t\t}\n"
  "\n"
  "\t\t\t/* Launch:\n"
  "\t\t\t * KERNEL_FUNCTION(void)\n"
  "\t\t\t * gpujoin_outer_nestloop(kern_gpujoin *kgjoin,\n"
  "\t\t\t *                        kern_data_store *kds,\n"
  "\t\t\t *                        kern_multirels *kmrels,\n"
  "\t\t\t *                        kern_resultbuf *kresults_src,\n"
  "\t\t\t *                        kern_resultbuf *kresults_dst,\n"
  "\t\t\t *                        cl_bool *outer_join_map,\n"
  "\t\t\t *                        cl_int depth,\n"
  "\t\t\t *                        cl_int cuda_index,\n"
  "\t\t\t *                        cl_uint window_base,\n"
  "\t\t\t *                        cl_uint window_size)\n"
  "\t\t\t *\n"
  "\t\t\t * NOTE: Host-side has to co-locate the outer join map\n"
  "\t\t\t * into this device, prior to the kernel launch.\n"
  "\t\t\t */\n"
  "\t\t\tif (exec_right_outer)\n"
  "\t\t\t{\n"
  "\t\t\t\ttv_start = GlobalTimer();\n"
  "\t\t\t\tkern_join_args = (kern_join_args_t *)\n"
  "\t\t\t\t\tcudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t   sizeof(kern_join_args_t));\n"
  "\t\t\t\tif (!kern_join_args)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tSETUP_KERN_JOIN_ARGS(kern_join_args);\n"
  "\n"
  "\t\t\t\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tgpujoin_outer_nestloop,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\twindow_size,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaLaunchDevice((void *)gpujoin_outer_nestloop,\n"
  "\t\t\t\t\t\t\t\t\t\t  kern_join_args,\n"
  "\t\t\t\t\t\t\t\t\t\t  grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tTIMEVAL_RECORD(kgjoin,kern_outer_nestloop,tv_start);\n"
  "\n"
  "\t\t\t\tif (kresults_dst->kerror.errcode==StromError_DataStoreNoSpace)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tassert(kresults_dst->nitems > kresults_dst->nrooms);\n"
  "\t\t\t\t\tnsplits = kresults_dst->nitems / kresults_dst->nrooms + 1;\n"
  "\t\t\t\t\tvictim = gpujoin_resize_window(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kgjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   nsplits,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   dest_consumed,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst->kerror);\n"
  "\t\t\t\t\tif (victim < 0)\n"
  "\t\t\t\t\t\tgoto out;\n"
  "\t\t\t\t\telse if (victim < depth)\n"
  "\t\t\t\t\t{\n"
  "\t\t\t\t\t\tkgjoin->pfm.num_major_retry++;\n"
  "\t\t\t\t\t\tgoto retry_major;\n"
  "\t\t\t\t\t}\n"
  "\t\t\t\t\tkgjoin->pfm.num_minor_retry++;\n"
  "\t\t\t\t\tgoto retry_minor;\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse if (kresults_dst->kerror.errcode != StromError_Success)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tkcxt.e = kresults_dst->kerror;\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\t/* update run-time statistics */\n"
  "\t\t\t\tjscale[depth].right_nitems_stage = kresults_dst->nitems;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\t/* Launch:\n"
  "\t\t\t * KERNEL_FUNCTION(void)\n"
  "\t\t\t * gpujoin_exec_hashjoin(kern_gpujoin *kgjoin,\n"
  "\t\t\t *                       kern_data_store *kds,\n"
  "\t\t\t *                       kern_multirels *kmrels,\n"
  "\t\t\t *                       kern_resultbuf *kresults_src,\n"
  "\t\t\t *                       kern_resultbuf *kresults_dst,\n"
  "\t\t\t *                       cl_bool *outer_join_map,\n"
  "\t\t\t *                       cl_int depth,\n"
  "\t\t\t *                       cl_int cuda_index,\n"
  "\t\t\t *                       cl_uint window_base,\n"
  "\t\t\t *                       cl_uint window_size);\n"
  "\t\t\t */\n"
  "\t\t\tif (kresults_src->nitems > 0)\n"
  "\t\t\t{\n"
  "\t\t\t\ttv_start = GlobalTimer();\n"
  "\t\t\t\tkern_join_args = (kern_join_args_t *)\n"
  "\t\t\t\t\tcudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t   sizeof(kern_join_args_t));\n"
  "\t\t\t\tif (!kern_join_args)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tSETUP_KERN_JOIN_ARGS(kern_join_args);\n"
  "\n"
  "\t\t\t\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tgpujoin_exec_hashjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tkresults_src->nitems,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaLaunchDevice((void *)gpujoin_exec_hashjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t  kern_join_args,\n"
  "\t\t\t\t\t\t\t\t\t\t  grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tTIMEVAL_RECORD(kgjoin,kern_exec_hashjoin,tv_start);\n"
  "\n"
  "\t\t\t\tif (kresults_dst->kerror.errcode==StromError_DataStoreNoSpace)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tassert(kresults_dst->nitems > kresults_dst->nrooms);\n"
  "\t\t\t\t\tnsplits = kresults_dst->nitems / kresults_dst->nrooms + 1;\n"
  "\t\t\t\t\tvictim = gpujoin_resize_window(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kgjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   nsplits,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   dest_consumed,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst->kerror);\n"
  "\t\t\t\t\tif (victim < 0)\n"
  "\t\t\t\t\t\tgoto out;\n"
  "\t\t\t\t\telse if (victim < depth)\n"
  "\t\t\t\t\t{\n"
  "\t\t\t\t\t\tkgjoin->pfm.num_major_retry++;\n"
  "\t\t\t\t\t\tgoto retry_major;\n"
  "\t\t\t\t\t}\n"
  "\t\t\t\t\tkgjoin->pfm.num_minor_retry++;\n"
  "\t\t\t\t\tgoto retry_minor;\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse if (kresults_dst->kerror.errcode != StromError_Success)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tkcxt.e = kresults_dst->kerror;\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\t/* update run-time statistics */\n"
  "\t\t\t\tjscale[depth].inner_nitems_stage = kresults_dst->nitems;\n"
  "\t\t\t\tjscale[depth].right_nitems_stage = 0;\n"
  "\t\t\t}\n"
  "\t\t\telse\n"
  "\t\t\t{\n"
  "\t\t\t\t/* no input rows, then no output rows */\n"
  "\t\t\t\tjscale[depth].inner_nitems_stage = 0;\n"
  "\t\t\t\tjscale[depth].right_nitems_stage = 0;\n"
  "\t\t\t}\n"
  "\n"
  "\t\t\t/* Launch:\n"
  "\t\t\t * KERNEL_FUNCTION(void)\n"
  "\t\t\t * gpujoin_outer_hashjoin(kern_gpujoin *kgjoin,\n"
  "\t\t\t *                        kern_data_store *kds,\n"
  "\t\t\t *                        kern_multirels *kmrels,\n"
  "\t\t\t *                        cl_bool *outer_join_map,\n"
  "\t\t\t *                        cl_int depth,\n"
  "\t\t\t *                        cl_int cuda_index);\n"
  "\t\t\t */\n"
  "\t\t\tif (exec_right_outer)\n"
  "\t\t\t{\n"
  "\t\t\t\ttv_start = GlobalTimer();\n"
  "\t\t\t\tkern_join_args = (kern_join_args_t *)\n"
  "\t\t\t\t\tcudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t   sizeof(kern_join_args_t));\n"
  "\t\t\t\tif (!kern_join_args)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tSETUP_KERN_JOIN_ARGS(kern_join_args);\n"
  "\n"
  "\t\t\t\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tgpujoin_outer_hashjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\twindow_size,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaLaunchDevice((void *)gpujoin_outer_hashjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t  kern_join_args,\n"
  "\t\t\t\t\t\t\t\t\t\t  grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\n"
  "\t\t\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tTIMEVAL_RECORD(kgjoin,kern_outer_hashjoin,tv_start);\n"
  "\n"
  "\t\t\t\tif (kresults_dst->kerror.errcode==StromError_DataStoreNoSpace)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tassert(kresults_dst->nitems > kresults_dst->nrooms);\n"
  "\t\t\t\t\tnsplits = kresults_dst->nitems / kresults_dst->nrooms + 1;\n"
  "\t\t\t\t\tvictim = gpujoin_resize_window(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kgjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   nsplits,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   dest_consumed,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kresults_dst->kerror);\n"
  "\t\t\t\t\tif (victim < 0)\n"
  "\t\t\t\t\t\tgoto out;\n"
  "\t\t\t\t\telse if (victim < depth)\n"
  "\t\t\t\t\t{\n"
  "\t\t\t\t\t\tkgjoin->pfm.num_major_retry++;\n"
  "\t\t\t\t\t\tgoto retry_major;\n"
  "\t\t\t\t\t}\n"
  "\t\t\t\t\tkgjoin->pfm.num_minor_retry++;\n"
  "\t\t\t\t\tgoto retry_minor;\n"
  "\t\t\t\t}\n"
  "\t\t\t\telse if (kresults_dst->kerror.errcode != StromError_Success)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\tkcxt.e = kresults_dst->kerror;\n"
  "\t\t\t\t\tgoto out;\n"
  "\t\t\t\t}\n"
  "\t\t\t\t/* update run-time statistics */\n"
  "\t\t\t\tjscale[depth].right_nitems_stage = kresults_dst->nitems;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Swap result buffer\n"
  "\t\t */\n"
  "\t\tkresults_tmp = kresults_src;\n"
  "\t\tkresults_src = kresults_dst;\n"
  "\t\tkresults_dst = kresults_tmp;\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Once nitems became zero, we have no chance to produce any\n"
  "\t\t * result tuples without RIGHT OUTER JOIN. So, we don't need\n"
  "\t\t * to walk down deeper level any more, for a niche optimization.\n"
  "\t\t */\n"
  "\t\tif (kds_src != NULL && kresults_src->nitems == 0)\n"
  "\t\t\tbreak;\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * gpujoin_projection_* makes sense only if any joined tuples are\n"
  "\t * produced. Elsewhere, we can skip it.\n"
  "\t */\n"
  "\tif (kresults_src->nitems > 0)\n"
  "\t{\n"
  "\t\tassert(kresults_src->kerror.errcode == StromError_Success);\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Launch:\n"
  "\t\t * KERNEL_FUNCTION(void)\n"
  "\t\t * gpujoin_projection_(row|slot)(kern_gpujoin *kgjoin,\n"
  "\t\t *                               kern_multirels *kmrels,\n"
  "\t\t *                               kern_data_store *kds_src,\n"
  "\t\t *                               kern_data_store *kds_dst,\n"
  "\t\t *                               kern_resultbuf *kresults)\n"
  "\t\t */\n"
  "\t\ttv_start = GlobalTimer();\n"
  "\t\tif (kds_dst->format == KDS_FORMAT_ROW)\n"
  "\t\t\tkernel_projection = (const void *)gpujoin_projection_row;\n"
  "\t\telse\n"
  "\t\t\tkernel_projection = (const void *)gpujoin_projection_slot;\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Setup kds_dst according to the final kern_resultbuf\n"
  "\t\t */\n"
  "\t\tif (kds_dst->nitems + kresults_src->nitems >= kds_dst->nrooms)\n"
  "\t\t{\n"
  "\t\t\t/* for temporary usage of error buffer */\n"
  "\t\t\tSTROM_SET_ERROR(&kresults_src->kerror,\n"
  "\t\t\t\t\t\t\tStromError_DataStoreNoSpace);\n"
  "\n"
  "\t\t\tassert(kds_dst->nitems < kds_dst->nrooms);\n"
  "\t\t\tnsplits = kresults_src->nitems /\n"
  "\t\t\t\t(kds_dst->nrooms - kds_dst->nitems) + 1;\n"
  "\t\t\tif (gpujoin_resize_window(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t  kgjoin,\n"
  "\t\t\t\t\t\t\t\t\t  kmrels,\n"
  "\t\t\t\t\t\t\t\t\t  kds_src,\n"
  "\t\t\t\t\t\t\t\t\t  kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t  kresults_src,\n"
  "\t\t\t\t\t\t\t\t\t  nsplits,\n"
  "\t\t\t\t\t\t\t\t\t  dest_consumed,\n"
  "\t\t\t\t\t\t\t\t\t  kresults_src->kerror) < 0)\n"
  "\t\t\t\tgoto out;\n"
  "\n"
  "\t\t\tkgjoin->pfm.num_major_retry++;\n"
  "\t\t\tgoto retry_major;\n"
  "\t\t}\n"
  "\t\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 5);\n"
  "\t\tif (!kern_args)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tkern_args[0] = kgjoin;\n"
  "\t\tkern_args[1] = kmrels;\n"
  "\t\tkern_args[2] = kds_src;\n"
  "\t\tkern_args[3] = kds_dst;\n"
  "\t\tkern_args[4] = kresults_src;\n"
  "\n"
  "\t\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\tkernel_projection,\n"
  "\t\t\t\t\t\t\t\t\t\tkresults_src->nitems,\n"
  "\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaLaunchDevice((void *)kernel_projection,\n"
  "\t\t\t\t\t\t\t\t  kern_args, grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tTIMEVAL_RECORD(kgjoin,kern_projection,tv_start);\n"
  "\n"
  "\t\tif (kresults_src->kerror.errcode == StromError_DataStoreNoSpace)\n"
  "\t\t{\n"
  "\t\t\tcl_uint\t\tncols = kds_dst->ncols;\n"
  "\t\t\tcl_uint\t\tnitems_to_fit;\n"
  "\t\t\tcl_uint\t\twidth_avg;\n"
  "\n"
  "\t\t\tif (kresults_src->nitems == 0)\n"
  "\t\t\t\treturn;\t\t/* should never happen */\n"
  "\n"
  "\t\t\tassert(kds_dst->usage >= dest_usage_saved);\n"
  "\t\t\tif (kds_dst->format == KDS_FORMAT_SLOT)\n"
  "\t\t\t{\n"
  "\t\t\t\twidth_avg = (LONGALIGN((sizeof(Datum) +\n"
  "\t\t\t\t\t\t\t\t\t\tsizeof(char)) * ncols) +\n"
  "\t\t\t\t\t\t\t MAXALIGN((kds_dst->usage - dest_usage_saved) /\n"
  "\t\t\t\t\t\t\t\t\t  kresults_src->nitems + 1));\n"
  "\t\t\t\tnitems_to_fit = kds_dst->length\n"
  "\t\t\t\t\t- STROMALIGN(offsetof(kern_data_store, colmeta[ncols]))\n"
  "\t\t\t\t\t- dest_usage_saved;\n"
  "\t\t\t\tnitems_to_fit /= width_avg;\n"
  "\t\t\t}\n"
  "\t\t\telse\n"
  "\t\t\t{\n"
  "\t\t\t\twidth_avg = (MAXALIGN((kds_dst->usage - dest_usage_saved) /\n"
  "\t\t\t\t\t\t\t\t\t  kresults_src->nitems + 1) +\n"
  "\t\t\t\t\t\t\t sizeof(cl_uint));\n"
  "\t\t\t\tnitems_to_fit = kds_dst->length\n"
  "\t\t\t\t\t- STROMALIGN(offsetof(kern_data_store, colmeta[ncols]))\n"
  "\t\t\t\t\t- STROMALIGN(sizeof(cl_uint) * kds_dst->nitems)\n"
  "\t\t\t\t\t- dest_usage_saved;\n"
  "\t\t\t\tnitems_to_fit /= width_avg;\n"
  "\t\t\t}\n"
  "\t\t\tnsplits = kresults_src->nitems / nitems_to_fit + 1;\n"
  "\t\t\tassert(nsplits > 1);\n"
  "\n"
  "\t\t\tvictim = gpujoin_resize_window(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t   kgjoin,\n"
  "\t\t\t\t\t\t\t\t\t\t   kmrels,\n"
  "\t\t\t\t\t\t\t\t\t\t   kds_src,\n"
  "\t\t\t\t\t\t\t\t\t\t   kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t   kresults_src,\n"
  "\t\t\t\t\t\t\t\t\t\t   nsplits,\n"
  "\t\t\t\t\t\t\t\t\t\t   dest_consumed,\n"
  "\t\t\t\t\t\t\t\t\t\t   kresults_src->kerror);\n"
  "\t\t\tif (victim < 0)\n"
  "\t\t\t\tgoto out;\n"
  "\n"
  "\t\t\tkgjoin->pfm.num_major_retry++;\n"
  "\t\t\tgoto retry_major;\n"
  "\t\t}\n"
  "\t\telse if (kresults_src->kerror.errcode != StromError_Success)\n"
  "\t\t{\n"
  "\t\t\tkcxt.e = kresults_src->kerror;\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t}\n"
  "\n"
  "\t/* OK, we could make up joined results on the kds_dst */\n"
  "\tkds_dst->nitems += kresults_src->nitems;\n"
  "\tassert(kds_dst->nitems <= kds_dst->nrooms);\n"
  "\tassert(dest_usage_saved <= kds_dst->usage);\n"
  "\n"
  "\t/*\n"
  "\t * - update statistics to be informed to the host side\n"
  "\t * - save the current position of window_base/size for give-up of\n"
  "\t *   the next try.\n"
  "\t */\n"
  "\tfor (depth = 0; depth <= kgjoin->num_rels; depth++)\n"
  "\t{\n"
  "\t\tjscale[depth].inner_nitems += jscale[depth].inner_nitems_stage;\n"
  "\t\tjscale[depth].right_nitems += jscale[depth].right_nitems_stage;\n"
  "\t\tjscale[depth].window_base_saved = jscale[depth].window_base;\n"
  "\t\tjscale[depth].window_size_saved = jscale[depth].window_size;\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * NOTE: If we still have enough space on the destination buffer, and\n"
  "\t * when virtual partition window is applied, it is worthful to retry\n"
  "\t * next join with new window_base/size, without returning to the user-\n"
  "\t * space once.\n"
  "\t *\n"
  "\t * Also note that we check error code here. It is valid check because\n"
  "\t * gpujoin_main() shall be in single-thread execution.\n"
  "\t */\n"
  "\tif (kgjoin->kerror.errcode == StromError_Success)\n"
  "\t{\n"
  "\t\tdest_consumed = (kds_dst->format == KDS_FORMAT_ROW\n"
  "\t\t\t\t\t\t ? (KERN_DATA_STORE_HEAD_LENGTH(kds_dst) +\n"
  "\t\t\t\t\t\t\tSTROMALIGN(sizeof(cl_uint) * kds_dst->nitems) +\n"
  "\t\t\t\t\t\t\tSTROMALIGN(kds_dst->usage))\n"
  "\t\t\t\t\t\t : (KERN_DATA_STORE_SLOT_LENGTH(kds_dst,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\tkds_dst->nitems) +\n"
  "\t\t\t\t\t\t\tSTROMALIGN(kds_dst->usage)));\n"
  "\t\tif (gpujoin_try_next_window(kgjoin,\n"
  "\t\t\t\t\t\t\t\t\tkmrels,\n"
  "\t\t\t\t\t\t\t\t\tkds_src,\n"
  "\t\t\t\t\t\t\t\t\tkds_dst,\n"
  "\t\t\t\t\t\t\t\t\tdest_consumed,\n"
  "\t\t\t\t\t\t\t\t\tkresults_src->nitems,\n"
  "\t\t\t\t\t\t\t\t\tkds_dst->usage - dest_usage_saved))\n"
  "\t\t{\n"
  "\t\t\tdest_usage_saved = kds_dst->usage;\n"
  "\t\t\tgoto retry_major;\n"
  "\t\t}\n"
  "\t}\n"
  "out:\n"
  "\tkern_writeback_error_status(&kgjoin->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "#endif\t/* __CUDACC__ */\n"
  "#endif\t/* CUDA_GPUJOIN_H */\n"
;
