const char *pgstrom_cuda_gpupreagg_code =
  "/*\n"
  " * cuda_gpupreagg.h\n"
  " *\n"
  " * Preprocess of aggregate using GPU acceleration, to reduce number of\n"
  " * rows to be processed by CPU; including the Sort reduction.\n"
  " * --\n"
  " * Copyright 2011-2016 (C) KaiGai Kohei <kaigai@kaigai.gr.jp>\n"
  " * Copyright 2014-2016 (C) The PG-Strom Development Team\n"
  " *\n"
  " * This program is free software; you can redistribute it and/or modify\n"
  " * it under the terms of the GNU General Public License version 2 as\n"
  " * published by the Free Software Foundation.\n"
  " *\n"
  " * This program is distributed in the hope that it will be useful,\n"
  " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
  " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
  " * GNU General Public License for more details.\n"
  " */\n"
  "#ifndef CUDA_GPUPREAGG_H\n"
  "#define CUDA_GPUPREAGG_H\n"
  "\n"
  "/*\n"
  " * Control data structure for GpuPreAgg kernel functions\n"
  " */\n"
  "#define GPUPREAGG_NOGROUP_REDUCTION\t\t1\n"
  "#define GPUPREAGG_LOCAL_REDUCTION\t\t2\n"
  "#define GPUPREAGG_GLOBAL_REDUCTION\t\t3\n"
  "#define GPUPREAGG_FINAL_REDUCTION\t\t4\n"
  "#define GPUPREAGG_ONLY_TERMINATION\t\t99\t/* used to urgent termination */\n"
  "\n"
  "/*\n"
  " * +--------------------+\n"
  " * | kern_gpupreagg     |\n"
  " * |     :              |\n"
  " * | kresults_1_offset -------+\n"
  " * | kresults_2_offset -----+ |\n"
  " * | +------------------+   | |\n"
  " * | | kern_parambuf    |   | |\n"
  " * | |    :             |   | |\n"
  " * +-+------------------+ <-|-+\n"
  " * | kern_resultbuf(1st)|   |\n"
  " * |      :             |   |\n"
  " * |      :             |   |\n"
  " * +--------------------+ <-+\n"
  " * | kern_resultbuf(2nd)|\n"
  " * |      :             |\n"
  " * |      :             |\n"
  " * +--------------------+\n"
  " */\n"
  "typedef struct\n"
  "{\n"
  "\tkern_errorbuf\tkerror;\t\t\t\t/* kernel error information */\n"
  "\tcl_uint\t\t\treduction_mode;\t\t/* one of GPUPREAGG_* above */\n"
  "\t/* -- runtime statistics -- */\n"
  "\tcl_uint\t\t\tnum_conflicts;\t\t/* only used in kernel space */\n"
  "\tcl_uint\t\t\tnum_groups;\t\t\t/* out: # of new groups */\n"
  "\tcl_uint\t\t\tvarlena_usage;\t\t/* out: size of varlena usage */\n"
  "\tcl_uint\t\t\tghash_conflicts;\t/* out: # of ghash conflicts */\n"
  "\tcl_uint\t\t\tfhash_conflicts;\t/* out: # of fhash conflicts */\n"
  "\t/* -- performance monitor -- */\n"
  "\tstruct {\n"
  "\t\tcl_uint\t\tnum_kern_prep;\t\t/* # of kern_preparation calls */\n"
  "\t\tcl_uint\t\tnum_kern_nogrp;\t\t/* # of kern_nogroup calls */\n"
  "\t\tcl_uint\t\tnum_kern_lagg;\t\t/* # of kern_local_reduction calls */\n"
  "\t\tcl_uint\t\tnum_kern_gagg;\t\t/* # of kern_global_reducation calls */\n"
  "\t\tcl_uint\t\tnum_kern_fagg;\t\t/* # of kern_final_reduction calls */\n"
  "\t\tcl_float\ttv_kern_prep;\t\t/* msec of kern_preparation */\n"
  "\t\tcl_float\ttv_kern_nogrp;\t\t/* msec of kern_nogroup */\n"
  "\t\tcl_float\ttv_kern_lagg;\t\t/* msec of kern_local_reduction */\n"
  "\t\tcl_float\ttv_kern_gagg;\t\t/* msec of kern_global_reducation */\n"
  "\t\tcl_float\ttv_kern_fagg;\t\t/* msec of kern_final_reduction */\n"
  "\t} pfm;\n"
  "\t/* -- other hashing parameters -- */\n"
  "\tcl_uint\t\t\tkey_dist_salt;\t\t\t/* hashkey distribution salt */\n"
  "\tcl_uint\t\t\thash_size;\t\t\t\t/* size of global hash-slots */\n"
  "\tcl_uint\t\t\tpg_crc32_table[256];\t/* master CRC32 table */\n"
  "\tcl_uint\t\t\tkresults_1_offset;\t\t/* offset to 1st kresults */\n"
  "\tcl_uint\t\t\tkresults_2_offset;\t\t/* offset to 2nd kresults */\n"
  "\tkern_parambuf\tkparams;\n"
  "\t/*\n"
  "\t * kern_resultbuf with nrels==1 shall be located next to kern_parambuf\n"
  "\t */\n"
  "} kern_gpupreagg;\n"
  "\n"
  "/* macro definitions to reference packed values */\n"
  "#define KERN_GPUPREAGG_PARAMBUF(kgpreagg)\t\t\t\t\\\n"
  "\t(&(kgpreagg)->kparams)\n"
  "#define KERN_GPUPREAGG_PARAMBUF_LENGTH(kgpreagg)\t\t\\\n"
  "\t((kgpreagg)->kparams.length)\n"
  "#define KERN_GPUPREAGG_1ST_RESULTBUF(kgpreagg)\t\t\t\\\n"
  "\t((kern_resultbuf *)\t\t\t\t\t\t\t\t\t\\\n"
  "\t ((char *)(kgpreagg) + (kgpreagg)->kresults_1_offset))\n"
  "#define KERN_GPUPREAGG_2ND_RESULTBUF(kgpreagg)\t\t\t\\\n"
  "\t((kern_resultbuf *)\t\t\t\t\t\t\t\t\t\\\n"
  "\t ((char *)(kgpreagg) + (kgpreagg)->kresults_2_offset))\n"
  "\n"
  "#define KERN_GPUPREAGG_LENGTH(kgpreagg)\t\t\t\t\t\\\n"
  "\t((uintptr_t)(kgpreagg)->kresults_1_offset +\t\t\t\\\n"
  "\t 2 * ((uintptr_t)(kgpreagg)->kresults_2_offset -\t\\\n"
  "\t\t  (uintptr_t)(kgpreagg)->kresults_1_offset))\n"
  "#define KERN_GPUPREAGG_DMASEND_OFFSET(kgpreagg)\t\t0\n"
  "#define KERN_GPUPREAGG_DMASEND_LENGTH(kgpreagg)\t\t\t\\\n"
  "\t(offsetof(kern_gpupreagg, kparams) +\t\t\t\t\\\n"
  "\t KERN_GPUPREAGG_PARAMBUF_LENGTH(kgpreagg))\n"
  "#define KERN_GPUPREAGG_DMARECV_OFFSET(kgpreagg)\t\t0\n"
  "#define KERN_GPUPREAGG_DMARECV_LENGTH(kgpreagg)\t\t\t\\\n"
  "\toffsetof(kern_gpupreagg, pg_crc32_table[0])\n"
  "\n"
  "/*\n"
  " * NOTE: hashtable of gpupreagg is an array of pagg_hashslot.\n"
  " * It contains a pair of hash value and get_local_id(0) of responsible\n"
  " * thread if local reduction, or index on the kern_data_store if global\n"
  " * reduction.\n"
  " * On hashtable construction phase, it fetches an item from the hash-\n"
  " * slot using hash % get_local_size(0) or hash % hash_size.\n"
  " * Then, if it is empty, thread put a pair of its hash value and either\n"
  " * get_local_id(0) or kds_index according to the context. If not empty,\n"
  " * reduction function tries to merge the value, or makes advance the\n"
  " * slot if grouping key is not same.\n"
  " */\n"
  "typedef union\n"
  "{\n"
  "\tcl_ulong\tvalue;\t/* for 64bit atomic operation */\n"
  "\tstruct {\n"
  "\t\tcl_uint\thash;\t/* hash value of the entry */\n"
  "\t\tcl_uint\tindex;\t/* loca/global thread-id that is responsible for */\n"
  "\t} s;\n"
  "} pagg_hashslot;\n"
  "\n"
  "/*\n"
  " * kern_global_hashslot\n"
  " *\n"
  " * An array of pagg_datum and its usage statistics, to be placed on\n"
  " * global memory area. Usage counter is used to break a loop to find-\n"
  " * out an empty slot if hash-slot is already filled-up.\n"
  " */\n"
  "#define GLOBAL_HASHSLOT_THRESHOLD(g_hashsize)\t\\\n"
  "\t((size_t)(0.75 * (double)(g_hashsize)))\n"
  "\n"
  "typedef struct\n"
  "{\n"
  "\tcl_uint\t\t\thash_usage;\t\t/* current number of hash_slot in use */\n"
  "\tcl_uint\t\t\thash_size;\t\t/* total size of the hash_slot below */\n"
  "\tpagg_hashslot\thash_slot[FLEXIBLE_ARRAY_MEMBER];\n"
  "} kern_global_hashslot;\n"
  "\n"
  "/*\n"
  " * NOTE: pagg_datum is a set of information to calculate running total.\n"
  " * group_id indicates which group does this work-item belong to, instead\n"
  " * of gpupreagg_keymatch().\n"
  " * isnull indicates whether the current running total is NULL, or not.\n"
  " * XXX_val is a running total itself.\n"
  " */\n"
  "typedef struct\n"
  "{\n"
  "\tcl_int\t\t\tisnull;\n"
  "\tcl_char\t\t\t__padding__[4];\n"
  "\tunion {\n"
  "\t\tcl_short\tshort_val;\n"
  "\t\tcl_ushort\tushort_val;\n"
  "\t\tcl_int\t\tint_val;\n"
  "\t\tcl_uint\t\tuint_val;\n"
  "\t\tcl_long\t\tlong_val;\n"
  "\t\tcl_ulong\tulong_val;\n"
  "\t\tcl_float\tfloat_val;\n"
  "\t\tcl_double\tdouble_val;\n"
  "\t};\n"
  "} pagg_datum;\n"
  "\n"
  "/*\n"
  " * definition for special system parameter\n"
  " *\n"
  " * KPARAM_0 - array of the GPUPREAGG_FIELD_IS_* flags as cl_char[] array.\n"
  " * Each item informs usage of the related field.\n"
  " */\n"
  "#define GPUPREAGG_FIELD_IS_GROUPKEY\t\t1\n"
  "#define GPUPREAGG_FIELD_IS_AGGFUNC\t\t2\n"
  "\n"
  "#ifdef __CUDACC__\n"
  "\n"
  "/*\n"
  " * hash value calculation function - to be generated by PG-Strom on the fly\n"
  " */\n"
  "STATIC_FUNCTION(cl_uint)\n"
  "gpupreagg_hashvalue(kern_context *kcxt,\n"
  "\t\t\t\t\tcl_uint *crc32_table,\t/* __shared__ memory */\n"
  "\t\t\t\t\tcl_uint hash_value,\n"
  "\t\t\t\t\tkern_data_store *kds,\n"
  "\t\t\t\t\tsize_t kds_index);\n"
  "\n"
  "/*\n"
  " * comparison function - to be generated by PG-Strom on the fly\n"
  " *\n"
  " * It compares two records indexed by 'x_index' and 'y_index' on the supplied\n"
  " * kern_data_store, then returns -1 if record[X] is less than record[Y],\n"
  " * 0 if record[X] is equivalent to record[Y], or 1 if record[X] is greater\n"
  " * than record[Y].\n"
  " * (auto generated function)\n"
  " */\n"
  "STATIC_FUNCTION(cl_bool)\n"
  "gpupreagg_keymatch(kern_context *kcxt,\n"
  "\t\t\t\t   kern_data_store *x_kds, size_t x_index,\n"
  "\t\t\t\t   kern_data_store *y_kds, size_t y_index);\n"
  "\n"
  "/*\n"
  " * local calculation function - to be generated by PG-Strom on the fly\n"
  " *\n"
  " * It aggregates the newval to accum using atomic operation on the\n"
  " * local pagg_datum array\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_local_calc(kern_context *kcxt,\n"
  "\t\t\t\t\t cl_int attnum,\n"
  "\t\t\t\t\t pagg_datum *accum,\n"
  "\t\t\t\t\t pagg_datum *newval);\n"
  "\n"
  "/*\n"
  " * global calculation function - to be generated by PG-Strom on the fly\n"
  " *\n"
  " * It also aggregates the newval to accum using atomic operation on\n"
  " * the global kern_data_store\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_global_calc(kern_context *kcxt,\n"
  "\t\t\t\t\t  cl_int attnum,\n"
  "\t\t\t\t\t  kern_data_store *accum_kds,  size_t accum_index,\n"
  "\t\t\t\t\t  kern_data_store *newval_kds, size_t newval_index);\n"
  "\n"
  "/*\n"
  " * Reduction operation with no atomic operations. It can be used if no\n"
  " * GROUP-BY clause is given, because all the rows shall be eventually\n"
  " * consolidated into one aggregated row.\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_nogroup_calc(kern_context *kcxt,\n"
  "\t\t\t\t\t   cl_int attnum,\n"
  "\t\t\t\t\t   pagg_datum *accum,\n"
  "\t\t\t\t\t   pagg_datum *newval);\n"
  "\n"
  "/*\n"
  " * translate a kern_data_store (input) into an output form\n"
  " * (auto generated function)\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_projection(kern_context *kcxt,\n"
  "\t\t\t\t\t kern_data_store *kds_src,\t/* in */\n"
  "\t\t\t\t\t kern_tupitem *tupitem,\t\t/* in */\n"
  "\t\t\t\t\t kern_data_store *kds_dst,\t/* out */\n"
  "\t\t\t\t\t Datum *dst_values,\t\t\t/* out */\n"
  "\t\t\t\t\t cl_char *dst_isnull);\t\t/* out */\n"
  "\n"
  "/*\n"
  " * check qualifiers being pulled-up from the outer relation.\n"
  " * if not valid, this record shall not be processed.\n"
  " */\n"
  "STATIC_FUNCTION(bool)\n"
  "gpupreagg_qual_eval(kern_context *kcxt,\n"
  "\t\t\t\t\tkern_data_store *kds,\n"
  "\t\t\t\t\tsize_t kds_index);\n"
  "\n"
  "/*\n"
  " * load the data from kern_data_store to pagg_datum structure\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_data_load(pagg_datum *pdatum,\t\t/* __shared__ */\n"
  "\t\t\t\t\tkern_context *kcxt,\n"
  "\t\t\t\t\tkern_data_store *kds,\n"
  "\t\t\t\t\tcl_uint colidx, cl_uint rowidx)\n"
  "{\n"
  "\tkern_colmeta\tcmeta;\n"
  "\tDatum\t\t   *values;\n"
  "\tcl_char\t\t   *isnull;\n"
  "\n"
  "\tassert(kds->format == KDS_FORMAT_SLOT);\n"
  "\tassert(colidx < kds->ncols);\n"
  "\n"
  "\tcmeta = kds->colmeta[colidx];\n"
  "\tvalues = KERN_DATA_STORE_VALUES(kds,rowidx);\n"
  "\tisnull = KERN_DATA_STORE_ISNULL(kds,rowidx);\n"
  "\n"
  "\t/*\n"
  "\t * Right now, expected data length for running total of partial\n"
  "\t * aggregates are 2, 4, or 8. Elasewhere, it may be a bug.\n"
  "\t */\n"
  "\tif (cmeta.attlen == sizeof(cl_short) ||\t\t/* also, cl_short */\n"
  "\t\tcmeta.attlen == sizeof(cl_int))\t\t\t/* also, cl_float */\n"
  "\t{\n"
  "\t\tpdatum->isnull = isnull[colidx];\n"
  "\t\tpdatum->int_val = (cl_int)(values[colidx] & 0xffffffffUL);\n"
  "\t}\n"
  "\telse if (cmeta.attlen == sizeof(cl_long) ||\t/* also, cl_double */\n"
  "\t\t\t cmeta.atttypid == PG_NUMERICOID)\t/* internal of numeric */\n"
  "\t{\n"
  "\t\tpdatum->isnull = isnull[colidx];\n"
  "\t\tpdatum->long_val = (cl_long)values[colidx];\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt->e, StromError_DataStoreCorruption);\n"
  "\t}\n"
  "}\n"
  "\n"
  "/*\n"
  " * store the data from pagg_datum structure to kern_data_store\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_data_store(pagg_datum *pdatum,\t/* __shared__ */\n"
  "\t\t\t\t\t kern_context *kcxt,\n"
  "\t\t\t\t\t kern_data_store *kds,\n"
  "\t\t\t\t\t cl_uint colidx, cl_uint rowidx)\n"
  "{\n"
  "\tkern_colmeta\tcmeta;\n"
  "\tDatum\t\t   *values;\n"
  "\tcl_char\t\t   *isnull;\n"
  "\n"
  "\tassert(kds->format == KDS_FORMAT_SLOT);\n"
  "\tassert(colidx < kds->ncols);\n"
  "\n"
  "\tcmeta = kds->colmeta[colidx];\n"
  "\tvalues = KERN_DATA_STORE_VALUES(kds,rowidx);\n"
  "\tisnull = KERN_DATA_STORE_ISNULL(kds,rowidx);\n"
  "\n"
  "\t/*\n"
  "\t * Right now, expected data length for running total of partial\n"
  "\t * aggregates are 2, 4, or 8. Elasewhere, it may be a bug.\n"
  "\t */\n"
  "\tif (cmeta.attlen == sizeof(cl_short))\n"
  "\t{\n"
  "\t\tisnull[colidx] = pdatum->isnull;\n"
  "\t\tvalues[colidx] = pdatum->int_val;\n"
  "\t}\n"
  "\telse if (cmeta.attlen == sizeof(cl_int))\t/* also, cl_float */\n"
  "\t{\n"
  "\t\tisnull[colidx] = pdatum->isnull;\n"
  "\t\tvalues[colidx] = pdatum->int_val;\n"
  "\t}\n"
  "\telse if (cmeta.attlen == sizeof(cl_long) ||\t/* also, cl_double */\n"
  "\t\t\t cmeta.atttypid == PG_NUMERICOID)\t/* internal of numeric */\n"
  "\t{\n"
  "\t\tisnull[colidx] = pdatum->isnull;\n"
  "\t\tvalues[colidx] = pdatum->long_val;\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt->e, StromError_DataStoreCorruption);\n"
  "\t}\n"
  "}\n"
  "\n"
  "/* gpupreagg_data_move - it moves grouping key from the source kds to\n"
  " * the destination kds as is. We assume toast buffer is shared and\n"
  " * resource number of varlena key is not changed. So, all we need to\n"
  " * do is copying the offset value, not varlena body itself.\n"
  " */\n"
  "STATIC_FUNCTION(void)\n"
  "gpupreagg_data_move(kern_context *kcxt,\n"
  "\t\t\t\t\tcl_uint colidx,\n"
  "\t\t\t\t\tkern_data_store *kds_src, cl_uint rowidx_src,\n"
  "\t\t\t\t\tkern_data_store *kds_dst, cl_uint rowidx_dst)\n"
  "{\n"
  "\tDatum\t   *src_values;\n"
  "\tDatum\t   *dst_values;\n"
  "\tcl_char\t   *src_isnull;\n"
  "\tcl_char\t   *dst_isnull;\n"
  "\n"
  "\t/*\n"
  "\t * XXX - Paranoire checks?\n"
  "\t */\n"
  "\tif (kds_src->format != KDS_FORMAT_SLOT ||\n"
  "\t\tkds_dst->format != KDS_FORMAT_SLOT)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt->e, StromError_DataStoreCorruption);\n"
  "\t\treturn;\n"
  "\t}\n"
  "\tif (colidx >= kds_src->ncols || colidx >= kds_dst->ncols)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt->e, StromError_DataStoreCorruption);\n"
  "\t\treturn;\n"
  "\t}\n"
  "\tsrc_values = KERN_DATA_STORE_VALUES(kds_src, rowidx_src);\n"
  "\tsrc_isnull = KERN_DATA_STORE_ISNULL(kds_src, rowidx_src);\n"
  "\tdst_values = KERN_DATA_STORE_VALUES(kds_dst, rowidx_dst);\n"
  "\tdst_isnull = KERN_DATA_STORE_ISNULL(kds_dst, rowidx_dst);\n"
  "\n"
  "\tdst_isnull[colidx] = src_isnull[colidx];\n"
  "\tdst_values[colidx] = src_values[colidx];\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_final_data_move\n"
  " *\n"
  " * It moves the value from source buffer to destination buffer. If it needs\n"
  " * to allocate variable-length buffer, it expands extra area of the final\n"
  " * buffer and returns allocated area.\n"
  " */\n"
  "STATIC_FUNCTION(cl_uint)\n"
  "gpupreagg_final_data_move(kern_context *kcxt,\n"
  "\t\t\t\t\t\t  kern_data_store *kds_src, cl_uint rowidx_src,\n"
  "\t\t\t\t\t\t  kern_data_store *kds_dst, cl_uint rowidx_dst)\n"
  "{\n"
  "\tDatum\t   *src_values = KERN_DATA_STORE_VALUES(kds_src, rowidx_src);\n"
  "\tDatum\t   *dst_values = KERN_DATA_STORE_VALUES(kds_dst, rowidx_dst);\n"
  "\tcl_char\t   *src_isnull = KERN_DATA_STORE_ISNULL(kds_src, rowidx_src);\n"
  "\tcl_char\t   *dst_isnull = KERN_DATA_STORE_ISNULL(kds_dst, rowidx_dst);\n"
  "\tcl_uint\t\ti, ncols = kds_src->ncols;\n"
  "\tcl_uint\t\talloc_size = 0;\n"
  "\tchar\t   *alloc_ptr = NULL;\n"
  "\n"
  "\t/* Paranoire checks? */\n"
  "\tassert(kds_src->format == KDS_FORMAT_SLOT &&\n"
  "\t\t   kds_dst->format == KDS_FORMAT_SLOT);\n"
  "\tassert(kds_src->ncols == kds_dst->ncols);\n"
  "\tassert(rowidx_src < kds_src->nitems);\n"
  "\tassert(rowidx_dst < kds_dst->nitems);\n"
  "\n"
  "\t/* size for allocation */\n"
  "\tfor (i=0; i < ncols; i++)\n"
  "\t{\n"
  "\t\tkern_colmeta cmeta = kds_src->colmeta[i];\n"
  "\n"
  "\t\tif (src_isnull[i])\n"
  "\t\t\tcontinue;\t/* no buffer needed */\n"
  "\t\tif (cmeta.atttypid == PG_NUMERICOID)\n"
  "\t\t\tcontinue;\t/* special internal data format */\n"
  "\t\tif (!cmeta.attbyval)\n"
  "\t\t{\n"
  "\t\t\tif (cmeta.attlen > 0)\n"
  "\t\t\t\talloc_size += MAXALIGN(cmeta.attlen);\n"
  "\t\t\telse\n"
  "\t\t\t{\n"
  "\t\t\t\tchar   *datum = DatumGetPointer(src_values[i]);\n"
  "\n"
  "\t\t\t\talloc_size += MAXALIGN(VARSIZE_ANY(datum));\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t}\n"
  "\t/* allocate extra buffer by atomic operation */\n"
  "\tif (alloc_size > 0)\n"
  "\t{\n"
  "\t\tcl_uint\t\tusage_prev = atomicAdd(&kds_dst->usage, alloc_size);\n"
  "\n"
  "\t\tif (KERN_DATA_STORE_SLOT_LENGTH(kds_dst, kds_dst->nrooms) +\n"
  "\t\t\tusage_prev + alloc_size >= kds_dst->length)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt->e, StromError_DataStoreNoSpace);\n"
  "\t\t\t/*\n"
  "\t\t\t * NOTE: Uninitialized dst_values[] for NULL values will lead\n"
  "\t\t\t * a problem around atomic operation, because we designed \n"
  "\t\t\t * reduction operation that assumes values are correctly\n"
  "\t\t\t * initialized even if it is NULL.\n"
  "\t\t\t */\n"
  "\t\t\tfor (i=0; i < ncols; i++)\n"
  "\t\t\t{\n"
  "\t\t\t\tdst_isnull[i] = true;\n"
  "\t\t\t\tdst_values[i] = src_values[i];\n"
  "\t\t\t}\n"
  "\t\t\treturn 0;\n"
  "\t\t}\n"
  "\t\talloc_ptr = ((char *)kds_dst + kds_dst->length -\n"
  "\t\t\t\t\t (usage_prev + alloc_size));\n"
  "\t}\n"
  "\t/* move the data */\n"
  "\tfor (i=0; i < ncols; i++)\n"
  "\t{\n"
  "\t\tkern_colmeta cmeta = kds_src->colmeta[i];\n"
  "\n"
  "\t\tif (src_isnull[i])\n"
  "\t\t{\n"
  "\t\t\tdst_isnull[i] = true;\n"
  "\t\t\tdst_values[i] = src_values[i];\n"
  "\t\t}\n"
  "\t\telse if (cmeta.atttypid == PG_NUMERICOID ||\n"
  "\t\t\t\t cmeta.attbyval)\n"
  "\t\t{\n"
  "\t\t\tdst_isnull[i] = false;\n"
  "\t\t\tdst_values[i] = src_values[i];\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\tchar\t   *datum = DatumGetPointer(src_values[i]);\n"
  "\t\t\tcl_uint\t\tdatum_len = (cmeta.attlen > 0 ?\n"
  "\t\t\t\t\t\t\t\t\t cmeta.attlen :\n"
  "\t\t\t\t\t\t\t\t\t VARSIZE_ANY(datum));\n"
  "\t\t\tmemcpy(alloc_ptr, datum, datum_len);\n"
  "\t\t\tdst_isnull[i] = false;\n"
  "\t\t\tdst_values[i] = PointerGetDatum(alloc_ptr);\n"
  "\n"
  "\t\t\talloc_ptr += MAXALIGN(datum_len);\n"
  "\t\t}\n"
  "\t}\n"
  "\treturn alloc_size;\n"
  "}\n"
  "\n"
  "\n"
  "/*\n"
  " * gpupreagg_preparation - It translaes an input kern_data_store (that\n"
  " * reflects outer relation's tupdesc) into the form of running total\n"
  " * and final result of gpupreagg (that reflects target-list of GpuPreAgg).\n"
  " *\n"
  " * Pay attention on a case when the kern_data_store with row-format is\n"
  " * translated. Row-format does not have toast buffer because variable-\n"
  " * length fields are in-place. gpupreagg_projection() treats the input\n"
  " * kern_data_store as toast buffer of the later stage. So, caller has to\n"
  " * give this kern_data_store (never used for data-store in the later\n"
  " * stage) as toast buffer if the source kds has row-format.\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpupreagg_preparation(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t\t\t  kern_data_store *kds_in,\t/* in: KDS_FORMAT_ROW */\n"
  "\t\t\t\t\t  kern_data_store *kds_src,\t/* out: KDS_FORMAT_SLOT */\n"
  "\t\t\t\t\t  kern_global_hashslot *g_hash)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_context\tkcxt;\n"
  "\tkern_tupitem   *tupitem;\n"
  "\tcl_uint\t\t\toffset;\n"
  "\tcl_uint\t\t\tcount;\n"
  "\tsize_t\t\t\tkds_index = get_global_id();\n"
  "\tsize_t\t\t\thash_size;\n"
  "\tsize_t\t\t\thash_index;\n"
  "\t__shared__ cl_uint base;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpupreagg_preparation,kparams);\n"
  "\n"
  "\t/* sanity checks */\n"
  "\tassert(kgpreagg->key_dist_salt > 0);\n"
  "\tassert(kds_in->format == KDS_FORMAT_ROW);\n"
  "\tassert(kds_src->format == KDS_FORMAT_SLOT);\n"
  "\n"
  "\t/* init global hash slot */\n"
  "\thash_size = kgpreagg->hash_size;;\n"
  "\tif (get_global_id() == 0)\n"
  "\t{\n"
  "\t\tg_hash->hash_usage = 0;\n"
  "\t\tg_hash->hash_size = hash_size;\n"
  "\t}\n"
  "\tfor (hash_index = get_global_id();\n"
  "\t\t hash_index < hash_size;\n"
  "\t\t hash_index += get_global_size())\n"
  "\t{\n"
  "\t\tg_hash->hash_slot[hash_index].s.hash = 0;\n"
  "\t\tg_hash->hash_slot[hash_index].s.index = (cl_uint)(0xffffffff);\n"
  "\t}\n"
  "\n"
  "\t/* check qualifiers */\n"
  "\tif (kds_index < kds_in->nitems &&\n"
  "\t\tgpupreagg_qual_eval(&kcxt, kds_in, kds_index))\n"
  "\t\ttupitem = KERN_DATA_STORE_TUPITEM(kds_in, kds_index);\n"
  "\telse\n"
  "\t\ttupitem = NULL;\t\t/* not a visible tuple */\n"
  "\n"
  "\t/* calculation of total number of rows to be processed in this work-\n"
  "\t * group.\n"
  "\t */\n"
  "\toffset = pgstromStairlikeSum(tupitem != NULL ? 1 : 0, &count);\n"
  "\n"
  "\t/* Allocation of the result slot on the kds_src. */\n"
  "\tif (get_local_id() == 0)\n"
  "\t{\n"
  "\t\tif (count > 0)\n"
  "\t\t\tbase = atomicAdd(&kds_src->nitems, count);\n"
  "\t\telse\n"
  "\t\t\tbase = 0;\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/* GpuPreAgg should never increase number of items */\n"
  "\tassert(base + count <= kds_src->nrooms);\n"
  "\n"
  "\t/* do projection */\n"
  "\tif (tupitem != NULL)\n"
  "\t{\n"
  "\t\tcl_uint\t\tdst_index = base + offset;\n"
  "\t\tDatum\t   *dst_values = KERN_DATA_STORE_VALUES(kds_src, dst_index);\n"
  "\t\tcl_char\t   *dst_isnull = KERN_DATA_STORE_ISNULL(kds_src, dst_index);\n"
  "\n"
  "\t\tgpupreagg_projection(&kcxt,\n"
  "\t\t\t\t\t\t\t kds_in,\t\t/* input kds */\n"
  "\t\t\t\t\t\t\t tupitem,\t\t/* input tuple */\n"
  "\t\t\t\t\t\t\t kds_src,\t\t/* destination kds */\n"
  "\t\t\t\t\t\t\t dst_values,\t/* destination values[] array */\n"
  "\t\t\t\t\t\t\t dst_isnull);\t/* destination isnull[] array */\n"
  "\t}\n"
  "\t/* write-back execution status into host-side */\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_nogroup_reduction\n"
  " *\n"
  " * It makes aggregation if no GROUP-BY clause given. We can omit atomic-\n"
  " * operations in this case, because all the rows are eventually consolidated\n"
  " * to just one record, thus usual reduction operation is sufficient.\n"
  " */\n"
  "KERNEL_FUNCTION_MAXTHREADS(void)\n"
  "gpupreagg_nogroup_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t\t\t\t\tkern_data_store *kds_slot,\n"
  "\t\t\t\t\t\t\tkern_resultbuf *kresults_src,\n"
  "\t\t\t\t\t\t\tkern_resultbuf *kresults_dst)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_context\tkcxt;\n"
  "\tvarlena\t\t   *kparam_0 = (varlena *)kparam_get_value(kparams, 0);\n"
  "\tcl_char\t\t   *gpagg_atts = (cl_char *)VARDATA(kparam_0);\n"
  "\tpagg_datum\t   *l_datum = SHARED_WORKMEM(pagg_datum);\n"
  "\tcl_uint\t\t\ti, ncols = kds_slot->ncols;\n"
  "\tcl_uint\t\t\tnvalids = 0;\n"
  "\tcl_uint\t\t\tkds_index = UINT_MAX;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt, gpupreagg_nogroup_reduction, kparams);\n"
  "\tif (kresults_src->all_visible)\n"
  "\t{\n"
  "\t\t/* scope of this block */\n"
  "\t\tif (get_global_base() < kds_slot->nitems)\n"
  "\t\t\tnvalids = min(kds_slot->nitems - get_global_base(),\n"
  "\t\t\t\t\t\t  get_local_size());\n"
  "\t\telse\n"
  "\t\t\tgoto out;\t/* should not happen */\n"
  "\n"
  "\t\t/* global index on the kds_slot */\n"
  "\t\tif (get_local_id() < nvalids)\n"
  "\t\t\tkds_index = get_global_base() + get_local_id();\n"
  "\t\t__syncthreads();\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\t/* scope of this block */\n"
  "\t\tif (get_global_base() < kresults_src->nitems)\n"
  "\t\t\tnvalids = min(kresults_src->nitems - get_global_base(),\n"
  "\t\t\t\t\t\t  get_local_size());\n"
  "\t\telse\n"
  "\t\t\tgoto out;\t/* should not happen */\n"
  "\n"
  "\t\t/* global index on the kds_slot */\n"
  "\n"
  "\t\t/* MEMO: On the CUDA 7.5 NVRTC, we met a strange behavior.\n"
  "\t\t * If and when a part of threads in warp didn't fit nvalids,\n"
  "\t\t * it is ought to ignore the following if-then block unless\n"
  "\t\t * else-block. At that time, our test workloads has nvalids=196,\n"
  "\t\t * thus 4 threads match the if-condition below.\n"
  "\t\t * According to the observation, these 4 threads didn't resume\n"
  "\t\t * until exit of any other threads (thread 0-191 and 196-223!).\n"
  "\t\t * Thus, final result lacks the values to be accumulated by the\n"
  "\t\t * 4 threads. This strange behavior was eliminated if we added\n"
  "\t\t * an else-block. It seems to me a bug of NVRTC 7.5?\n"
  "\t\t *\n"
  "\t\t * MEMO: additional investigation - This problem happens on\n"
  "\t\t * GTX980 (GM104) but didn't happen in Tesla K20c (GK110).\n"
  "\t\t * Here is no difference in the PTX image, expect for .target\n"
  "\t\t * directive. One strange is, the reduction loop below contains\n"
  "\t\t * three __syncthreads(), but I could find only two bar.sync\n"
  "\t\t * operation in this function. So, I doubt the threads out of\n"
  "\t\t * range unexpectedly reached end of the function, then it\n"
  "\t\t * made H/W synchronization mechanism confused.\n"
  "\t\t *\n"
  "\t\t * As a workaround, I could find an extra '__syncthreads()'\n"
  "\t\t * just after that if-block below can hide the problem.\n"
  "\t\t * However, it shouldn't be necessary\n"
  "\t\t *\n"
  "\t\t * This strange behavior was reported to NVIDIA at 22-03-2016,\n"
  "\t\t * as a bug report bug#160322-000527. The PG-Strom development\n"
  "\t\t * team is now waiting for their response.\n"
  "\t\t */\n"
  "\t\tif (get_local_id() < nvalids)\n"
  "\t\t\tkds_index = kresults_src->results[get_global_id()];\n"
  "\t\t__syncthreads();\n"
  "\t}\n"
  "\n"
  "\t/* loop for each columns */\n"
  "\tfor (i=0; i < ncols; i++)\n"
  "\t{\n"
  "\t\tsize_t\t\tdist;\n"
  "\n"
  "\t\t/* if not GPUPREAGG_FIELD_IS_AGGFUNC, do nothing */\n"
  "\t\tif (gpagg_atts[i] != GPUPREAGG_FIELD_IS_AGGFUNC)\n"
  "\t\t\tcontinue;\n"
  "\n"
  "\t\t/* load this value from kds_slot onto pagg_datum */\n"
  "\t\tif (get_local_id() < nvalids)\n"
  "\t\t{\n"
  "\t\t\tgpupreagg_data_load(l_datum + get_local_id(),\n"
  "\t\t\t\t\t\t\t\t&kcxt, kds_slot, i, kds_index);\n"
  "\t\t}\n"
  "\n"
  "\t\t/* do reduction */\n"
  "\t\tfor (dist = 2; dist < 2 * nvalids; dist *= 2)\n"
  "\t\t{\n"
  "\t\t\tif ((get_local_id() % dist) == 0 &&\n"
  "\t\t\t\t(get_local_id() + dist / 2) < nvalids)\n"
  "\t\t\t{\n"
  "\t\t\t\tgpupreagg_nogroup_calc(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t   i,\n"
  "\t\t\t\t\t\t\t\t\t   l_datum + get_local_id(),\n"
  "\t\t\t\t\t\t\t\t\t   l_datum + get_local_id() + dist / 2);\n"
  "\t\t\t}\n"
  "\t\t\t__syncthreads();\n"
  "\t\t}\n"
  "\n"
  "\t\t/* store this value to kds_dst from datum */\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t{\n"
  "\t\t\tgpupreagg_data_store(l_datum + get_local_id(),\n"
  "\t\t\t\t\t\t\t\t &kcxt, kds_slot, i, kds_index);\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\t}\n"
  "\n"
  "\t/* update kresults_dst */\n"
  "\tif (get_local_id() == 0)\n"
  "\t{\n"
  "\t\tcl_uint\t\tdest_index\n"
  "\t\t\t= atomicAdd(&kresults_dst->nitems, 1);\n"
  "\t\tassert(dest_index < kresults_dst->nrooms);\n"
  "\t\tkresults_dst->results[dest_index] = kds_index;\n"
  "\t}\n"
  "out:\n"
  "\t/* write-back execution status into host-side */\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_local_reduction\n"
  " */\n"
  "KERNEL_FUNCTION_MAXTHREADS(void)\n"
  "gpupreagg_local_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t\t\t\t  kern_data_store *kds_slot,\n"
  "\t\t\t\t\t\t  kern_resultbuf *kresults)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_context\tkcxt;\n"
  "\tvarlena\t\t   *kparam_0 = (varlena *)kparam_get_value(kparams, 0);\n"
  "\tcl_char\t\t   *gpagg_atts = (cl_char *) VARDATA(kparam_0);\n"
  "\tsize_t\t\t\thash_size = 2 * get_local_size();\n"
  "\tcl_uint\t\t\towner_index;\n"
  "\tcl_uint\t\t\tkey_dist_salt = kgpreagg->key_dist_salt;\n"
  "\tcl_uint\t\t\tkey_dist_index = 0;\n"
  "\tcl_uint\t\t\thash_value;\n"
  "\tcl_uint\t\t\thash_value_base;\n"
  "\tcl_uint\t\t\tnitems = kds_slot->nitems;\n"
  "\tcl_uint\t\t\ti, ncols = kds_slot->ncols;\n"
  "\tcl_uint\t\t\tcount;\n"
  "\tcl_uint\t\t\tindex;\n"
  "\tpg_int4_t\t\tkey_dist_factor;\n"
  "\tpagg_hashslot\told_slot;\n"
  "\tpagg_hashslot\tnew_slot;\n"
  "\tpagg_hashslot\tcur_slot;\n"
  "\tpagg_datum\t   *l_datum;\n"
  "\tpagg_hashslot  *l_hashslot;\n"
  "\t__shared__ cl_uint\tcrc32_table[256];\n"
  "\t__shared__ size_t\tbase;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpupreagg_local_reduction,kparams);\n"
  "\n"
  "\t/*\n"
  "\t * calculation of the hash value of grouping keys in this record.\n"
  "\t * It tends to take massive amount of random access on global memory,\n"
  "\t * so it makes performance advantage to move the master table from\n"
  "\t * gloabl to the local memory first.\n"
  "\t */\n"
  "\tfor (index = get_local_id();\n"
  "\t\t index < lengthof(kgpreagg->pg_crc32_table);\n"
  "\t\t index += get_local_size())\n"
  "\t\tcrc32_table[index] = kgpreagg->pg_crc32_table[index];\n"
  "\t__syncthreads();\n"
  "\n"
  "\tINIT_LEGACY_CRC32(hash_value);\n"
  "\tif (get_global_id() < nitems)\n"
  "\t\thash_value = gpupreagg_hashvalue(&kcxt, crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\t hash_value,\n"
  "\t\t\t\t\t\t\t\t\t\t kds_slot,\n"
  "\t\t\t\t\t\t\t\t\t\t get_global_id());\n"
  "\thash_value_base = hash_value;\n"
  "\tif (key_dist_salt > 1)\n"
  "\t{\n"
  "\t\tkey_dist_factor.isnull = false;\n"
  "\t\tkey_dist_factor.value = (get_global_id() % key_dist_salt);\n"
  "\t\thash_value = pg_int4_comp_crc32(crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\thash_value,\n"
  "\t\t\t\t\t\t\t\t\t\tkey_dist_factor);\n"
  "\t}\n"
  "\tFIN_LEGACY_CRC32(hash_value);\n"
  "\n"
  "\t/*\n"
  "\t * Find a hash-slot to determine the item index that represents\n"
  "\t * a particular group-keys.\n"
  "\t * The array of global hash-slot should be initialized to 'all\n"
  "\t * empty' state on the projection kernel.\n"
  "\t * one will take a place using atomic operation. Then. here are\n"
  "\t * two cases for hash conflicts; case of same grouping-key, or\n"
  "\t * case of different grouping-key but same hash-value.\n"
  "\t * The first conflict case informs us the item-index responsible\n"
  "\t * to the grouping key. We cannot help the later case, so retry\n"
  "\t * the steps with next hash-slot.\n"
  "\t */\n"
  "\tl_hashslot = SHARED_WORKMEM(pagg_hashslot);\n"
  "\tfor (index = get_local_id();\n"
  "\t\t index < hash_size;\n"
  "\t\t index += get_local_size())\n"
  "\t{\n"
  "\t\tl_hashslot[index].s.hash = 0;\n"
  "\t\tl_hashslot[index].s.index = (cl_uint)(0xffffffff);\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\n"
  "\tif (get_global_id() < nitems)\n"
  "\t{\n"
  "\tretry_major:\n"
  "\t\tnew_slot.s.hash = hash_value;\n"
  "\t\tnew_slot.s.index = get_local_id();\n"
  "\t\told_slot.s.hash = 0;\n"
  "\t\told_slot.s.index = (cl_uint)(0xffffffff);\n"
  "\t\tindex = hash_value % hash_size;\n"
  "\tretry_minor:\n"
  "\t\tcur_slot.value = atomicCAS(&l_hashslot[index].value,\n"
  "\t\t\t\t\t\t\t\t   old_slot.value,\n"
  "\t\t\t\t\t\t\t\t   new_slot.value);\n"
  "\t\tif (cur_slot.value == old_slot.value)\n"
  "\t\t{\n"
  "\t\t\t/* Hash slot was empty, so this thread shall be responsible\n"
  "\t\t\t * to this grouping-key.\n"
  "\t\t\t */\n"
  "\t\t\towner_index = new_slot.s.index;\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\tsize_t\tbuddy_index\n"
  "\t\t\t\t= (get_global_id() - get_local_id() + cur_slot.s.index);\n"
  "\n"
  "\t\t\tif (cur_slot.s.hash == new_slot.s.hash &&\n"
  "\t\t\t\tgpupreagg_keymatch(&kcxt,\n"
  "\t\t\t\t\t\t\t\t   kds_slot, get_global_id(),\n"
  "\t\t\t\t\t\t\t\t   kds_slot, buddy_index))\n"
  "\t\t\t{\n"
  "\t\t\t\towner_index = cur_slot.s.index;\n"
  "\t\t\t}\n"
  "\t\t\telse\n"
  "\t\t\t{\n"
  "\t\t\t\tif (key_dist_salt > 1 && ++key_dist_index < key_dist_salt)\n"
  "\t\t\t\t{\n"
  "\t\t\t\t\thash_value = hash_value_base;\n"
  "\t\t\t\t\tkey_dist_factor.isnull = false;\n"
  "\t\t\t\t\tkey_dist_factor.value =\n"
  "\t\t\t\t\t\t(get_global_id() + key_dist_index) % key_dist_salt;\n"
  "\t\t\t\t\thash_value = pg_int4_comp_crc32(crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\thash_value,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tkey_dist_factor);\n"
  "\t\t\t\t\tFIN_LEGACY_CRC32(hash_value);\n"
  "\t\t\t\t\tgoto retry_major;\n"
  "\t\t\t\t}\n"
  "\t\t\t\tindex = (index + 1) % hash_size;\n"
  "\t\t\t\tgoto retry_minor;\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t\towner_index = (cl_uint)(0xffffffff);\n"
  "    __syncthreads();\n"
  "\n"
  "\t/*\n"
  "\t * Make a reservation on the destination kern_data_store\n"
  "\t * Only thread that is responsible to grouping-key (also, it shall\n"
  "\t * have same hash-index with get_local_id(0)) takes a place on the\n"
  "\t * destination kern_data_store.\n"
  "\t */\n"
  "\tindex = pgstromStairlikeSum(get_local_id() == owner_index ? 1 : 0,\n"
  "\t\t\t\t\t\t\t\t&count);\n"
  "\tif (get_local_id() == 0)\n"
  "\t\tbase = atomicAdd(&kresults->nitems, count);\n"
  "\t__syncthreads();\n"
  "\tif (get_local_id() == owner_index)\n"
  "\t\tkresults->results[base + index] = get_global_id();\n"
  "\t/* Number of items should not be larger than nrooms  */\n"
  "\tassert(base + count <= kresults->nrooms);\n"
  "\n"
  "\t/*\n"
  "\t * Local reduction for each column\n"
  "\t *\n"
  "\t * Any threads that are NOT responsible to grouping-key calculates\n"
  "\t * aggregation on the item that is responsibles.\n"
  "\t * Once atomic operations got finished, values of pagg_datum in the\n"
  "\t * respobsible thread will have partially aggregated one.\n"
  "\t *\n"
  "\t * NOTE: local memory shall be reused to l_datum array, so l_hashslot[]\n"
  "\t * array is no longer available across here\n"
  "\t */\n"
  "\tl_datum = SHARED_WORKMEM(pagg_datum);\n"
  "\tfor (i=0; i < ncols; i++)\n"
  "\t{\n"
  "\t\t/*\n"
  "\t\t * In case when this column is either a grouping-key or not-\n"
  "\t\t * referenced one (thus, not a partial aggregation), all we\n"
  "\t\t * need to do is copying the data from the source to the\n"
  "\t\t * destination; without modification anything.\n"
  "\t\t */\n"
  "\t\tif (gpagg_atts[i] != GPUPREAGG_FIELD_IS_AGGFUNC)\n"
  "\t\t\tcontinue;\n"
  "\n"
  "\t\t/* Load aggregation item to pagg_datum */\n"
  "\t\tif (get_global_id() < nitems)\n"
  "\t\t{\n"
  "\t\t\tgpupreagg_data_load(l_datum + get_local_id(),\n"
  "\t\t\t\t\t\t\t\t&kcxt,\n"
  "\t\t\t\t\t\t\t\tkds_slot,\n"
  "\t\t\t\t\t\t\t\ti,\n"
  "\t\t\t\t\t\t\t\tget_global_id());\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/* Reduction, using local atomic operation */\n"
  "\t\tif (get_global_id() < nitems &&\n"
  "\t\t\tget_local_id() != owner_index)\n"
  "\t\t{\n"
  "\t\t\tgpupreagg_local_calc(&kcxt,\n"
  "\t\t\t\t\t\t\t\t i,\n"
  "\t\t\t\t\t\t\t\t l_datum + owner_index,\n"
  "\t\t\t\t\t\t\t\t l_datum + get_local_id());\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/* Move the value that is aggregated */\n"
  "\t\tif (owner_index == get_local_id())\n"
  "\t\t{\n"
  "\t\t\tassert(get_global_id() < nitems);\n"
  "\t\t\tgpupreagg_data_store(l_datum + owner_index,\n"
  "\t\t\t\t\t\t\t\t &kcxt,\n"
  "\t\t\t\t\t\t\t\t kds_slot,\n"
  "\t\t\t\t\t\t\t\t i,\n"
  "\t\t\t\t\t\t\t\t get_global_id());\n"
  "\t\t\t/*\n"
  "\t\t\t * varlena should never appear here, so we don't need to\n"
  "\t\t\t * put pg_fixup_tupslot_varlena() here\n"
  "\t\t\t */\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\t}\n"
  "\t/* write-back execution status into host-side */\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * Check whether the global hash-slot has enough free space at this moment.\n"
  " */\n"
  "STATIC_INLINE(cl_bool)\n"
  "check_global_hashslot_usage(kern_context *kcxt,\n"
  "\t\t\t\t\t\t\tsize_t g_hashusage, size_t g_hashlimit)\n"
  "{\n"
  "\tif (g_hashusage >= g_hashlimit)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt->e, StromError_DataStoreNoSpace);\n"
  "\t\treturn false;\t/* hash usage exceeds the limitation */\n"
  "\t}\n"
  "\treturn true;\t\t/* ok, we still have rooms */\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_global_reduction\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpupreagg_global_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t\t\t\t   kern_data_store *kds_slot,\n"
  "\t\t\t\t\t\t   kern_resultbuf *kresults_src,\n"
  "\t\t\t\t\t\t   kern_resultbuf *kresults_dst,\n"
  "\t\t\t\t\t\t   kern_global_hashslot *g_hash)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_context\tkcxt;\n"
  "\tvarlena\t\t   *kparam_0 = (varlena *) kparam_get_value(kparams, 0);\n"
  "\tcl_char\t\t   *gpagg_atts = (cl_char *) VARDATA(kparam_0);\n"
  "\tsize_t\t\t\tg_hashsize = g_hash->hash_size;\n"
  "\tsize_t\t\t\tg_hashlimit = GLOBAL_HASHSLOT_THRESHOLD(g_hashsize);\n"
  "\tsize_t\t\t\towner_index;\n"
  "\tsize_t\t\t\tkds_index;\n"
  "\tcl_bool\t\t\tis_valid_slot = false;\n"
  "\tcl_uint\t\t\tkey_dist_salt = kgpreagg->key_dist_salt;\n"
  "\tcl_uint\t\t\tkey_dist_index = 0;\n"
  "\tcl_uint\t\t\thash_value;\n"
  "\tcl_uint\t\t\thash_value_base;\n"
  "\tcl_uint\t\t\tindex;\n"
  "\tcl_uint\t\t\ti, ncols = kds_slot->ncols;\n"
  "\tcl_uint\t\t\tnconflicts = 0;\n"
  "\tcl_uint\t\t\tcount;\n"
  "\tpg_int4_t\t\tkey_dist_factor;\n"
  "\tpagg_hashslot\told_slot;\n"
  "\tpagg_hashslot\tnew_slot;\n"
  "\tpagg_hashslot\tcur_slot;\n"
  "\t__shared__ cl_uint\tcrc32_table[256];\n"
  "\t__shared__ cl_uint\tbase;\n"
  "\t__shared__ cl_uint\tg_hashusage;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpupreagg_global_reduction,kparams);\n"
  "\n"
  "\t/*\n"
  "\t * calculation of the hash value of grouping keys in this record.\n"
  "\t * It tends to take massive amount of random access on global memory,\n"
  "\t * so it makes performance advantage to move the master table from\n"
  "\t * gloabl to the local memory first.\n"
  "\t */\n"
  "\tfor (index = get_local_id();\n"
  "\t\t index < lengthof(kgpreagg->pg_crc32_table);\n"
  "\t\t index += get_local_size())\n"
  "\t\tcrc32_table[index] = kgpreagg->pg_crc32_table[index];\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/*\n"
  "\t * check g_hash slot usage first - this kernel uses staircase operation,\n"
  "\t * so quick exit must be atomically.\n"
  "\t */\n"
  "\t__threadfence();\n"
  "\tif (get_local_id() == 0)\n"
  "\t\tg_hashusage = g_hash->hash_usage;\n"
  "\t__syncthreads();\n"
  "\tif (!check_global_hashslot_usage(&kcxt, g_hashusage, g_hashlimit))\n"
  "\t\tgoto out;\n"
  "\n"
  "\tif (kresults_src->all_visible)\n"
  "\t{\n"
  "\t\tif (get_global_id() < kds_slot->nitems)\n"
  "\t\t{\n"
  "\t\t\tkds_index = get_global_id();\n"
  "\t\t\tis_valid_slot = true;\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\tif (get_global_id() < kresults_src->nitems)\n"
  "\t\t{\n"
  "\t\t\tkds_index = kresults_src->results[get_global_id()];\n"
  "\t\t\tassert(kds_index < kds_slot->nitems);\n"
  "\t\t\tis_valid_slot = true;\n"
  "\t\t}\n"
  "\t}\n"
  "\n"
  "\tif (is_valid_slot)\n"
  "\t{\n"
  "\t\t/*\n"
  "\t\t * Calculation of initial hash value\n"
  "\t\t */\n"
  "\t\tINIT_LEGACY_CRC32(hash_value);\n"
  "\t\thash_value = gpupreagg_hashvalue(&kcxt, crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\t hash_value,\n"
  "\t\t\t\t\t\t\t\t\t\t kds_slot,\n"
  "\t\t\t\t\t\t\t\t\t\t kds_index);\n"
  "\t\thash_value_base = hash_value;\n"
  "\t\tif (key_dist_salt > 1)\n"
  "\t\t{\n"
  "\t\t\tkey_dist_factor.isnull = false;\n"
  "\t\t\tkey_dist_factor.value = (get_global_id() % key_dist_salt);\n"
  "\t\t\thash_value = pg_int4_comp_crc32(crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\t\thash_value,\n"
  "\t\t\t\t\t\t\t\t\t\t\tkey_dist_factor);\n"
  "\t\t}\n"
  "\t\tFIN_LEGACY_CRC32(hash_value);\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Find a hash-slot to determine the item index that represents\n"
  "\t\t * a particular group-keys.\n"
  "\t\t * The array of hash-slot is initialized to 'all empty', so first\n"
  "\t\t * one will take a place using atomic operation. Then. here are\n"
  "\t\t * two cases for hash conflicts; case of same grouping-key, or\n"
  "\t\t * case of different grouping-key but same hash-value.\n"
  "\t\t * The first conflict case informs us the item-index responsible\n"
  "\t\t * to the grouping key. We cannot help the later case, so retry\n"
  "\t\t * the steps with next hash-slot.\n"
  "\t\t */\n"
  "\tretry_major:\n"
  "\t\tnew_slot.s.hash = hash_value;\n"
  "\t\tnew_slot.s.index = kds_index;\n"
  "\t\told_slot.s.hash = 0;\n"
  "\t\told_slot.s.index = (cl_uint)(0xffffffff);\n"
  "\t\tindex = hash_value % g_hashsize;\n"
  "\tretry_minor:\n"
  "\t\tcur_slot.value = atomicCAS(&g_hash->hash_slot[index].value,\n"
  "\t\t\t\t\t\t\t\t   old_slot.value,\n"
  "\t\t\t\t\t\t\t\t   new_slot.value);\n"
  "\t\tif (cur_slot.value == old_slot.value)\n"
  "\t\t{\n"
  "\t\t\tcl_uint\t\tg_hashusage = atomicAdd(&g_hash->hash_usage, 1);\n"
  "\t\t\tassert(g_hashusage < g_hashsize);\n"
  "\t\t\t/*\n"
  "\t\t\t * Hash slot was empty, so this thread shall be responsible\n"
  "\t\t\t * to this grouping-key.\n"
  "\t\t\t */\n"
  "\t\t\towner_index = new_slot.s.index;\n"
  "\t\t}\n"
  "\t\telse if (cur_slot.s.hash == new_slot.s.hash &&\n"
  "\t\t\t\t gpupreagg_keymatch(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\tkds_slot, kds_index,\n"
  "\t\t\t\t\t\t\t\t\tkds_slot, cur_slot.s.index))\n"
  "\t\t{\n"
  "\t\t\tassert(cur_slot.s.index < kds_slot->nitems);\n"
  "\t\t\towner_index = cur_slot.s.index;\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\tnconflicts++;\n"
  "\t\t\tif (key_dist_salt > 1 && ++key_dist_index < key_dist_salt)\n"
  "\t\t\t{\n"
  "\t\t\t\thash_value = hash_value_base;\n"
  "\t\t\t\tkey_dist_factor.isnull = false;\n"
  "\t\t\t\tkey_dist_factor.value =\n"
  "\t\t\t\t\t(get_global_id() + key_dist_index) % key_dist_salt;\n"
  "\t\t\t\thash_value = pg_int4_comp_crc32(crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\thash_value,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tkey_dist_factor);\n"
  "\t\t\t\tFIN_LEGACY_CRC32(hash_value);\n"
  "\t\t\t\tgoto retry_major;\n"
  "\t\t\t}\n"
  "\t\t\t__threadfence();\n"
  "\t\t\tif (check_global_hashslot_usage(&kcxt, g_hash->hash_usage,\n"
  "\t\t\t\t\t\t\t\t\t\t\tg_hashlimit))\n"
  "\t\t\t{\n"
  "\t\t\t\tindex = (index + 1) % g_hashsize;\n"
  "\t\t\t\tgoto retry_minor;\n"
  "\t\t\t}\n"
  "\t\t\towner_index = (cl_uint)(0xffffffff);\n"
  "\t\t}\n"
  "\t}\n"
  "\telse\n"
  "\t\towner_index = (cl_uint)(0xffffffff);\n"
  "\n"
  "\t/*\n"
  "\t * Allocation of a slot of kern_rowmap to point which slot is\n"
  "\t * responsible to grouping key.\n"
  "\t * All the threads that are not responsible to the grouping-key,\n"
  "\t * it updates the value of responsible thread.\n"
  "\t *\n"
  "\t * NOTE: Length of kern_row_map should be same as kds->nrooms.\n"
  "\t * So, we can use kds->nrooms to check array boundary.\n"
  "\t */\n"
  "\t__syncthreads();\n"
  "\tindex = pgstromStairlikeSum(owner_index != (cl_uint)(0xffffffff) &&\n"
  "\t\t\t\t\t\t\t\towner_index == kds_index ? 1 : 0,\n"
  "\t\t\t\t\t\t\t\t&count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t__syncthreads();\n"
  "\t\tassert(base + count <= kresults_dst->nrooms);\n"
  "\t\tif (kds_index == owner_index)\n"
  "\t\t\tkresults_dst->results[base + index] = kds_index;\n"
  "\t}\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/*\n"
  "\t * Quick bailout if thread is not valid, or no hash slot is available.\n"
  "\t * check_global_hashslot_usage() already put an error code, for CPU\n"
  "\t * fallback logic, so we can exit anyway.\n"
  "\t */\n"
  "\tif (owner_index == (cl_uint)(0xffffffff))\n"
  "\t\tgoto out;\n"
  "\n"
  "\t/*\n"
  "\t * Global reduction for each column\n"
  "\t *\n"
  "\t * Any non-owner thread shall accumulate its own value onto the value\n"
  "\t * owned by grouping-key owner. Once atomic operations got done, the\n"
  "\t * accumulated value means the partial aggregation.\n"
  "\t * In case when thread points invalid item or thread could not find\n"
  "\t * a hash slot (decision by check_global_hashslot_usage), this thread\n"
  "\t * will skip the reduction phase.\n"
  "\t * Even in the later case, check_global_hashslot_usage already set an\n"
  "\t * error code, so we don't care about here.\n"
  "\t */\n"
  "\tif (owner_index != 0xffffffffU &&\t\t/* a valid thread? */\n"
  "\t\towner_index != kds_index)\t\t\t/* not a owner thread? */\n"
  "\t{\n"
  "\t\tfor (i=0; i < ncols; i++)\n"
  "\t\t{\n"
  "\t\t\tif (gpagg_atts[i] != GPUPREAGG_FIELD_IS_AGGFUNC)\n"
  "\t\t\t\tcontinue;\n"
  "\t\t\tassert(owner_index < kds_slot->nrooms);\n"
  "\t\t\tgpupreagg_global_calc(&kcxt,\n"
  "\t\t\t\t\t\t\t\t  i,\n"
  "\t\t\t\t\t\t\t\t  kds_slot, owner_index,\n"
  "\t\t\t\t\t\t\t\t  kds_slot, kds_index);\n"
  "\t\t}\n"
  "\t}\n"
  "out:\n"
  "\t/* collect run-time statistics */\n"
  "\tpgstromStairlikeSum(nconflicts, &count);\n"
  "\tif (count > 0 && get_local_id() == 0)\n"
  "\t\tatomicAdd(&kgpreagg->ghash_conflicts, count);\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/* write-back execution status into host-side */\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_final_preparation\n"
  " *\n"
  " * It initializes the f_hash prior to gpupreagg_final_reduction\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpupreagg_final_preparation(size_t f_hashsize,\n"
  "\t\t\t\t\t\t\tkern_global_hashslot *f_hash)\n"
  "{\n"
  "\tsize_t\t\thash_index;\n"
  "\n"
  "\tif (get_global_id() == 0)\n"
  "\t{\n"
  "\t\tf_hash->hash_usage = 0;\n"
  "\t\tf_hash->hash_size = f_hashsize;\n"
  "\t}\n"
  "\n"
  "\tfor (hash_index = get_global_id();\n"
  "\t\t hash_index < f_hashsize;\n"
  "\t\t hash_index += get_global_size())\n"
  "\t{\n"
  "\t\tf_hash->hash_slot[hash_index].s.hash = 0;\n"
  "\t\tf_hash->hash_slot[hash_index].s.index = (cl_uint)(0xffffffff);\n"
  "\t}\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_final_reduction\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpupreagg_final_reduction(kern_gpupreagg *kgpreagg,\t\t/* in */\n"
  "\t\t\t\t\t\t  kern_data_store *kds_slot,\t/* in */\n"
  "\t\t\t\t\t\t  kern_data_store *kds_final,\t/* out */\n"
  "\t\t\t\t\t\t  kern_resultbuf *kresults_src,\t/* in */\n"
  "\t\t\t\t\t\t  kern_resultbuf *kresults_dst,\t/* out, locked only */\n"
  "\t\t\t\t\t\t  kern_global_hashslot *f_hash)\t/* only internal */\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_context\tkcxt;\n"
  "\tvarlena\t\t   *kparam_0 = (varlena *) kparam_get_value(kparams, 0);\n"
  "\tcl_char\t\t   *gpagg_atts = (cl_char *) VARDATA(kparam_0);\n"
  "\tcl_uint\t\t\tkds_index;\n"
  "\tcl_uint\t\t\towner_index = (cl_uint)(0xffffffff); //INVALID\n"
  "\tsize_t\t\t\tf_hashsize = f_hash->hash_size;\n"
  "\tsize_t\t\t\tf_hashlimit = GLOBAL_HASHSLOT_THRESHOLD(f_hashsize);\n"
  "\tcl_uint\t\t\tkey_dist_salt = kgpreagg->key_dist_salt;\n"
  "\tcl_uint\t\t\tkey_dist_index = 0;\n"
  "\tcl_uint\t\t\thash_value;\n"
  "\tcl_uint\t\t\thash_value_base;\n"
  "\tcl_uint\t\t\ti, ncols = kds_slot->ncols;\n"
  "\tcl_uint\t\t\tindex;\n"
  "\tcl_uint\t\t\tcount;\n"
  "\tcl_uint\t\t\tnconflicts = 0;\n"
  "\tcl_uint\t\t\tallocated = 0;\n"
  "\tcl_bool\t\t\tisOwner = false;\n"
  "\tcl_bool\t\t\tmeet_locked = false;\n"
  "\tpg_int4_t\t\tkey_dist_factor;\n"
  "\tpagg_hashslot\told_slot;\n"
  "\tpagg_hashslot\tnew_slot;\n"
  "\tpagg_hashslot\tcur_slot;\n"
  "\t__shared__ cl_uint\tcrc32_table[256];\n"
  "\t__shared__ cl_uint\tf_hashusage;\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpupreagg_final_reduction,kparams);\n"
  "\n"
  "\t/*\n"
  "\t * check availability of final hashslot usage\n"
  "\t */\n"
  "\t__threadfence();\n"
  "\tif (get_local_id() == 0)\n"
  "\t\tf_hashusage = f_hash->hash_usage;\n"
  "\t__syncthreads();\n"
  "\tif (!check_global_hashslot_usage(&kcxt, f_hashusage, f_hashlimit))\n"
  "\t\tgoto out;\n"
  "\n"
  "\t/*\n"
  "\t * calculation of the hash value of grouping keys in this record.\n"
  "\t * It tends to take massive amount of random access on global memory,\n"
  "\t * so it makes performance advantage to move the master table from\n"
  "\t * gloabl to the local memory first.\n"
  "\t */\n"
  "\tfor (index = get_local_id();\n"
  "\t\t index < lengthof(kgpreagg->pg_crc32_table);\n"
  "\t\t index += get_local_size())\n"
  "\t\tcrc32_table[index] = kgpreagg->pg_crc32_table[index];\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/* row-index on the kds_slot buffer */\n"
  "\tif (kresults_src->all_visible)\n"
  "\t{\n"
  "\t\tif (get_global_id() < kds_slot->nitems)\n"
  "\t\t\tkds_index = get_global_id();\n"
  "\t\telse\n"
  "\t\t\tgoto out;\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\tif (get_global_id() < min(kresults_src->nitems,\n"
  "\t\t\t\t\t\t\t\t  kresults_src->nrooms))\n"
  "\t\t\tkds_index = kresults_src->results[get_global_id()];\n"
  "\t\telse\n"
  "\t\t\tgoto out;\n"
  "\t\tassert(kds_index < kds_slot->nitems);\n"
  "\t}\n"
  "\n"
  "\t/*\n"
  "\t * Calculation of initial hash value\n"
  "\t */\n"
  "\tINIT_LEGACY_CRC32(hash_value);\n"
  "\thash_value = gpupreagg_hashvalue(&kcxt, crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t hash_value,\n"
  "\t\t\t\t\t\t\t\t\t kds_slot,\n"
  "\t\t\t\t\t\t\t\t\t kds_index);\n"
  "\thash_value_base = hash_value;\n"
  "\tif (key_dist_salt > 1)\n"
  "\t{\n"
  "\t\tkey_dist_factor.isnull = false;\n"
  "\t\tkey_dist_factor.value = (get_global_id() % key_dist_salt);\n"
  "\t\thash_value = pg_int4_comp_crc32(crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\thash_value,\n"
  "\t\t\t\t\t\t\t\t\t\tkey_dist_factor);\n"
  "\t}\n"
  "\tFIN_LEGACY_CRC32(hash_value);\n"
  "\n"
  "\t/*\n"
  "\t * Find a hash-slot to determine the item index that represents\n"
  "\t * a particular group-keys.\n"
  "\t * The array of hash-slot is initialized to 'all empty', so first\n"
  "\t * one will take a place using atomic operation. Then. here are\n"
  "\t * two cases for hash conflicts; case of same grouping-key, or\n"
  "\t * case of different grouping-key but same hash-value.\n"
  "\t * The first conflict case informs us the item-index responsible\n"
  "\t * to the grouping key. We cannot help the later case, so retry\n"
  "\t * the steps with next hash-slot.\n"
  "\t */\n"
  "retry_major:\n"
  "\tnew_slot.s.hash  = hash_value;\n"
  "\tnew_slot.s.index = (cl_uint)(0xfffffffe); /* LOCK */\n"
  "\told_slot.s.hash  = 0;\n"
  "\told_slot.s.index = (cl_uint)(0xffffffff); /* INVALID */\n"
  "\tindex  = hash_value % f_hashsize;\n"
  "retry_minor:\n"
  "\tcur_slot.value = atomicCAS(&f_hash->hash_slot[index].value,\n"
  "\t\t\t\t\t\t\t   old_slot.value, new_slot.value);\n"
  "\n"
  "\tif (cur_slot.value == old_slot.value)\n"
  "\t{\n"
  "\t\t/*\n"
  "\t\t * We could get an empty slot, so hash_usage should be smaller\n"
  "\t\t * than hash_size itself, at least.\n"
  "\t\t */\n"
  "\t\tcl_uint\t\tf_hashusage = atomicAdd(&f_hash->hash_usage, 1);\n"
  "\t\tassert(f_hashusage < f_hashsize);\n"
  "\n"
  "\t\t/*\n"
  "\t\t * This thread shall be responsible to this grouping-key\n"
  "\t\t *\n"
  "\t\t * MEMO: We may need to check whether the new nitems exceeds\n"
  "\t\t * usage of the extra length of the kds_final from the tail,\n"
  "\t\t * instead of the nrooms, however, some code assumes kds_final\n"
  "\t\t * can store at least 'nrooms' items. So, right now, we don't\n"
  "\t\t * allow to expand extra area across nrooms boundary.\n"
  "\t\t */\n"
  "\t\tnew_slot.s.index = atomicAdd(&kds_final->nitems, 1);\n"
  "\t\tif (new_slot.s.index < kds_final->nrooms)\n"
  "\t\t{\n"
  "\t\t\tallocated += gpupreagg_final_data_move(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_slot,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_index,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   kds_final,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t   new_slot.s.index);\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\t}\n"
  "\t\t__threadfence();\n"
  "\t\tf_hash->hash_slot[index].s.index = new_slot.s.index;\n"
  "\t\t/* this thread is the owner of this slot */\n"
  "\t\towner_index = new_slot.s.index;\n"
  "\t\tcur_slot.value = new_slot.value;\n"
  "\t\tisOwner = true;\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\t/* it may be a slot we can use later */\n"
  "\t\tif (cur_slot.s.index == (cl_uint)(0xfffffffe) &&\n"
  "\t\t\tcur_slot.s.hash == hash_value)\n"
  "\t\t\tmeet_locked = true;\n"
  "\n"
  "\t\tif (cur_slot.s.index != (cl_uint)(0xfffffffe) &&\n"
  "\t\t\tcur_slot.s.hash  == hash_value &&\n"
  "\t\t\tgpupreagg_keymatch(&kcxt,\n"
  "\t\t\t\t\t\t\t   kds_slot, kds_index,\n"
  "\t\t\t\t\t\t\t   kds_final, cur_slot.s.index))\n"
  "\t\t{\n"
  "\t\t\t/* grouping key matched */\n"
  "\t\t\towner_index = cur_slot.s.index;\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\t/* hash slot conflicts */\n"
  "\t\t\tnconflicts++;\n"
  "\t\t\tif (key_dist_salt > 1 && ++key_dist_index < key_dist_salt)\n"
  "\t\t\t{\n"
  "\t\t\t\thash_value = hash_value_base;\n"
  "\t\t\t\tkey_dist_factor.isnull = false;\n"
  "\t\t\t\tkey_dist_factor.value =\n"
  "\t\t\t\t\t(get_global_id() + key_dist_index) % key_dist_salt;\n"
  "\t\t\t\thash_value = pg_int4_comp_crc32(crc32_table,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\thash_value,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tkey_dist_factor);\n"
  "\t\t\t\tFIN_LEGACY_CRC32(hash_value);\n"
  "\t\t\t\tgoto retry_major;\n"
  "\t\t\t}\n"
  "\t\t\t/*\n"
  "\t\t\t * If we already meet a hash entry that was locked but has\n"
  "\t\t\t * same hash value, we try to walk on the next kernel\n"
  "\t\t\t * invocation, than enlarge hash slot.\n"
  "\t\t\t */\n"
  "\t\t\tif (meet_locked)\n"
  "\t\t\t{\n"
  "\t\t\t\towner_index = (cl_uint)(0xfffffffe); // LOCKED\n"
  "\t\t\t\tgoto out;\n"
  "\t\t\t}\n"
  "\n"
  "\t\t\tif (!check_global_hashslot_usage(&kcxt, f_hash->hash_usage,\n"
  "\t\t\t\t\t\t\t\t\t\t\t f_hashlimit))\n"
  "\t\t\t\tgoto out;\n"
  "\t\t\tindex = (index + 1) % f_hashsize;\n"
  "\t\t\tgoto retry_minor;\n"
  "\t\t}\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Global reduction for each column\n"
  "\t\t *\n"
  "\t\t * Any threads that are NOT responsible to grouping-key calculates\n"
  "\t\t * aggregation on the item that is responsibles.\n"
  "\t\t * Once atomic operations got finished, values of pagg_datum in the\n"
  "\t\t * respobsible thread will have partially aggregated one.\n"
  "\t\t */\n"
  "\t\tif (owner_index < Min(kds_final->nitems,\n"
  "\t\t\t\t\t\t\t  kds_final->nrooms))\n"
  "\t\t{\n"
  "\t\t\tfor (i=0; i < ncols; i++)\n"
  "\t\t\t{\n"
  "\t\t\t\tif (gpagg_atts[i] != GPUPREAGG_FIELD_IS_AGGFUNC)\n"
  "\t\t\t\t\tcontinue;\n"
  "\t\t\t\n"
  "\t\t\t\t/*\n"
  "\t\t\t\t * Reduction, using global atomic operation\n"
  "\t\t\t\t *\n"
  "\t\t\t\t * If thread is responsible to the grouping-key, other\n"
  "\t\t\t\t * threads but NOT responsible will accumlate their values\n"
  "\t\t\t\t * here, then it shall become aggregated result. So, we mark\n"
  "\t\t\t\t * the \"responsible\" thread identifier on the kern_row_map.\n"
  "\t\t\t\t * Once kernel execution gets done, this index points the\n"
  "\t\t\t\t * location of aggregate value.\n"
  "\t\t\t\t */\n"
  "\t\t\t\tgpupreagg_global_calc(&kcxt,\n"
  "\t\t\t\t\t\t\t\t\t  i,\n"
  "\t\t\t\t\t\t\t\t\t  kds_final, owner_index,\n"
  "\t\t\t\t\t\t\t\t\t  kds_slot, kds_index);\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t}\n"
  "out:\n"
  "\t/* Do we try to update kds_final again on the next kernel call? */\n"
  "\t__syncthreads();\n"
  "\tindex = pgstromStairlikeSum(owner_index == 0xfffffffeU ? 1 : 0,\n"
  "\t\t\t\t\t\t\t\t&count);\n"
  "\tif (count > 0)\n"
  "\t{\n"
  "\t\t__shared__ cl_uint base;\n"
  "\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t\tbase = atomicAdd(&kresults_dst->nitems, count);\n"
  "\t\t__syncthreads();\n"
  "\t\tif (owner_index == 0xfffffffeU)\n"
  "\t\t\tkresults_dst->results[base + index] = kds_index;\n"
  "\t}\n"
  "\n"
  "\t/* update run-time statistics */\n"
  "\tpgstromStairlikeSum(isOwner ? 1 : 0, &count);\n"
  "\tif (count > 0 && get_local_id() == 0)\n"
  "\t\tatomicAdd(&kgpreagg->num_groups, count);\n"
  "\t__syncthreads();\n"
  "\n"
  "\tpgstromStairlikeSum(allocated, &count);\n"
  "\tif (count > 0 && get_local_id() == 0)\n"
  "\t\tatomicAdd(&kgpreagg->varlena_usage, count);\n"
  "\t__syncthreads();\n"
  "\n"
  "\tpgstromStairlikeSum(nconflicts, &count);\n"
  "\tif (count > 0 && get_local_id() == 0)\n"
  "\t\tatomicAdd(&kgpreagg->fhash_conflicts, count);\n"
  "\t__syncthreads();\n"
  "\n"
  "\t/* write-back execution status into host-side */\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_fixup_varlena\n"
  " *\n"
  " * In case when varlena datum (excludes numeric) is used in grouping-key,\n"
  " * datum on kds with tupslot format has not-interpretable for host systems.\n"
  " * So, we need to fix up its value to adjust offset by hostptr.\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpupreagg_fixup_varlena(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t\t\t\tkern_data_store *kds_final)\n"
  "{\n"
  "\tkern_parambuf  *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_context\tkcxt;\n"
  "\tvarlena\t\t   *kparam_0 = (varlena *) kparam_get_value(kparams, 0);\n"
  "\tcl_uint\t\t\ti, ncols = kds_final->ncols;\n"
  "\tkern_colmeta\tcmeta;\n"
  "\tsize_t\t\t\tkds_index = get_global_id();\n"
  "\tDatum\t\t   *dst_values = KERN_DATA_STORE_VALUES(kds_final, kds_index);\n"
  "\tcl_bool\t\t   *dst_isnull = KERN_DATA_STORE_ISNULL(kds_final, kds_index);\n"
  "\tcl_char\t\t   *numeric_ptr = NULL;\n"
  "\tcl_uint\t\t\tnumeric_len = 0;\n"
  "\n"
  "\t/* Sanity checks */\n"
  "\tassert(kds_final->format == KDS_FORMAT_SLOT);\n"
  "\tassert(kds_final->has_notbyval);\n"
  "\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt,gpupreagg_fixup_varlena,kparams);\n"
  "\n"
  "\t/*\n"
  "\t * Expand extra field to fixup numeric data type; varlena or indirect\n"
  "\t * data types are already copied to the extra field, so all we have to\n"
  "\t * fixup here is numeric data type.\n"
  "\t */\n"
  "\tif (kds_final->has_numeric)\n"
  "\t{\n"
  "\t\tcl_uint\t\toffset;\n"
  "\t\tcl_uint\t\tcount;\n"
  "\t\t__shared__ cl_uint base;\n"
  "\n"
  "\t\tif (kds_index < kds_final->nitems)\n"
  "\t\t{\n"
  "\t\t\tfor (i=0; i < ncols; i++)\n"
  "\t\t\t{\n"
  "\t\t\t\tcmeta = kds_final->colmeta[i];\n"
  "\n"
  "\t\t\t\tif (cmeta.atttypid != PG_NUMERICOID)\n"
  "\t\t\t\t\tcontinue;\n"
  "\t\t\t\tif (dst_isnull[i])\n"
  "\t\t\t\t\tcontinue;\n"
  "\t\t\t\tnumeric_len += MAXALIGN(pg_numeric_to_varlena(&kcxt, NULL,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  dst_values[i],\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  dst_isnull[i]));\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t\t/* allocation of the extra buffer on demand */\n"
  "\t\toffset = pgstromStairlikeSum(numeric_len, &count);\n"
  "\t\tif (get_local_id() == 0)\n"
  "\t\t{\n"
  "\t\t\tif (count > 0)\n"
  "\t\t\t\tbase = atomicAdd(&kds_final->usage, count);\n"
  "\t\t\telse\n"
  "\t\t\t\tbase = 0;\n"
  "\t\t}\n"
  "\t\t__syncthreads();\n"
  "\n"
  "\t\t/*\n"
  "\t\t * At this point, number of items will be never increased any more,\n"
  "\t\t * so extra area is limited by nitems, not nrooms. It is actually\n"
  "\t\t * 'extra' area than final_reduction phase. :-)\n"
  "\t\t */\n"
  "\t\tif (KERN_DATA_STORE_SLOT_LENGTH(kds_final, kds_final->nitems) +\n"
  "\t\t\tbase + count >= kds_final->length)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_DataStoreNoSpace);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tnumeric_ptr = ((char *)kds_final + kds_final->length -\n"
  "\t\t\t\t\t   (base + offset + numeric_len));\n"
  "\t}\n"
  "\n"
  "\tif (kds_index < kds_final->nitems)\n"
  "\t{\n"
  "\t\tfor (i=0; i < ncols; i++)\n"
  "\t\t{\n"
  "\t\t\t/* No need to fixup NULL value anyway */\n"
  "\t\t\tif (dst_isnull[i])\n"
  "\t\t\t\tcontinue;\n"
  "\n"
  "\t\t\tcmeta = kds_final->colmeta[i];\n"
  "\t\t\tif (cmeta.atttypid == PG_NUMERICOID)\n"
  "\t\t\t{\n"
  "\t\t\t\tassert(numeric_ptr != NULL);\n"
  "\t\t\t\tnumeric_len = pg_numeric_to_varlena(&kcxt, numeric_ptr,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tdst_values[i],\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tdst_isnull[i]);\n"
  "\t\t\t\tdst_values[i] = devptr_to_host(kds_final, numeric_ptr);\n"
  "\t\t\t\tnumeric_ptr += MAXALIGN(numeric_len);\n"
  "\t\t\t}\n"
  "\t\t\telse if (!cmeta.attbyval)\n"
  "\t\t\t{\n"
  "\t\t\t\t/* validation of the device pointer */\n"
  "\t\t\t\tassert(dst_values[i] >= ((size_t)kds_final +\n"
  "\t\t\t\t\t\t\t\t\t\t kds_final->length -\n"
  "\t\t\t\t\t\t\t\t\t\t kds_final->usage) &&\n"
  "\t\t\t\t\t   dst_values[i] <  ((size_t)kds_final +\n"
  "\t\t\t\t\t\t\t\t\t\t kds_final->length));\n"
  "\t\t\t\tdst_values[i] = devptr_to_host(kds_final, dst_values[i]);\n"
  "\t\t\t}\n"
  "\t\t}\n"
  "\t}\n"
  "out:\n"
  "\t/* write-back execution status into host-side */\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/*\n"
  " * gpupreagg_main\n"
  " *\n"
  " * The controller kernel function that launches a \n"
  " *\n"
  " *\n"
  " *\n"
  " *\n"
  " *\n"
  " */\n"
  "KERNEL_FUNCTION(void)\n"
  "gpupreagg_main(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t   kern_data_store *kds_row,\t\t/* KDS_FORMAT_ROW */\n"
  "\t\t\t   kern_data_store *kds_slot,\t\t/* KDS_FORMAT_SLOT */\n"
  "\t\t\t   kern_global_hashslot *g_hash,\t/* For global reduction */\n"
  "\t\t\t   kern_data_store *kds_final,\t\t/* KDS_FORMAT_SLOT + Extra */\n"
  "\t\t\t   kern_global_hashslot *f_hash)\n"
  "{\n"
  "\tkern_parambuf\t   *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);\n"
  "\tkern_resultbuf\t   *kresults_src = KERN_GPUPREAGG_1ST_RESULTBUF(kgpreagg);\n"
  "\tkern_resultbuf\t   *kresults_dst = KERN_GPUPREAGG_2ND_RESULTBUF(kgpreagg);\n"
  "\tkern_resultbuf\t   *kresults_tmp;\n"
  "\tcl_uint\t\t\t\tkresults_nrooms = kds_row->nitems;\n"
  "\tkern_context\t\tkcxt;\n"
  "\tvoid\t\t\t  **kern_args;\n"
  "\tdim3\t\t\t\tgrid_sz;\n"
  "\tdim3\t\t\t\tblock_sz;\n"
  "\tcl_ulong\t\t\ttv_start;\n"
  "\tcudaError_t\t\t\tstatus = cudaSuccess;\n"
  "\n"
  "\t/* Init kernel context */\n"
  "\tINIT_KERNEL_CONTEXT(&kcxt, gpupreagg_main, kparams);\n"
  "\tassert(get_global_size() == 1);\t/* !!single thread!! */\n"
  "\tassert(kgpreagg->reduction_mode != GPUPREAGG_ONLY_TERMINATION);\n"
  "\n"
  "\t/* Launch:\n"
  "\t * KERNEL_FUNCTION(void)\n"
  "\t * gpupreagg_preparation(kern_gpupreagg *kgpreagg,\n"
  "\t *                       kern_data_store *kds_row,\n"
  "\t *                       kern_data_store *kds_slot,\n"
  "\t *                       kern_global_hashslot *g_hash)\n"
  "\t */\n"
  "\ttv_start = GlobalTimer();\n"
  "\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 4);\n"
  "\tif (!kern_args)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\tkern_args[0] = kgpreagg;\n"
  "\tkern_args[1] = kds_row;\n"
  "\tkern_args[2] = kds_slot;\n"
  "\tkern_args[3] = g_hash;\n"
  "\n"
  "\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\tgpupreagg_preparation,\n"
  "\t\t\t\t\t\t\t\t\tkds_row->nitems,\n"
  "\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\n"
  "\tstatus = cudaLaunchDevice((void *)gpupreagg_preparation,\n"
  "\t\t\t\t\t\t\t  kern_args, grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t  NULL);\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\n"
  "\tstatus = cudaDeviceSynchronize();\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\telse if (kgpreagg->kerror.errcode != StromError_Success)\n"
  "\t\treturn;\n"
  "\n"
  "\tTIMEVAL_RECORD(kgpreagg,kern_prep,tv_start);\n"
  "\n"
  "\tif (kgpreagg->reduction_mode == GPUPREAGG_NOGROUP_REDUCTION)\n"
  "\t{\n"
  "\t\t/* Launch:\n"
  "\t\t * KERNEL_FUNCTION(void)\n"
  "\t\t * gpupreagg_nogroup_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t\t *                             kern_data_store *kds_slot,\n"
  "\t\t *                             kern_resultbuf *kresults_src,\n"
  "\t\t *                             kern_resultbuf *kresults_dst)\n"
  "\t\t */\n"
  "\t\ttv_start = GlobalTimer();\n"
  "\n"
  "\t\t/* setup kern_resultbuf */\n"
  "\t\tmemset(kresults_src, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_src->nrels = 1;\n"
  "\t\tkresults_src->nrooms = kresults_nrooms;\n"
  "\t\tkresults_src->all_visible = true;\n"
  "\n"
  "\t\tmemset(kresults_dst, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_dst->nrels = 1;\n"
  "\t\tkresults_dst->nrooms = kresults_nrooms;\n"
  "\n"
  "\t\t/* 1st trial of the reduction */\n"
  "\t\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 4);\n"
  "\t\tif (!kern_args)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tkern_args[0] = kgpreagg;\n"
  "\t\tkern_args[1] = kds_slot;\n"
  "\t\tkern_args[2] = kresults_src;\n"
  "\t\tkern_args[3] = kresults_dst;\n"
  "\t\tstatus = largest_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\tgpupreagg_nogroup_reduction,\n"
  "\t\t\t\t\t\t\t\t\t\tkds_slot->nitems,\n"
  "\t\t\t\t\t\t\t\t\t\t0, Max(sizeof(pagg_datum),\n"
  "\t\t\t\t\t\t\t\t\t\t\t   sizeof(kern_errorbuf)));\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaLaunchDevice((void *)gpupreagg_nogroup_reduction,\n"
  "\t\t\t\t\t\t\t\t  kern_args, grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t  Max(sizeof(pagg_datum),\n"
  "\t\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf)) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t  NULL);\n"
  "        if (status != cudaSuccess)\n"
  "        {\n"
  "            STROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "            goto out;\n"
  "        }\n"
  "\n"
  "\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\telse if (kgpreagg->kerror.errcode != StromError_Success)\n"
  "\t\t\treturn;\n"
  "\n"
  "\t\t/* 2nd trial of the reduction */\n"
  "\t\tmemset(kresults_src, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_src->nrels = 1;\n"
  "\t\tkresults_src->nrooms = kresults_nrooms;\n"
  "\n"
  "\t\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 4);\n"
  "\t\tif (!kern_args)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tkern_args[0] = kgpreagg;\n"
  "        kern_args[1] = kds_slot;\n"
  "\t\tkern_args[2] = kresults_dst;\t/* reverse */\n"
  "        kern_args[3] = kresults_src;\t/* reverse */\n"
  "\t\tstatus = largest_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\tgpupreagg_nogroup_reduction,\n"
  "\t\t\t\t\t\t\t\t\t\tkresults_dst->nitems,\n"
  "\t\t\t\t\t\t\t\t\t\t0, Max(sizeof(pagg_datum),\n"
  "\t\t\t\t\t\t\t\t\t\t\t   sizeof(kern_errorbuf)));\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaLaunchDevice((void *)gpupreagg_nogroup_reduction,\n"
  "\t\t\t\t\t\t\t\t  kern_args, grid_sz, block_sz,\n"
  "\t\t\t\t\t\t\t\t  Max(sizeof(pagg_datum),\n"
  "\t\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf)) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\telse if (kgpreagg->kerror.errcode != StromError_Success)\n"
  "\t\t\treturn;\n"
  "\n"
  "\t\tTIMEVAL_RECORD(kgpreagg,kern_nogrp,tv_start);\n"
  "\t}\n"
  "\telse if (kgpreagg->reduction_mode == GPUPREAGG_LOCAL_REDUCTION ||\n"
  "\t\t\t kgpreagg->reduction_mode == GPUPREAGG_GLOBAL_REDUCTION)\n"
  "\t{\n"
  "\t\tmemset(kresults_src, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_src->nrels = 1;\n"
  "\t\tkresults_src->nrooms = kresults_nrooms;\n"
  "\t\tmemset(kresults_dst, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_dst->nrels = 1;\n"
  "\t\tkresults_dst->nrooms = kresults_nrooms;\n"
  "\n"
  "\t\tif (kgpreagg->reduction_mode == GPUPREAGG_LOCAL_REDUCTION)\n"
  "\t\t{\n"
  "\t\t\tsize_t\t\tdynamic_shmem_unitsz = Max3(sizeof(kern_errorbuf),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(pagg_hashslot) * 2,\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(pagg_datum));\n"
  "\t\t\t/*\n"
  "\t\t\t * Launch:\n"
  "\t\t\t * KERNEL_FUNCTION_MAXTHREADS(void)\n"
  "\t\t\t * gpupreagg_local_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t\t\t *                           kern_data_store *kds_slot,\n"
  "\t\t\t *                           kern_resultbuf *kresults\n"
  "\t\t\t */\n"
  "\t\t\ttv_start = GlobalTimer();\n"
  "\t\t\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 3);\n"
  "\t\t\tif (!kern_args)\n"
  "\t\t\t{\n"
  "\t\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\t\tgoto out;\n"
  "\t\t\t}\n"
  "\t\t\tkern_args[0] = kgpreagg;\n"
  "\t\t\tkern_args[1] = kds_slot;\t\t/* in */\n"
  "\t\t\tkern_args[2] = kresults_src;\t/* out */\n"
  "\t\t\tstatus = largest_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\t\tgpupreagg_local_reduction,\n"
  "\t\t\t\t\t\t\t\t\t\t\tkds_slot->nitems,\n"
  "\t\t\t\t\t\t\t\t\t\t\t0, dynamic_shmem_unitsz);\n"
  "\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t{\n"
  "\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\tgoto out;\n"
  "\t\t\t}\n"
  "\n"
  "\t\t\tstatus = cudaLaunchDevice((void *)gpupreagg_local_reduction,\n"
  "\t\t\t\t\t\t\t\t\t  kern_args,\n"
  "\t\t\t\t\t\t\t\t\t  grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t  block_sz,\n"
  "\t\t\t\t\t\t\t\t\t  dynamic_shmem_unitsz * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t{\n"
  "\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\tgoto out;\n"
  "\t\t\t}\n"
  "\n"
  "\t\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\t\tif (status != cudaSuccess)\n"
  "\t\t\t{\n"
  "\t\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\t\tgoto out;\n"
  "\t\t\t}\n"
  "\t\t\telse if (kgpreagg->kerror.errcode != StromError_Success)\n"
  "\t\t\t\treturn;\n"
  "\t\t\t/* perfmon */\n"
  "\t\t\tTIMEVAL_RECORD(kgpreagg,kern_lagg,tv_start);\n"
  "\t\t}\n"
  "\t\telse\n"
  "\t\t{\n"
  "\t\t\t/* if no local reduction, global reduction takes all the rows */\n"
  "\t\t\tkresults_src->all_visible = true;\n"
  "\t\t}\n"
  "\n"
  "\t\t/*\n"
  "\t\t * Launch:\n"
  "\t\t * KERNEL_FUNCTION(void)\n"
  "\t\t * gpupreagg_global_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t\t *                            kern_data_store *kds_slot,\n"
  "\t\t *                            kern_resultbuf *kresults_src,\n"
  "\t\t *                            kern_resultbuf *kresults_dst,\n"
  "\t\t *                            kern_global_hashslot *g_hash)\n"
  "\t\t */\n"
  "\t\ttv_start = GlobalTimer();\n"
  "\t\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 5);\n"
  "\t\tif (!kern_args)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\tkern_args[0] = kgpreagg;\n"
  "\t\tkern_args[1] = kds_slot;\n"
  "\t\tkern_args[2] = kresults_src;\n"
  "\t\tkern_args[3] = kresults_dst;\n"
  "\t\tkern_args[4] = g_hash;\n"
  "\t\tstatus = largest_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\t\tgpupreagg_global_reduction,\n"
  "\t\t\t\t\t\t\t\t\t\tkresults_src->all_visible\n"
  "\t\t\t\t\t\t\t\t\t\t? kds_slot->nitems\n"
  "\t\t\t\t\t\t\t\t\t\t: kresults_src->nitems,\n"
  "\t\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaLaunchDevice((void *)gpupreagg_global_reduction,\n"
  "\t\t\t\t\t\t\t\t  kern_args,\n"
  "\t\t\t\t\t\t\t\t  grid_sz,\n"
  "\t\t\t\t\t\t\t\t  block_sz,\n"
  "\t\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t\t  NULL);\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\n"
  "\t\tstatus = cudaDeviceSynchronize();\n"
  "\t\tif (status != cudaSuccess)\n"
  "\t\t{\n"
  "\t\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\t\tgoto out;\n"
  "\t\t}\n"
  "\t\telse if (kgpreagg->kerror.errcode != StromError_Success)\n"
  "\t\t\treturn;\n"
  "\n"
  "\t\tTIMEVAL_RECORD(kgpreagg,kern_gagg,tv_start);\n"
  "\n"
  "\t\t/* swap */\n"
  "\t\tkresults_tmp = kresults_src;\n"
  "\t\tkresults_src = kresults_dst;\n"
  "\t\tkresults_dst = kresults_tmp;\n"
  "\t}\n"
  "\telse\n"
  "\t{\n"
  "\t\t/* only final reduction - all input slot should be visible */\n"
  "\t\tmemset(kresults_src, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\t\tkresults_src->nrels = 1;\n"
  "\t\tkresults_src->nrooms = kresults_nrooms;\n"
  "\t\tkresults_src->all_visible = true;\n"
  "\t}\n"
  "\n"
  "\t/* Launch:\n"
  "\t * KERNEL_FUNCTION(void)\n"
  "\t * gpupreagg_final_reduction(kern_gpupreagg *kgpreagg,\n"
  "\t *                           kern_data_store *kds_slot,\n"
  "\t *                           kern_data_store *kds_final,\n"
  "\t *                           kern_resultbuf *kresults_src,\n"
  "\t *                           kern_resultbuf *kresults_dst,\n"
  "\t *                           kern_global_hashslot *f_hash)\n"
  "\t */\n"
  "\ttv_start = GlobalTimer();\n"
  "final_retry:\n"
  "\t/* init destination kern_resultbuf */\n"
  "\tmemset(kresults_dst, 0, offsetof(kern_resultbuf, results[0]));\n"
  "\tkresults_dst->nrels = 1;\n"
  "\tkresults_dst->nrooms = kresults_nrooms;\n"
  "\n"
  "\tkern_args = (void **)cudaGetParameterBuffer(sizeof(void *),\n"
  "\t\t\t\t\t\t\t\t\t\t\t\tsizeof(void *) * 6);\n"
  "\tif (!kern_args)\n"
  "\t{\n"
  "\t\tSTROM_SET_ERROR(&kcxt.e, StromError_OutOfKernelArgs);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\tkern_args[0] = kgpreagg;\n"
  "\tkern_args[1] = kds_slot;\n"
  "\tkern_args[2] = kds_final;\n"
  "\tkern_args[3] = kresults_src;\n"
  "\tkern_args[4] = kresults_dst;\n"
  "\tkern_args[5] = f_hash;\n"
  "\n"
  "\tstatus = optimal_workgroup_size(&grid_sz,\n"
  "\t\t\t\t\t\t\t\t\t&block_sz,\n"
  "\t\t\t\t\t\t\t\t\t(const void *)\n"
  "\t\t\t\t\t\t\t\t\tgpupreagg_final_reduction,\n"
  "\t\t\t\t\t\t\t\t\tkresults_src->all_visible\n"
  "\t\t\t\t\t\t\t\t\t? kds_slot->nitems\n"
  "\t\t\t\t\t\t\t\t\t: kresults_src->nitems,\n"
  "\t\t\t\t\t\t\t\t\t0, sizeof(kern_errorbuf));\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\tstatus = cudaLaunchDevice((void *)gpupreagg_final_reduction,\n"
  "\t\t\t\t\t\t\t  kern_args,\n"
  "\t\t\t\t\t\t\t  grid_sz,\n"
  "\t\t\t\t\t\t\t  block_sz,\n"
  "\t\t\t\t\t\t\t  sizeof(kern_errorbuf) * block_sz.x,\n"
  "\t\t\t\t\t\t\t  NULL);\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\tstatus = cudaDeviceSynchronize();\n"
  "\tif (status != cudaSuccess)\n"
  "\t{\n"
  "\t\tSTROM_SET_RUNTIME_ERROR(&kcxt.e, status);\n"
  "\t\tgoto out;\n"
  "\t}\n"
  "\telse if (kgpreagg->kerror.errcode != StromError_Success)\n"
  "\t\treturn;\n"
  "\n"
  "\tif (kresults_dst->nitems > 0)\n"
  "\t{\n"
  "\t\t/* swap */\n"
  "\t\tkresults_tmp = kresults_src;\n"
  "\t\tkresults_src = kresults_dst;\n"
  "\t\tkresults_dst = kresults_tmp;\n"
  "\t\t/* increment num_kern_fagg */\n"
  "\t\tkgpreagg->pfm.num_kern_fagg++;\n"
  "\t\tgoto final_retry;\n"
  "\t}\n"
  "\tTIMEVAL_RECORD(kgpreagg,kern_fagg,tv_start);\n"
  "\n"
  "\t/*\n"
  "\t * NOTE: gpupreagg_fixup_varlena shall be launched by CPU thread\n"
  "\t * after synchronization of all the concurrent tasks within same\n"
  "\t * segments. So, we have no chance to launch this kernel by GPU.\n"
  "\t */\n"
  "out:\n"
  "\tkern_writeback_error_status(&kgpreagg->kerror, kcxt.e);\n"
  "}\n"
  "\n"
  "/* ----------------------------------------------------------------\n"
  " *\n"
  " * A thin abstraction layer for atomic functions\n"
  " *\n"
  " * Due to hardware capability difference we have to implement\n"
  " * alternative one using atomicCAS operation. For simplification.\n"
  " * we wrap all the atomic functions using:\n"
  " *   pg_atomic_(min|max|add)_<type>(<type> *addr, <type> value) \n"
  " *\n"
  " * ----------------------------------------------------------------\n"
  " */\n"
  "STATIC_INLINE(cl_int)\n"
  "pg_atomic_min_int(cl_int *addr, cl_int value)\n"
  "{\n"
  "\treturn __iAtomicMin(addr, value);\n"
  "}\n"
  "STATIC_INLINE(cl_int)\n"
  "pg_atomic_max_int(cl_int *addr, cl_int value)\n"
  "{\n"
  "\treturn __iAtomicMax(addr, value);\n"
  "}\n"
  "STATIC_INLINE(cl_int)\n"
  "pg_atomic_add_int(cl_int *addr, cl_int value)\n"
  "{\n"
  "\treturn __iAtomicAdd(addr, value);\n"
  "}\n"
  "\n"
  "\n"
  "#define PG_ATOMIC_LONG_TEMPLATE(operation,addr,value)\t\t\t\t\\\n"
  "\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tcl_ulong\tcurval = *((cl_ulong *) (addr));\t\t\t\t\\\n"
  "\t\tcl_ulong\toldval;\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tcl_ulong\tnewval;\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\toldval = curval;\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tnewval = operation(oldval, (value));\t\t\t\t\t\\\n"
  "\t\t} while ((curval = atomicCAS((cl_ulong *) (addr),\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t oldval, newval)) != oldval);\t\\\n"
  "\t\treturn (cl_long) oldval;\t\t\t\t\t\t\t\t\t\\\n"
  "\t} while(0)\n"
  "\n"
  "STATIC_INLINE(cl_long)\n"
  "pg_atomic_min_long(cl_long *addr, cl_long value)\n"
  "{\n"
  "#if __CUDA_ARCH__ < 350\n"
  "\tPG_ATOMIC_LONG_TEMPLATE(Min, addr, value);\n"
  "#else\n"
  "\treturn __illAtomicMin(addr, value);\t\n"
  "#endif\n"
  "}\n"
  "\n"
  "STATIC_INLINE(cl_long)\n"
  "pg_atomic_max_long(cl_long *addr, cl_long value)\n"
  "{\n"
  "#if __CUDA_ARCH__ < 350\n"
  "\tPG_ATOMIC_LONG_TEMPLATE(Max, addr, value);\n"
  "#else\n"
  "\treturn __illAtomicMax(addr, value);\t\n"
  "#endif\n"
  "}\n"
  "\n"
  "STATIC_INLINE(cl_long)\n"
  "pg_atomic_add_long(cl_long *addr, cl_long value)\n"
  "{\n"
  "\treturn (cl_long) atomicAdd((cl_ulong *) addr, (cl_ulong) value);\n"
  "}\n"
  "\n"
  "#define PG_ATOMIC_FLOAT_TEMPLATE(operation,addr,value)\t\t\t\t\t\\\n"
  "\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tcl_uint\t\tcurval = __float_as_int(*(addr));\t\t\t\t\t\\\n"
  "\t\tcl_uint\t\toldval;\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tcl_uint\t\tnewval;\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tfloat\t\ttemp;\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\toldval = curval;\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\ttemp = operation(__int_as_float(oldval), (value));\t\t\t\\\n"
  "\t\t\tnewval = __float_as_int(temp);\t\t\t\t\t\t\t\t\\\n"
  "\t\t} while ((curval = atomicCAS((cl_uint *) (addr),\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t oldval, newval)) != oldval);\t\t\\\n"
  "\t\treturn __int_as_float(oldval);\t\t\t\t\t\t\t\t\t\\\n"
  "\t} while(0)\n"
  "\n"
  "STATIC_INLINE(cl_float)\n"
  "pg_atomic_min_float(cl_float *addr, float value)\n"
  "{\n"
  "\tPG_ATOMIC_FLOAT_TEMPLATE(Min, addr, value);\n"
  "}\n"
  "\n"
  "STATIC_INLINE(cl_float)\n"
  "pg_atomic_max_float(cl_float *addr, float value)\n"
  "{\n"
  "\tPG_ATOMIC_FLOAT_TEMPLATE(Max, addr, value);\n"
  "}\n"
  "\n"
  "STATIC_INLINE(cl_float)\n"
  "pg_atomic_add_float(cl_float *addr, float value)\n"
  "{\n"
  "#if __CUDA_ARCH__ < 350\n"
  "\tPG_ATOMIC_FLOAT_TEMPLATE(Add, addr, value);\n"
  "#else\n"
  "\treturn atomicAdd(addr, value);\n"
  "#endif\n"
  "}\n"
  "\n"
  "#define PG_ATOMIC_DOUBLE_TEMPLATE(operation,addr,value)\t\t\t\t\t\\\n"
  "\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tcl_ulong\tcurval = __double_as_longlong(*(addr));\t\t\t\t\\\n"
  "\t\tcl_ulong\toldval;\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tcl_ulong\tnewval;\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tdouble\t\ttemp;\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\toldval = curval;\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\ttemp = operation(__longlong_as_double(oldval), (value));\t\\\n"
  "\t\t\tnewval = __double_as_longlong(temp);\t\t\t\t\t\t\\\n"
  "\t\t} while ((curval = atomicCAS((cl_ulong *) (addr),\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t oldval, newval)) != oldval);\t\t\\\n"
  "\t\treturn __longlong_as_double(oldval);\t\t\t\t\t\t\t\\\n"
  "\t} while(0)\n"
  "\n"
  "STATIC_INLINE(cl_double)\n"
  "pg_atomic_min_double(cl_double *addr, cl_double value)\n"
  "{\n"
  "\tPG_ATOMIC_DOUBLE_TEMPLATE(Min, addr, value);\n"
  "}\n"
  "\n"
  "STATIC_INLINE(cl_double)\n"
  "pg_atomic_max_double(cl_double *addr, cl_double value)\n"
  "{\n"
  "\tPG_ATOMIC_DOUBLE_TEMPLATE(Max, addr, value);\n"
  "}\n"
  "\n"
  "STATIC_INLINE(cl_double)\n"
  "pg_atomic_add_double(cl_double *addr, cl_double value)\n"
  "{\n"
  "\tPG_ATOMIC_DOUBLE_TEMPLATE(Add, addr, value);\n"
  "}\n"
  "\n"
  "/* macro to check overflow on accumlate operation */\n"
  "#define CHECK_OVERFLOW_NONE(x,y)\t\t(0)\n"
  "\n"
  "#define CHECK_OVERFLOW_SHORT(x,y)\t\t\\\n"
  "\t(((x)+(y)) < SHRT_MIN || SHRT_MAX < ((x)+(y)))\n"
  "\n"
  "#define CHECK_OVERFLOW_INT(x,y)\t\t\t\\\n"
  "\t((((x) < 0) == ((y) < 0)) && (((x) + (y) < 0) != ((x) < 0)))\n"
  "\t\n"
  "#define CHECK_OVERFLOW_FLOAT(x,y)\t\t\\\n"
  "\t(isinf((x) + (y)) && !isinf(x) && !isinf(y))\n"
  "\n"
  "#define CHECK_OVERFLOW_NUMERIC(x,y)\t\tCHECK_OVERFLOW_NONE(x,y)\n"
  "\n"
  "/*\n"
  " * Helper macros for gpupreagg_local_calc\n"
  " */\n"
  "#define AGGCALC_LOCAL_TEMPLATE(TYPE,kcxt,\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t   accum_isnull,accum_val,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t   newval_isnull,newval_val,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\t\t\\\n"
  "\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tif (!(newval_isnull))\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tTYPE old = ATOMIC_FUNC_CALL;\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tif (OVERFLOW_CHECK(old, (newval_val)))\t\t\t\t\t\t\\\n"
  "\t\t\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\tSTROM_SET_ERROR(&kcxt->e, StromError_CpuReCheck);\t\t\\\n"
  "\t\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t(accum_isnull) = false;\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t} while (0)\n"
  "\n"
  "#define AGGCALC_LOCAL_TEMPLATE_SHORT(kcxt,accum,newval,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE(cl_int,kcxt,\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t   (accum)->isnull,(accum)->int_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   (newval)->isnull,(newval)->int_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_LOCAL_TEMPLATE_INT(kcxt,accum,newval,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE(cl_int,kcxt,\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t   (accum)->isnull,(accum)->int_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   (newval)->isnull,(newval)->int_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_LOCAL_TEMPLATE_LONG(kcxt,accum,newval,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE(cl_long,kcxt,\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t   (accum)->isnull,(accum)->long_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   (newval)->isnull,(newval)->long_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_LOCAL_TEMPLATE_FLOAT(kcxt,accum,newval,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE(cl_float,kcxt,\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t   (accum)->isnull,(accum)->float_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   (newval)->isnull,(newval)->float_val,\t\t\\\n"
  "\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_LOCAL_TEMPLATE_DOUBLE(kcxt,accum,newval,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE(cl_double,kcxt,\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t   (accum)->isnull,(accum)->double_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   (newval)->isnull,(newval)->double_val,\t\t\\\n"
  "\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_LOCAL_TEMPLATE_NUMERIC(kcxt,accum,newval,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE(cl_ulong,kcxt,\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t   (accum)->isnull,(accum)->ulong_val,\t\t\t\\\n"
  "\t\t\t\t\t\t   (newval)->isnull,(newval)->ulong_val,\t\t\\\n"
  "\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "\n"
  "/* calculation for local partial min */\n"
  "#define AGGCALC_LOCAL_PMIN_SHORT(kcxt,accum,newval)\t\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_SHORT((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_min_int(&(accum)->int_val,(newval)->int_val))\n"
  "#define AGGCALC_LOCAL_PMIN_INT(kcxt,accum,newval)\t\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_INT((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_min_int(&(accum)->int_val,(newval)->int_val))\n"
  "#define AGGCALC_LOCAL_PMIN_LONG(kcxt,accum,newval)\t\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_LONG((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_min_long(&(accum)->long_val,(newval)->long_val))\n"
  "#define AGGCALC_LOCAL_PMIN_FLOAT(kcxt,accum,newval)\t\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_FLOAT((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_min_float(&(accum)->float_val,(newval)->float_val))\n"
  "#define AGGCALC_LOCAL_PMIN_DOUBLE(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_DOUBLE((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_min_double(&(accum)->double_val,(newval)->double_val))\n"
  "#define AGGCALC_LOCAL_PMIN_NUMERIC(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_NUMERIC(kcxt,accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_min_numeric((kcxt),&(accum)->ulong_val,(newval)->ulong_val))\n"
  "\n"
  "/* calculation for local partial max */\n"
  "#define AGGCALC_LOCAL_PMAX_SHORT(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_SHORT((kcxt),accum,newval,CHECK_OVERFLOW_NONE,\t\\\n"
  "\t\tpg_atomic_max_int(&(accum)->int_val,(newval)->int_val))\n"
  "#define AGGCALC_LOCAL_PMAX_INT(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_INT((kcxt),accum,newval,CHECK_OVERFLOW_NONE,\t\\\n"
  "\t\tpg_atomic_max_int(&(accum)->int_val,(newval)->int_val))\n"
  "#define AGGCALC_LOCAL_PMAX_LONG(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_LONG((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_max_long(&(accum)->long_val,(newval)->long_val))\n"
  "#define AGGCALC_LOCAL_PMAX_FLOAT(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_FLOAT((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_max_float(&(accum)->float_val,(newval)->float_val))\n"
  "#define AGGCALC_LOCAL_PMAX_DOUBLE(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_DOUBLE((kcxt),accum,newval,CHECK_OVERFLOW_NONE, \\\n"
  "\t\tpg_atomic_max_double(&(accum)->double_val,(newval)->double_val))\n"
  "#define AGGCALC_LOCAL_PMAX_NUMERIC(kcxt,accum,newval)\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_NUMERIC((kcxt),accum,newval,CHECK_OVERFLOW_NONE,\t\\\n"
  "\t\tpg_atomic_max_numeric((kcxt),&(accum)->ulong_val,(newval)->ulong_val))\n"
  "\n"
  "/* calculation for local partial add */\n"
  "#define AGGCALC_LOCAL_PADD_SHORT(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_SHORT((kcxt),accum,newval,CHECK_OVERFLOW_SHORT, \\\n"
  "\t\tpg_atomic_add_int(&(accum)->int_val,(newval)->int_val))\n"
  "#define AGGCALC_LOCAL_PADD_INT(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_INT((kcxt),accum,newval,CHECK_OVERFLOW_INT,\t\\\n"
  "\t\tpg_atomic_add_int(&(accum)->int_val,(newval)->int_val))\n"
  "#define AGGCALC_LOCAL_PADD_LONG(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_LONG((kcxt),accum,newval,CHECK_OVERFLOW_INT,\t\\\n"
  "\t\tpg_atomic_add_long(&(accum)->long_val,(newval)->long_val))\n"
  "#define AGGCALC_LOCAL_PADD_FLOAT(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_FLOAT((kcxt),accum,newval,CHECK_OVERFLOW_FLOAT, \\\n"
  "\t\tpg_atomic_add_float(&(accum)->float_val,(newval)->float_val))\n"
  "#define AGGCALC_LOCAL_PADD_DOUBLE(kcxt,accum,newval)\t\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_DOUBLE((kcxt),accum,newval,CHECK_OVERFLOW_FLOAT,\t\\\n"
  "\t\tpg_atomic_add_double(&(accum)->double_val,(newval)->double_val))\n"
  "#define AGGCALC_LOCAL_PADD_NUMERIC(kcxt,accum,newval)\t\t\t\t\\\n"
  "\tAGGCALC_LOCAL_TEMPLATE_NUMERIC((kcxt),accum,newval,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_NUMERIC,\t\t\t\t\\\n"
  "\t\tpg_atomic_add_numeric((kcxt),&(accum)->ulong_val,(newval)->ulong_val))\n"
  "\n"
  "/*\n"
  " * Helper macros for gpupreagg_global_calc\n"
  " *\n"
  " * NOTE: please assume the variables below are available in the context\n"
  " * these macros in use.\n"
  " *   char            new_isnull;\n"
  " *   Datum           new_value;\n"
  " *   __global char  *accum_isnull;\n"
  " *   __global Datum *accum_value;\n"
  " */\n"
  "#define AGGCALC_GLOBAL_TEMPLATE(TYPE,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tif (!(new_isnull))\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tTYPE old = ATOMIC_FUNC_CALL;\t\t\t\t\t\t\t\t\\\n"
  "\t\tif (OVERFLOW_CHECK(old, (new_value)))\t\t\t\t\t\t\\\n"
  "\t\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tSTROM_SET_ERROR(&(kcxt)->e, StromError_CpuReCheck);\t\t\\\n"
  "\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t*(accum_isnull) = false;\t\t\t\t\t\t\t\t\t\\\n"
  "\t}\n"
  "\n"
  "#define AGGCALC_GLOBAL_TEMPLATE_SHORT(kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  new_isnull,new_value,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE(cl_int, kcxt,\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_GLOBAL_TEMPLATE_INT(kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE(cl_int,kcxt,\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_GLOBAL_TEMPLATE_LONG(kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t new_isnull,new_value,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE(cl_long,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_GLOBAL_TEMPLATE_FLOAT(kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  new_isnull,new_value,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE(cl_float,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_GLOBAL_TEMPLATE_DOUBLE(kcxt,\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t   accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   new_isnull,new_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t   OVERFLOW_CHECK,ATOMIC_FUNC_CALL)\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE(cl_double,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "#define AGGCALC_GLOBAL_TEMPLATE_NUMERIC(kcxt,\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\taccum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL) \\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE(cl_ulong,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\taccum_isnull,accum_value,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tnew_isnull,new_value,\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\tOVERFLOW_CHECK,ATOMIC_FUNC_CALL)\n"
  "\n"
  "/* calculation for global partial max */\n"
  "#define AGGCALC_GLOBAL_PMAX_SHORT(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t  new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_SHORT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_int)(new_value), CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\tpg_atomic_max_int((cl_int *)(accum_value),(cl_int)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMAX_INT(kcxt,accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\tnew_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_INT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_int)(new_value),CHECK_OVERFLOW_NONE,\t\t\t\\\n"
  "\t\tpg_atomic_max_int((cl_int *)(accum_value),(cl_int)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMAX_LONG(kcxt,accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_LONG(\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_long)(new_value),CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\tpg_atomic_max_long((cl_long *)(accum_value),(cl_long)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMAX_FLOAT(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t  new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_FLOAT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull, __int_as_float((cl_uint)(new_value)),\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_max_float((cl_float *)(accum_value),\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t__int_as_float((cl_uint)(new_value))))\n"
  "#define AGGCALC_GLOBAL_PMAX_DOUBLE(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t   new_isnull,new_value)\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_DOUBLE(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull, __longlong_as_double(new_value),\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_max_double((cl_double *)(accum_value),\t\t\t\\\n"
  "\t\t\t\t\t\t\t __longlong_as_double(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMAX_NUMERIC(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t\tnew_isnull,new_value)\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_NUMERIC(\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt, accum_isnull, accum_value,\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull, (cl_ulong)(new_value),\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_max_numeric((kcxt),(cl_ulong *)(accum_value),\t\t\\\n"
  "\t\t\t\t\t\t\t  (cl_ulong)(new_value)))\n"
  "/* calculation for global partial min */\n"
  "#define AGGCALC_GLOBAL_PMIN_SHORT(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t  new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_SHORT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_int)(new_value),\t\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_min_int((cl_int *)(accum_value),(cl_int)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMIN_INT(kcxt,accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\tnew_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_INT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_int)(new_value),\t\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_min_int((cl_int *)(accum_value),(cl_int)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMIN_LONG(kcxt,accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_LONG(\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_long)(new_value),\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_min_long((cl_long *)(accum_value),(cl_long)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMIN_FLOAT(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t  new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_FLOAT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,__int_as_float((cl_uint)(new_value)),\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_min_float((cl_float *)(accum_value),\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t__int_as_float((cl_uint)(new_value))))\n"
  "#define AGGCALC_GLOBAL_PMIN_DOUBLE(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t   new_isnull,new_value)\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_DOUBLE(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt, accum_isnull, accum_value,\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,__longlong_as_double(new_value),\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NONE,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_min_double((cl_double *)(accum_value),\t\t\t\\\n"
  "\t\t\t\t\t\t\t __longlong_as_double(new_value)))\n"
  "#define AGGCALC_GLOBAL_PMIN_NUMERIC(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t\tnew_isnull,new_value)\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_NUMERIC(\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_ulong)(new_value),CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\tpg_atomic_min_numeric((kcxt),(cl_ulong *)(accum_value),\t\t\\\n"
  "\t\t\t\t\t\t\t  (cl_ulong)(new_value)))\n"
  "/* calculation for global partial add */\n"
  "#define AGGCALC_GLOBAL_PADD_SHORT(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t  new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_SHORT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_int)(new_value),\t\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_SHORT,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_add_int((cl_int *)(accum_value), (cl_int)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PADD_INT(kcxt,accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\tnew_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_INT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_int)(new_value),\t\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_INT,\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_add_int((cl_int *)(accum_value), (cl_int)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PADD_LONG(kcxt,accum_isnull,accum_value,\t\t\\\n"
  "\t\t\t\t\t\t\t\t new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_LONG(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_long)(new_value),\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_INT,\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_add_long((cl_long *)(accum_value), (cl_long)(new_value)))\n"
  "#define AGGCALC_GLOBAL_PADD_FLOAT(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t  new_isnull,new_value)\t\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_FLOAT(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t    new_isnull,__int_as_float((cl_uint)(new_value)),\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_FLOAT,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_add_float((cl_float *)(accum_value),\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t__int_as_float((cl_uint)(new_value))))\n"
  "#define AGGCALC_GLOBAL_PADD_DOUBLE(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t   new_isnull,new_value)\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_DOUBLE(\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,__longlong_as_double(new_value),\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_FLOAT,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_add_double((cl_double *)(accum_value),\t\t\t\\\n"
  "\t\t\t\t\t\t\t __longlong_as_double(new_value)))\n"
  "#define AGGCALC_GLOBAL_PADD_NUMERIC(kcxt,accum_isnull,accum_value,\t\\\n"
  "\t\t\t\t\t\t\t\t\tnew_isnull,new_value)\t\t\t\\\n"
  "\tAGGCALC_GLOBAL_TEMPLATE_NUMERIC(\t\t\t\t\t\t\t\t\\\n"
  "\t\tkcxt,accum_isnull,accum_value,\t\t\t\t\t\t\t\t\\\n"
  "\t\tnew_isnull,(cl_ulong)(new_value),\t\t\t\t\t\t\t\\\n"
  "\t\tCHECK_OVERFLOW_NUMERIC,\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_atomic_add_numeric((kcxt),(cl_ulong *)(accum_value),\t\t\\\n"
  "\t\t\t\t\t\t\t  (cl_ulong)(new_value)))\n"
  "\n"
  "/*\n"
  " * Helper macros for gpupreagg_nogroup_calc\n"
  " */\n"
  "#define AGGCALC_NOGROUP_TEMPLATE(TYPE,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t accum_isnull,accum_val,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t newval_isnull,newval_val,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\t\t\t\\\n"
  "\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tif (!(newval_isnull))\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tTYPE tmp = FUNC_CALL;\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tif (OVERFLOW_CHECK((accum_val), (newval_val)))\t\t\t\\\n"
  "\t\t\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\tSTROM_SET_ERROR(&(kcxt)->e, StromError_CpuReCheck);\t\\\n"
  "\t\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t(accum_val)    = tmp;\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t(accum_isnull) = false;\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t} while (0)\n"
  "\n"
  "#define AGGCALC_NOGROUP_TEMPLATE_SHORT(kcxt,accum,newval,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t   OVERFLOW_CHECK,FUNC_CALL)\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE(cl_int,(kcxt),\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t (accum)->isnull,(accum)->int_val,\t\t\\\n"
  "\t\t\t\t\t\t\t (newval)->isnull,(newval)->int_val,\t\\\n"
  "\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\n"
  "#define AGGCALC_NOGROUP_TEMPLATE_INT(kcxt,accum,newval,\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE(cl_int,kcxt,\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t (accum)->isnull,(accum)->int_val,\t\t\\\n"
  "\t\t\t\t\t\t\t (newval)->isnull,(newval)->int_val,\t\\\n"
  "\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\n"
  "#define AGGCALC_NOGROUP_TEMPLATE_LONG(kcxt,accum,newval,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t  OVERFLOW_CHECK,FUNC_CALL)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE(cl_long,(kcxt),\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t (accum)->isnull,(accum)->long_val,\t\t\\\n"
  "\t\t\t\t\t\t\t (newval)->isnull,(newval)->long_val,\t\\\n"
  "\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\n"
  "#define AGGCALC_NOGROUP_TEMPLATE_FLOAT(kcxt,accum,newval,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t   OVERFLOW_CHECK,FUNC_CALL)\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE(cl_float,(kcxt),\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t (accum)->isnull,(accum)->float_val,\t\\\n"
  "\t\t\t\t\t\t\t (newval)->isnull,(newval)->float_val,\t\\\n"
  "\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\n"
  "#define AGGCALC_NOGROUP_TEMPLATE_DOUBLE(kcxt,accum,newval,\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\tOVERFLOW_CHECK,FUNC_CALL)\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE(cl_double,(kcxt),\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t (accum)->isnull,(accum)->double_val,\t\\\n"
  "\t\t\t\t\t\t\t (newval)->isnull,(newval)->double_val,\t\\\n"
  "\t\t\t\t\t\t\t OVERFLOW_CHECK,FUNC_CALL)\n"
  "\n"
  "#ifdef PG_NUMERIC_TYPE_DEFINED\n"
  "#define AGGCALC_NOGROUP_TEMPLATE_NUMERIC(kcxt,accum_val,new_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t\t FUNC_CALL)\t\t\t\t\t\\\n"
  "\tdo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tpg_numeric_t\tx, y, z;\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\tif (!(new_val)->isnull)\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tx.isnull = false;\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\tx.value = (accum_val)->ulong_val;\t\t\t\t\t\t\\\n"
  "\t\t\ty.isnull = false;\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\ty.value = (new_val)->ulong_val;\t\t\t\t\t\t\t\\\n"
  "\t\t\tz = FUNC_CALL((kcxt), x, y);\t\t\t\t\t\t\t\\\n"
  "\t\t\tif (z.isnull)\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t\t\t\tSTROM_SET_ERROR(&(kcxt)->e, StromError_CpuReCheck);\t\\\n"
  "\t\t\t(accum_val)->ulong_val = z.value;\t\t\t\t\t\t\\\n"
  "\t\t\t(accum_val)->isnull = z.isnull;\t\t\t\t\t\t\t\\\n"
  "\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n"
  "\t} while (0)\n"
  "#endif\n"
  "\n"
  "/* calculation for no group partial max */\n"
  "#define AGGCALC_NOGROUP_PMAX_SHORT(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_SHORT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   Max((accum)->int_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   (newval)->int_val))\n"
  "#define AGGCALC_NOGROUP_PMAX_INT(kcxt,accum,newval)\t\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_INT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t Max((accum)->int_val,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t (newval)->int_val))\n"
  "#define AGGCALC_NOGROUP_PMAX_LONG(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_LONG((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t  CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t  Max((accum)->long_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t  (newval)->long_val))\n"
  "#define AGGCALC_NOGROUP_PMAX_FLOAT(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_FLOAT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   Max((accum)->float_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   (newval)->float_val))\n"
  "#define AGGCALC_NOGROUP_PMAX_DOUBLE(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_DOUBLE((kcxt),accum,newval,\t\\\n"
  "\t\t\t\t\t\t\t\t\tCHECK_OVERFLOW_NONE,\t\\\n"
  "\t\t\t\t\t\t\t\t\tMax((accum)->double_val,\\\n"
  "\t\t\t\t\t\t\t\t\t\t(newval)->double_val))\n"
  "#define AGGCALC_NOGROUP_PMAX_NUMERIC(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_NUMERIC((kcxt),accum,newval,\t\\\n"
  "\t\t\t\t\t\t\t\t\t pgfn_numeric_max)\n"
  "\n"
  "/* calculation for no group partial min */\n"
  "#define AGGCALC_NOGROUP_PMIN_SHORT(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_SHORT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   Min((accum)->int_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   (newval)->int_val))\n"
  "#define AGGCALC_NOGROUP_PMIN_INT(kcxt,accum,newval)\t\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_INT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t Min((accum)->int_val,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t (newval)->int_val))\n"
  "#define AGGCALC_NOGROUP_PMIN_LONG(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_LONG((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t  CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t  Min((accum)->long_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t  (newval)->long_val))\n"
  "#define AGGCALC_NOGROUP_PMIN_FLOAT(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_FLOAT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_NONE,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   Min((accum)->float_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   (newval)->float_val))\n"
  "#define AGGCALC_NOGROUP_PMIN_DOUBLE(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_DOUBLE((kcxt),accum,newval,\t\\\n"
  "\t\t\t\t\t\t\t\t\tCHECK_OVERFLOW_NONE,\t\\\n"
  "\t\t\t\t\t\t\t\t\tMin((accum)->double_val,\\\n"
  "\t\t\t\t\t\t\t\t\t\t(newval)->double_val))\n"
  "#define AGGCALC_NOGROUP_PMIN_NUMERIC(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_NUMERIC((kcxt),accum,newval,\t\\\n"
  "\t\t\t\t\t\t\t\t\t pgfn_numeric_min)\n"
  "\n"
  "/* calculation for no group partial add */\n"
  "#define AGGCALC_NOGROUP_PADD_SHORT(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_SHORT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_SHORT,\t\\\n"
  "\t\t\t\t\t\t\t\t   Add((accum)->int_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   (newval)->int_val))\n"
  "#define AGGCALC_NOGROUP_PADD_INT(kcxt,accum,newval)\t\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_INT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t CHECK_OVERFLOW_INT,\t\t\\\n"
  "\t\t\t\t\t\t\t\t Add((accum)->int_val,\t\t\\\n"
  "\t\t\t\t\t\t\t\t\t (newval)->int_val))\n"
  "#define AGGCALC_NOGROUP_PADD_LONG(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_LONG((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t  CHECK_OVERFLOW_INT,\t\t\\\n"
  "\t\t\t\t\t\t\t\t  Add((accum)->long_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t  (newval)->long_val))\n"
  "#define AGGCALC_NOGROUP_PADD_FLOAT(kcxt,accum,newval)\t\t\\\n"
  "    AGGCALC_NOGROUP_TEMPLATE_FLOAT((kcxt),accum,newval,\t\t\\\n"
  "\t\t\t\t\t\t\t\t   CHECK_OVERFLOW_FLOAT,\t\\\n"
  "\t\t\t\t\t\t\t\t   Add((accum)->float_val,\t\\\n"
  "\t\t\t\t\t\t\t\t\t   (newval)->float_val))\n"
  "#define AGGCALC_NOGROUP_PADD_DOUBLE(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_DOUBLE((kcxt),accum,newval,\t\\\n"
  "\t\t\t\t\t\t\t\t\tCHECK_OVERFLOW_FLOAT,\t\\\n"
  "\t\t\t\t\t\t\t\t\tAdd((accum)->double_val,\\\n"
  "\t\t\t\t\t\t\t\t\t\t(newval)->double_val))\n"
  "#define AGGCALC_NOGROUP_PADD_NUMERIC(kcxt,accum,newval)\t\t\\\n"
  "\tAGGCALC_NOGROUP_TEMPLATE_NUMERIC((kcxt),accum,newval,\t\\\n"
  "\t\t\t\t\t\t\t\t\t pgfn_numeric_add)\n"
  "#endif\t/* __CUDACC__ */\n"
  "#endif\t/* CUDA_GPUPREAGG_H */\n"
;
